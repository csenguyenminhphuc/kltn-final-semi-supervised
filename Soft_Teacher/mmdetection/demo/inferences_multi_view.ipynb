{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdaccd4c",
   "metadata": {},
   "source": [
    "# Multi-View Inference Notebook\n",
    "\n",
    "**M√¥ t·∫£:** Notebook n√†y th·ª±c hi·ªán inference cho multi-view model (8 crops/image)\n",
    "\n",
    "**C√°c b∆∞·ªõc th·ª±c hi·ªán:**\n",
    "1. Load config t·ª´ file training\n",
    "2. Kh·ªüi t·∫°o model (extract detector t·ª´ MultiViewSoftTeacher)\n",
    "3. Load checkpoint weights\n",
    "4. Chu·∫©n b·ªã d·ªØ li·ªáu test\n",
    "5. Inference t·ª´ng group 8 crops\n",
    "6. ƒê√°nh gi√° k·∫øt qu·∫£"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889cad4a",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 1: Import th∆∞ vi·ªán v√† thi·∫øt l·∫≠p m√¥i tr∆∞·ªùng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5abd51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Add mmdetection to path\n",
    "sys.path.insert(0, '/home/coder/data/trong/KLTN/Soft_Teacher/mmdetection')\n",
    "\n",
    "# Import mmengine v√† mmdet\n",
    "from mmengine.config import Config\n",
    "from mmengine.runner import Runner\n",
    "from mmdet.apis import init_detector, inference_detector\n",
    "\n",
    "# Check GPU\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Current GPU: {torch.cuda.current_device()}\")\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b412a49",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 2: Load config v√† x√°c ƒë·ªãnh paths\n",
    "\n",
    "K·∫øt qu·∫£ training t·ª´ log:\n",
    "- **Teacher bbox_mAP_50**: 0.1290 (12.90%)\n",
    "- **Teacher bbox_mAP**: 0.0410 (4.10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf286bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "config_file = '/home/coder/data/trong/KLTN/Soft_Teacher/work_dirs/soft_teacher_custom_multi_view/20251206_182403/vis_data/config.py'\n",
    "checkpoint_file = '/home/coder/data/trong/KLTN/Soft_Teacher/work_dirs/soft_teacher_custom_multi_view/best_teacher_coco_bbox_mAP_50_epoch_0.pth'\n",
    "\n",
    "# Check files exist\n",
    "print(f\"Config exists: {os.path.exists(config_file)}\")\n",
    "print(f\"Checkpoint exists: {os.path.exists(checkpoint_file)}\")\n",
    "\n",
    "# Load config\n",
    "cfg = Config.fromfile(config_file)\n",
    "print(f\"\\nModel type: {cfg.model.type}\")\n",
    "print(f\"Backbone fusion: {cfg.model.detector.backbone.fusion}\")\n",
    "print(f\"MVViT spatial_attention: {cfg.model.detector.backbone.mvvit.spatial_attention}\")\n",
    "print(f\"Views per sample: {cfg.views_per_sample}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dab427",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 3: Gi·∫£m batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dba48e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override batch size to 1 (per-crop evaluation)\n",
    "cfg.test_dataloader.batch_size = 1\n",
    "cfg.val_dataloader.batch_size = 1\n",
    "\n",
    "# Reduce num_workers to save memory\n",
    "cfg.test_dataloader.num_workers = 1\n",
    "cfg.val_dataloader.num_workers = 1\n",
    "\n",
    "print(f\"Test batch size: {cfg.test_dataloader.batch_size}\")\n",
    "print(f\"Val batch size: {cfg.val_dataloader.batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0e1566",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 4: Kh·ªüi t·∫°o Runner v√† load checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38229848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set checkpoint path\n",
    "cfg.load_from = checkpoint_file\n",
    "\n",
    "# Set work dir for evaluation results\n",
    "cfg.work_dir = '/home/coder/data/trong/KLTN/Soft_Teacher/work_dirs/eval_checkpoint_notebook'\n",
    "os.makedirs(cfg.work_dir, exist_ok=True)\n",
    "\n",
    "# Build runner\n",
    "print(\"Building runner...\")\n",
    "runner = Runner.from_cfg(cfg)\n",
    "print(f\"‚úÖ Runner created successfully!\")\n",
    "print(f\"Model type: {type(runner.model).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc0b126",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 5: Ch·∫°y evaluation\n",
    "\n",
    "**L∆∞u √Ω:** Cell n√†y c√≥ th·ªÉ ch·∫°y l√¢u (~2-3 ph√∫t) v√† t·ªën memory. N·∫øu b·ªã OOM, c·∫ßn gi·∫£m K trong `multi_view_transformer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce753c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Starting evaluation...\")\n",
    "print(f\"Checkpoint: {checkpoint_file}\")\n",
    "print(f\"Config: {config_file}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Run test\n",
    "metrics = runner.test()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ Evaluation completed!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e12afe",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 6: Hi·ªÉn th·ªã k·∫øt qu·∫£ chi ti·∫øt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502ee664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract teacher results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEACHER MODEL RESULTS:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "teacher_keys = sorted([k for k in metrics.keys() if k.startswith('teacher/')])\n",
    "for key in teacher_keys:\n",
    "    metric_name = key.replace('teacher/coco/', '')\n",
    "    value = metrics[key]\n",
    "    if isinstance(value, (int, float)) and value >= 0:\n",
    "        print(f\"  {metric_name:30s}: {value:.4f} ({value*100:.2f}%)\")\n",
    "    else:\n",
    "        print(f\"  {metric_name:30s}: {value}\")\n",
    "\n",
    "# Extract student results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STUDENT MODEL RESULTS:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "student_keys = sorted([k for k in metrics.keys() if k.startswith('student/')])\n",
    "for key in student_keys:\n",
    "    metric_name = key.replace('student/coco/', '')\n",
    "    value = metrics[key]\n",
    "    if isinstance(value, (int, float)) and value >= 0:\n",
    "        print(f\"  {metric_name:30s}: {value:.4f} ({value*100:.2f}%)\")\n",
    "    else:\n",
    "        print(f\"  {metric_name:30s}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7453b29e",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 7: So s√°nh v·ªõi k·∫øt qu·∫£ training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a3256d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected results from training log\n",
    "expected_teacher_map50 = 0.1290\n",
    "expected_teacher_map = 0.0410\n",
    "\n",
    "# Actual results from re-evaluation\n",
    "actual_teacher_map50 = metrics.get('teacher/coco/bbox_mAP_50', -1)\n",
    "actual_teacher_map = metrics.get('teacher/coco/bbox_mAP', -1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON WITH TRAINING RESULTS:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìä Teacher bbox_mAP_50:\")\n",
    "print(f\"  Training:   {expected_teacher_map50:.4f} ({expected_teacher_map50*100:.2f}%)\")\n",
    "print(f\"  Re-eval:    {actual_teacher_map50:.4f} ({actual_teacher_map50*100:.2f}%)\")\n",
    "diff_map50 = abs(actual_teacher_map50 - expected_teacher_map50)\n",
    "print(f\"  Difference: {diff_map50:.4f} ({diff_map50*100:.2f}%)\")\n",
    "match_map50 = diff_map50 < 0.001\n",
    "print(f\"  Match:      {'‚úÖ YES' if match_map50 else '‚ùå NO'}\")\n",
    "\n",
    "print(\"\\nüìä Teacher bbox_mAP:\")\n",
    "print(f\"  Training:   {expected_teacher_map:.4f} ({expected_teacher_map*100:.2f}%)\")\n",
    "print(f\"  Re-eval:    {actual_teacher_map:.4f} ({actual_teacher_map*100:.2f}%)\")\n",
    "diff_map = abs(actual_teacher_map - expected_teacher_map)\n",
    "print(f\"  Difference: {diff_map:.4f} ({diff_map*100:.2f}%)\")\n",
    "match_map = diff_map < 0.001\n",
    "print(f\"  Match:      {'‚úÖ YES' if match_map else '‚ùå NO'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "if match_map50 and match_map:\n",
    "    print(\"‚úÖ K·∫æT LU·∫¨N: K·∫øt qu·∫£ re-eval KH·ªöP v·ªõi training! Checkpoint ƒë√∫ng.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è K·∫æT LU·∫¨N: C√≥ s·ª± kh√°c bi·ªát. Ki·ªÉm tra l·∫°i config ho·∫∑c data.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8fff33",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 8: Eval tr√™n Test Set (Bright Images)\n",
    "\n",
    "Gi·ªù ta s·∫Ω eval tr√™n test set ƒë·ªÉ so s√°nh performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a29ff8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure test set\n",
    "test_ann_file = '/home/coder/data/trong/KLTN/Soft_Teacher/data_drill/anno_test/_annotations_filtered.bright.coco.json'\n",
    "test_data_prefix = 'test/'\n",
    "\n",
    "# Check if test annotation exists\n",
    "print(f\"Test annotation exists: {os.path.exists(test_ann_file)}\")\n",
    "\n",
    "# Update test dataloader config\n",
    "cfg.test_dataloader.dataset.ann_file = test_ann_file\n",
    "cfg.test_dataloader.dataset.data_prefix.img = test_data_prefix\n",
    "cfg.test_dataloader.batch_size = 1\n",
    "cfg.test_dataloader.num_workers = 1\n",
    "\n",
    "# Update test evaluator\n",
    "cfg.test_evaluator.ann_file = test_ann_file\n",
    "\n",
    "print(f\"Test annotation: {test_ann_file}\")\n",
    "print(f\"Test data prefix: {test_data_prefix}\")\n",
    "print(f\"Batch size: {cfg.test_dataloader.batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b3cdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebuild runner with test set config\n",
    "cfg.work_dir = '/home/coder/data/trong/KLTN/Soft_Teacher/work_dirs/eval_test_set'\n",
    "os.makedirs(cfg.work_dir, exist_ok=True)\n",
    "\n",
    "print(\"Rebuilding runner for test set...\")\n",
    "runner_test = Runner.from_cfg(cfg)\n",
    "print(f\"‚úÖ Test runner created!\")\n",
    "\n",
    "# Run evaluation on test set\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Evaluating on TEST SET...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_metrics = runner_test.test()\n",
    "\n",
    "print(\"\\n‚úÖ Test evaluation completed!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4728474f",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 9: So s√°nh Validation vs Test Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b5a505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Extract metrics for comparison\n",
    "def extract_metrics(metrics_dict, prefix='teacher'):\n",
    "    results = {}\n",
    "    for key, value in metrics_dict.items():\n",
    "        if key.startswith(f'{prefix}/coco/'):\n",
    "            metric_name = key.replace(f'{prefix}/coco/', '')\n",
    "            if isinstance(value, (int, float)) and value >= 0:\n",
    "                results[metric_name] = value\n",
    "    return results\n",
    "\n",
    "# Get validation metrics (from previous eval)\n",
    "val_teacher = extract_metrics(metrics, 'teacher')\n",
    "val_student = extract_metrics(metrics, 'student')\n",
    "\n",
    "# Get test metrics\n",
    "test_teacher = extract_metrics(test_metrics, 'teacher')\n",
    "test_student = extract_metrics(test_metrics, 'student')\n",
    "\n",
    "# Create comparison dataframe for Teacher\n",
    "teacher_comparison = pd.DataFrame({\n",
    "    'Validation': val_teacher,\n",
    "    'Test': test_teacher\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEACHER MODEL: Validation vs Test Comparison\")\n",
    "print(\"=\"*80)\n",
    "print(teacher_comparison.to_string())\n",
    "\n",
    "# Calculate difference\n",
    "teacher_comparison['Diff'] = teacher_comparison['Test'] - teacher_comparison['Validation']\n",
    "teacher_comparison['Diff%'] = (teacher_comparison['Diff'] / teacher_comparison['Validation'] * 100).round(2)\n",
    "\n",
    "print(\"\\nüìä Key Metrics Comparison (Teacher):\")\n",
    "for metric in ['bbox_mAP', 'bbox_mAP_50', 'bbox_mAP_75']:\n",
    "    if metric in teacher_comparison.index:\n",
    "        val_val = teacher_comparison.loc[metric, 'Validation']\n",
    "        test_val = teacher_comparison.loc[metric, 'Test']\n",
    "        diff = teacher_comparison.loc[metric, 'Diff']\n",
    "        print(f\"\\n{metric}:\")\n",
    "        print(f\"  Validation: {val_val:.4f} ({val_val*100:.2f}%)\")\n",
    "        print(f\"  Test:       {test_val:.4f} ({test_val*100:.2f}%)\")\n",
    "        print(f\"  Difference: {diff:+.4f} ({diff*100:+.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53fe50d",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 10: Visualize Per-Class Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd05e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 10)\n",
    "\n",
    "# Class names\n",
    "classes = ['Broken', 'Chipped', 'Scratched', 'Severe_Rust', 'Tip_Wear']\n",
    "\n",
    "# Extract per-class precision for Teacher\n",
    "val_class_metrics = {cls: val_teacher.get(f'{cls}_precision', 0) for cls in classes}\n",
    "test_class_metrics = {cls: test_teacher.get(f'{cls}_precision', 0) for cls in classes}\n",
    "\n",
    "# Create figure with 2 subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Bar chart: Validation vs Test per-class\n",
    "ax1 = axes[0, 0]\n",
    "x = np.arange(len(classes))\n",
    "width = 0.35\n",
    "bars1 = ax1.bar(x - width/2, list(val_class_metrics.values()), width, label='Validation', alpha=0.8, color='steelblue')\n",
    "bars2 = ax1.bar(x + width/2, list(test_class_metrics.values()), width, label='Test', alpha=0.8, color='coral')\n",
    "ax1.set_xlabel('Class', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Precision (AP)', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Teacher Model: Per-Class Precision (Validation vs Test)', fontsize=14, fontweight='bold')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(classes, rotation=45, ha='right')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        if height > 0.001:\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{height*100:.1f}%', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# 2. Overall mAP comparison\n",
    "ax2 = axes[0, 1]\n",
    "metrics_to_plot = ['bbox_mAP', 'bbox_mAP_50', 'bbox_mAP_75']\n",
    "val_map_values = [val_teacher.get(m, 0) for m in metrics_to_plot]\n",
    "test_map_values = [test_teacher.get(m, 0) for m in metrics_to_plot]\n",
    "\n",
    "x_map = np.arange(len(metrics_to_plot))\n",
    "bars3 = ax2.bar(x_map - width/2, val_map_values, width, label='Validation', alpha=0.8, color='steelblue')\n",
    "bars4 = ax2.bar(x_map + width/2, test_map_values, width, label='Test', alpha=0.8, color='coral')\n",
    "ax2.set_xlabel('Metric', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Teacher Model: Overall mAP Metrics', fontsize=14, fontweight='bold')\n",
    "ax2.set_xticks(x_map)\n",
    "ax2.set_xticklabels(['mAP', 'mAP@50', 'mAP@75'], rotation=0)\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for bars in [bars3, bars4]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        if height > 0.001:\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{height*100:.1f}%', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# 3. Difference heatmap\n",
    "ax3 = axes[1, 0]\n",
    "differences = [test_class_metrics[cls] - val_class_metrics[cls] for cls in classes]\n",
    "diff_df = pd.DataFrame({\n",
    "    'Class': classes,\n",
    "    'Difference (Test - Val)': [d*100 for d in differences]  # Convert to percentage\n",
    "})\n",
    "colors = ['red' if x < 0 else 'green' for x in differences]\n",
    "bars5 = ax3.barh(classes, [d*100 for d in differences], color=colors, alpha=0.7)\n",
    "ax3.set_xlabel('Difference (%)', fontsize=12, fontweight='bold')\n",
    "ax3.set_ylabel('Class', fontsize=12, fontweight='bold')\n",
    "ax3.set_title('Teacher Model: Performance Difference (Test - Validation)', fontsize=14, fontweight='bold')\n",
    "ax3.axvline(x=0, color='black', linestyle='--', linewidth=1)\n",
    "ax3.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, diff) in enumerate(zip(bars5, differences)):\n",
    "    width_val = bar.get_width()\n",
    "    label_x = width_val + (0.5 if width_val > 0 else -0.5)\n",
    "    ax3.text(label_x, bar.get_y() + bar.get_height()/2, \n",
    "            f'{diff*100:+.1f}%', ha='left' if width_val > 0 else 'right', \n",
    "            va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# 4. Summary statistics table\n",
    "ax4 = axes[1, 1]\n",
    "ax4.axis('off')\n",
    "\n",
    "summary_data = [\n",
    "    ['Metric', 'Validation', 'Test', 'Diff'],\n",
    "    ['‚îÄ'*20, '‚îÄ'*12, '‚îÄ'*12, '‚îÄ'*12],\n",
    "    ['mAP', f\"{val_teacher.get('bbox_mAP', 0)*100:.2f}%\", \n",
    "     f\"{test_teacher.get('bbox_mAP', 0)*100:.2f}%\",\n",
    "     f\"{(test_teacher.get('bbox_mAP', 0) - val_teacher.get('bbox_mAP', 0))*100:+.2f}%\"],\n",
    "    ['mAP@50', f\"{val_teacher.get('bbox_mAP_50', 0)*100:.2f}%\", \n",
    "     f\"{test_teacher.get('bbox_mAP_50', 0)*100:.2f}%\",\n",
    "     f\"{(test_teacher.get('bbox_mAP_50', 0) - val_teacher.get('bbox_mAP_50', 0))*100:+.2f}%\"],\n",
    "    ['mAP@75', f\"{val_teacher.get('bbox_mAP_75', 0)*100:.2f}%\", \n",
    "     f\"{test_teacher.get('bbox_mAP_75', 0)*100:.2f}%\",\n",
    "     f\"{(test_teacher.get('bbox_mAP_75', 0) - val_teacher.get('bbox_mAP_75', 0))*100:+.2f}%\"],\n",
    "    ['', '', '', ''],\n",
    "    ['Per-Class (Precision):', '', '', ''],\n",
    "]\n",
    "\n",
    "for cls in classes:\n",
    "    val_p = val_class_metrics.get(cls, 0)\n",
    "    test_p = test_class_metrics.get(cls, 0)\n",
    "    diff_p = test_p - val_p\n",
    "    summary_data.append([\n",
    "        f'  {cls}',\n",
    "        f'{val_p*100:.2f}%',\n",
    "        f'{test_p*100:.2f}%',\n",
    "        f'{diff_p*100:+.2f}%'\n",
    "    ])\n",
    "\n",
    "table = ax4.table(cellText=summary_data, cellLoc='left', loc='center',\n",
    "                 colWidths=[0.35, 0.2, 0.2, 0.2])\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1, 2.5)\n",
    "\n",
    "# Style header row\n",
    "for i in range(4):\n",
    "    table[(0, i)].set_facecolor('#4472C4')\n",
    "    table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "ax4.set_title('Teacher Model: Summary Statistics', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{cfg.work_dir}/evaluation_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"‚úÖ Visualization saved to: {cfg.work_dir}/evaluation_comparison.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92566be4",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 11: L∆∞u k·∫øt qu·∫£ ra file\n",
    "\n",
    "L∆∞u t·∫•t c·∫£ metrics v√†o CSV v√† JSON ƒë·ªÉ ph√¢n t√≠ch sau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44481ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Create results directory\n",
    "results_dir = f'{cfg.work_dir}/results'\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# 1. Save metrics to JSON\n",
    "results = {\n",
    "    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'checkpoint': checkpoint_file,\n",
    "    'validation': {\n",
    "        'teacher': val_teacher,\n",
    "        'student': val_student\n",
    "    },\n",
    "    'test': {\n",
    "        'teacher': test_teacher,\n",
    "        'student': test_student\n",
    "    }\n",
    "}\n",
    "\n",
    "json_file = f'{results_dir}/metrics_comparison.json'\n",
    "with open(json_file, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print(f\"‚úÖ Metrics saved to: {json_file}\")\n",
    "\n",
    "# 2. Save to CSV\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Class': classes + ['Overall mAP', 'Overall mAP@50', 'Overall mAP@75'],\n",
    "    'Val_Precision': list(val_class_metrics.values()) + [\n",
    "        val_teacher.get('bbox_mAP', 0),\n",
    "        val_teacher.get('bbox_mAP_50', 0),\n",
    "        val_teacher.get('bbox_mAP_75', 0)\n",
    "    ],\n",
    "    'Test_Precision': list(test_class_metrics.values()) + [\n",
    "        test_teacher.get('bbox_mAP', 0),\n",
    "        test_teacher.get('bbox_mAP_50', 0),\n",
    "        test_teacher.get('bbox_mAP_75', 0)\n",
    "    ]\n",
    "})\n",
    "comparison_df['Difference'] = comparison_df['Test_Precision'] - comparison_df['Val_Precision']\n",
    "comparison_df['Difference_%'] = (comparison_df['Difference'] / comparison_df['Val_Precision'] * 100).round(2)\n",
    "\n",
    "csv_file = f'{results_dir}/metrics_comparison.csv'\n",
    "comparison_df.to_csv(csv_file, index=False)\n",
    "print(f\"‚úÖ CSV saved to: {csv_file}\")\n",
    "\n",
    "# 3. Save full metrics to separate files\n",
    "val_metrics_file = f'{results_dir}/validation_metrics.json'\n",
    "with open(val_metrics_file, 'w') as f:\n",
    "    json.dump(metrics, f, indent=2, default=str)\n",
    "print(f\"‚úÖ Validation metrics saved to: {val_metrics_file}\")\n",
    "\n",
    "test_metrics_file = f'{results_dir}/test_metrics.json'\n",
    "with open(test_metrics_file, 'w') as f:\n",
    "    json.dump(test_metrics, f, indent=2, default=str)\n",
    "print(f\"‚úÖ Test metrics saved to: {test_metrics_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìÅ ALL RESULTS SAVED TO:\")\n",
    "print(f\"  Directory: {results_dir}\")\n",
    "print(f\"  - metrics_comparison.json\")\n",
    "print(f\"  - metrics_comparison.csv\")\n",
    "print(f\"  - validation_metrics.json\")\n",
    "print(f\"  - test_metrics.json\")\n",
    "print(f\"  - ../evaluation_comparison.png\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebf5e1e",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 12: T·ªïng k·∫øt\n",
    "\n",
    "In ra t·ªïng k·∫øt cu·ªëi c√πng v·ªÅ performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dafe43",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéØ FINAL EVALUATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìä TEACHER MODEL PERFORMANCE:\")\n",
    "print(f\"\\n  {'Metric':<20} {'Validation':>12} {'Test':>12} {'Difference':>12}\")\n",
    "print(f\"  {'-'*20} {'-'*12} {'-'*12} {'-'*12}\")\n",
    "\n",
    "key_metrics = [\n",
    "    ('mAP', 'bbox_mAP'),\n",
    "    ('mAP@50', 'bbox_mAP_50'),\n",
    "    ('mAP@75', 'bbox_mAP_75'),\n",
    "]\n",
    "\n",
    "for name, key in key_metrics:\n",
    "    val_v = val_teacher.get(key, 0)\n",
    "    test_v = test_teacher.get(key, 0)\n",
    "    diff = test_v - val_v\n",
    "    print(f\"  {name:<20} {val_v*100:>11.2f}% {test_v*100:>11.2f}% {diff*100:>+11.2f}%\")\n",
    "\n",
    "print(f\"\\nüìå PER-CLASS PRECISION:\")\n",
    "print(f\"\\n  {'Class':<15} {'Validation':>12} {'Test':>12} {'Difference':>12}\")\n",
    "print(f\"  {'-'*15} {'-'*12} {'-'*12} {'-'*12}\")\n",
    "\n",
    "for cls in classes:\n",
    "    val_p = val_class_metrics.get(cls, 0)\n",
    "    test_p = test_class_metrics.get(cls, 0)\n",
    "    diff = test_p - val_p\n",
    "    status = \"‚úÖ\" if diff >= 0 else \"‚ö†Ô∏è\"\n",
    "    print(f\"  {cls:<15} {val_p*100:>11.2f}% {test_p*100:>11.2f}% {diff*100:>+11.2f}% {status}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ EVALUATION COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Identify best and worst performing classes\n",
    "best_class = max(test_class_metrics, key=test_class_metrics.get)\n",
    "worst_class = min(test_class_metrics, key=test_class_metrics.get)\n",
    "\n",
    "print(f\"\\nüí° INSIGHTS:\")\n",
    "print(f\"  ‚Ä¢ Best performing class on test:  {best_class} ({test_class_metrics[best_class]*100:.2f}%)\")\n",
    "print(f\"  ‚Ä¢ Worst performing class on test: {worst_class} ({test_class_metrics[worst_class]*100:.2f}%)\")\n",
    "\n",
    "# Check generalization\n",
    "avg_diff = np.mean([test_class_metrics[c] - val_class_metrics[c] for c in classes])\n",
    "if abs(avg_diff) < 0.02:\n",
    "    print(f\"  ‚Ä¢ Model generalizes well! (avg diff: {avg_diff*100:+.2f}%)\")\n",
    "elif avg_diff > 0.02:\n",
    "    print(f\"  ‚Ä¢ Test performance better than validation (avg diff: {avg_diff*100:+.2f}%)\")\n",
    "else:\n",
    "    print(f\"  ‚Ä¢ Test performance worse than validation (avg diff: {avg_diff*100:+.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85494106",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 13: Visualize 8 Views c·ªßa Base Images\n",
    "\n",
    "Hi·ªÉn th·ªã 8 crops (views) c·ªßa t·ª´ng base image v·ªõi predictions v√† ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8d5fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from collections import defaultdict\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Load COCO annotation to group images\n",
    "import json\n",
    "test_ann_file = '/home/coder/data/trong/KLTN/Soft_Teacher/data_drill/anno_test/_annotations_filtered.bright.coco.json'\n",
    "with open(test_ann_file, 'r') as f:\n",
    "    coco_data = json.load(f)\n",
    "\n",
    "# Group images by base_img_id\n",
    "image_groups = defaultdict(list)\n",
    "for img in coco_data['images']:\n",
    "    # Extract base_img_id from filename\n",
    "    # Format: S245_Image__2025-11-11__12-09-08_bright_2_crop_5_jpg.rf.xxx.jpg\n",
    "    filename = img['file_name']\n",
    "    \n",
    "    # Try to extract base name (before _crop_)\n",
    "    if '_crop_' in filename:\n",
    "        base_name = filename.split('_crop_')[0]  # e.g., S245_...bright_2\n",
    "    else:\n",
    "        # Fallback: use first part\n",
    "        base_name = filename.split('_')[0]\n",
    "    \n",
    "    image_groups[base_name].append({\n",
    "        'id': img['id'],\n",
    "        'file_name': img['file_name'],\n",
    "        'width': img['width'],\n",
    "        'height': img['height']\n",
    "    })\n",
    "\n",
    "# Get annotations mapping\n",
    "annotations_by_image = defaultdict(list)\n",
    "for ann in coco_data['annotations']:\n",
    "    annotations_by_image[ann['image_id']].append(ann)\n",
    "\n",
    "print(f\"Found {len(image_groups)} base images\")\n",
    "print(f\"Total images: {sum(len(v) for v in image_groups.values())}\")\n",
    "\n",
    "# Show some examples\n",
    "for i, (base_name, imgs) in enumerate(list(image_groups.items())[:3]):\n",
    "    print(f\"\\nBase {i+1}: {base_name} ‚Üí {len(imgs)} views\")\n",
    "    for img in imgs[:2]:\n",
    "        print(f\"  - {img['file_name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09b0f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run inference and visualize\n",
    "from mmdet.apis import init_detector, inference_detector\n",
    "from mmdet.structures import DetDataSample\n",
    "import torch\n",
    "\n",
    "def visualize_8_views(base_name, image_list, data_prefix, score_threshold=0.3):\n",
    "    \"\"\"Visualize 8 crops of a base image with predictions and GT\"\"\"\n",
    "    \n",
    "    # Sort by crop number\n",
    "    def get_crop_num(filename):\n",
    "        if '_crop_' in filename:\n",
    "            try:\n",
    "                crop_part = filename.split('_crop_')[1]\n",
    "                crop_num = int(crop_part.split('_')[0])\n",
    "                return crop_num\n",
    "            except:\n",
    "                return 0\n",
    "        return 0\n",
    "    \n",
    "    image_list_sorted = sorted(image_list, key=lambda x: get_crop_num(x['file_name']))\n",
    "    \n",
    "    # Create figure for 8 views\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Class names and colors\n",
    "    class_names = ['Broken', 'Chipped', 'Scratched', 'Severe_Rust', 'Tip_Wear']\n",
    "    class_colors = [\n",
    "        (134/255, 34/255, 255/255),   # Broken - Purple\n",
    "        (0/255, 255/255, 206/255),     # Chipped - Cyan\n",
    "        (255/255, 128/255, 0/255),     # Scratched - Orange\n",
    "        (254/255, 0/255, 86/255),      # Severe_Rust - Red\n",
    "        (199/255, 252/255, 0/255)      # Tip_Wear - Yellow\n",
    "    ]\n",
    "    \n",
    "    for idx, img_info in enumerate(image_list_sorted[:8]):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Load image\n",
    "        img_path = os.path.join('/home/coder/data/trong/KLTN/Soft_Teacher/data_drill', \n",
    "                                data_prefix, img_info['file_name'])\n",
    "        \n",
    "        if not os.path.exists(img_path):\n",
    "            ax.text(0.5, 0.5, 'Image not found', ha='center', va='center')\n",
    "            ax.axis('off')\n",
    "            continue\n",
    "        \n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        ax.imshow(img)\n",
    "        \n",
    "        # Get ground truth boxes\n",
    "        gt_anns = annotations_by_image.get(img_info['id'], [])\n",
    "        \n",
    "        # Draw ground truth boxes (thick, solid)\n",
    "        for ann in gt_anns:\n",
    "            bbox = ann['bbox']  # [x, y, w, h]\n",
    "            category_id = ann['category_id'] - 1  # COCO is 1-indexed\n",
    "            \n",
    "            if 0 <= category_id < len(class_names):\n",
    "                color = class_colors[category_id]\n",
    "                rect = mpatches.Rectangle(\n",
    "                    (bbox[0], bbox[1]), bbox[2], bbox[3],\n",
    "                    linewidth=3, edgecolor=color, facecolor='none',\n",
    "                    linestyle='-', label=f'GT: {class_names[category_id]}'\n",
    "                )\n",
    "                ax.add_patch(rect)\n",
    "                \n",
    "                # Add GT label\n",
    "                ax.text(bbox[0], bbox[1] - 5, f'GT: {class_names[category_id]}',\n",
    "                       fontsize=8, color='white', weight='bold',\n",
    "                       bbox=dict(boxstyle='round,pad=0.3', facecolor=color, alpha=0.8))\n",
    "        \n",
    "        # Extract crop number\n",
    "        crop_num = get_crop_num(img_info['file_name'])\n",
    "        ax.set_title(f'View {idx+1} (Crop {crop_num})\\n{len(gt_anns)} GT boxes', \n",
    "                    fontsize=10, weight='bold')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    # Hide extra subplots if less than 8\n",
    "    for idx in range(len(image_list_sorted), 8):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    # Add legend\n",
    "    legend_elements = [\n",
    "        mpatches.Patch(color=class_colors[i], label=class_names[i]) \n",
    "        for i in range(len(class_names))\n",
    "    ]\n",
    "    fig.legend(handles=legend_elements, loc='lower center', ncol=5, \n",
    "              bbox_to_anchor=(0.5, -0.02), fontsize=11, frameon=True)\n",
    "    \n",
    "    plt.suptitle(f'8-View Visualization: {base_name}', fontsize=16, weight='bold', y=0.98)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.96])\n",
    "    \n",
    "    return fig\n",
    "\n",
    "print(\"‚úÖ Visualization function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a25884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize first 3 base images\n",
    "vis_output_dir = f'{cfg.work_dir}/8view_visualizations'\n",
    "os.makedirs(vis_output_dir, exist_ok=True)\n",
    "\n",
    "num_samples_to_visualize = 3\n",
    "data_prefix = 'test/'\n",
    "\n",
    "print(f\"Visualizing {num_samples_to_visualize} base images...\")\n",
    "print(f\"Output directory: {vis_output_dir}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, (base_name, image_list) in enumerate(list(image_groups.items())[:num_samples_to_visualize]):\n",
    "    print(f\"\\n[{i+1}/{num_samples_to_visualize}] Processing: {base_name}\")\n",
    "    print(f\"  Number of views: {len(image_list)}\")\n",
    "    \n",
    "    # Visualize\n",
    "    fig = visualize_8_views(base_name, image_list, data_prefix)\n",
    "    \n",
    "    # Save figure\n",
    "    output_path = f'{vis_output_dir}/{base_name}_8views.png'\n",
    "    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"  ‚úÖ Saved to: {output_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"‚úÖ All visualizations saved to: {vis_output_dir}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae34244",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 14: Visualize v·ªõi Model Predictions\n",
    "\n",
    "Ch·∫°y inference v√† v·∫Ω c·∫£ predictions l·∫´n ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9379866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize detector for inference\n",
    "print(\"Initializing detector for visualization...\")\n",
    "\n",
    "# Extract teacher detector from MultiViewSoftTeacher\n",
    "if hasattr(runner.model, 'teacher'):\n",
    "    detector = runner.model.teacher\n",
    "elif hasattr(runner.model, 'module') and hasattr(runner.model.module, 'teacher'):\n",
    "    detector = runner.model.module.teacher\n",
    "else:\n",
    "    detector = runner.model\n",
    "\n",
    "print(f\"Detector type: {type(detector).__name__}\")\n",
    "detector.eval()\n",
    "\n",
    "# Move to GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    detector = detector.cuda()\n",
    "    print(f\"‚úÖ Detector on GPU\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Detector on CPU\")\n",
    "\n",
    "print(\"‚úÖ Detector ready for inference!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ede7cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_8_views_with_predictions(base_name, image_list, data_prefix, \n",
    "                                       detector, score_threshold=0.3):\n",
    "    \"\"\"Visualize 8 crops: GT on top row, Predictions on bottom row for each view\"\"\"\n",
    "    \n",
    "    # Sort by crop number\n",
    "    def get_crop_num(filename):\n",
    "        if '_crop_' in filename:\n",
    "            try:\n",
    "                crop_part = filename.split('_crop_')[1]\n",
    "                crop_num = int(crop_part.split('_')[0])\n",
    "                return crop_num\n",
    "            except:\n",
    "                return 0\n",
    "        return 0\n",
    "    \n",
    "    image_list_sorted = sorted(image_list, key=lambda x: get_crop_num(x['file_name']))\n",
    "    \n",
    "    # Create figure with 2 rows per view: GT (top), Pred (bottom)\n",
    "    # Total: 2 rows √ó 8 views = 16 subplots\n",
    "    fig, axes = plt.subplots(2, 8, figsize=(28, 8))\n",
    "    \n",
    "    # Class names and colors\n",
    "    class_names = ['Broken', 'Chipped', 'Scratched', 'Severe_Rust', 'Tip_Wear']\n",
    "    class_colors = [\n",
    "        (134/255, 34/255, 255/255),   # Broken\n",
    "        (0/255, 255/255, 206/255),     # Chipped\n",
    "        (255/255, 128/255, 0/255),     # Scratched\n",
    "        (254/255, 0/255, 86/255),      # Severe_Rust\n",
    "        (199/255, 252/255, 0/255)      # Tip_Wear\n",
    "    ]\n",
    "    \n",
    "    from mmdet.apis import inference_detector\n",
    "    \n",
    "    for idx, img_info in enumerate(image_list_sorted[:8]):\n",
    "        # Get axes for this view\n",
    "        ax_gt = axes[0, idx]    # Top row: Ground Truth\n",
    "        ax_pred = axes[1, idx]  # Bottom row: Predictions\n",
    "        \n",
    "        # Load image\n",
    "        img_path = os.path.join('/home/coder/data/trong/KLTN/Soft_Teacher/data_drill', \n",
    "                                data_prefix, img_info['file_name'])\n",
    "        \n",
    "        if not os.path.exists(img_path):\n",
    "            ax_gt.text(0.5, 0.5, 'Image not found', ha='center', va='center')\n",
    "            ax_gt.axis('off')\n",
    "            ax_pred.text(0.5, 0.5, 'Image not found', ha='center', va='center')\n",
    "            ax_pred.axis('off')\n",
    "            continue\n",
    "        \n",
    "        img = cv2.imread(img_path)\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Display image on both subplots\n",
    "        ax_gt.imshow(img_rgb)\n",
    "        ax_pred.imshow(img_rgb)\n",
    "        \n",
    "        # Get ground truth boxes\n",
    "        gt_anns = annotations_by_image.get(img_info['id'], [])\n",
    "        \n",
    "        # ===== TOP ROW: Draw Ground Truth only =====\n",
    "        for ann in gt_anns:\n",
    "            bbox = ann['bbox']  # [x, y, w, h]\n",
    "            category_id = ann['category_id'] - 1\n",
    "            \n",
    "            if 0 <= category_id < len(class_names):\n",
    "                color = class_colors[category_id]\n",
    "                rect = mpatches.Rectangle(\n",
    "                    (bbox[0], bbox[1]), bbox[2], bbox[3],\n",
    "                    linewidth=3, edgecolor=color, facecolor='none',\n",
    "                    linestyle='-'\n",
    "                )\n",
    "                ax_gt.add_patch(rect)\n",
    "                \n",
    "                # GT label\n",
    "                ax_gt.text(bbox[0], bbox[1] - 5, class_names[category_id],\n",
    "                          fontsize=8, color='white', weight='bold',\n",
    "                          bbox=dict(boxstyle='round,pad=0.3', \n",
    "                                  facecolor=color, alpha=0.9))\n",
    "        \n",
    "        crop_num = get_crop_num(img_info['file_name'])\n",
    "        ax_gt.set_title(f'View {idx+1} (Crop {crop_num}) - GT: {len(gt_anns)} boxes', \n",
    "                       fontsize=10, weight='bold', color='darkgreen')\n",
    "        ax_gt.axis('off')\n",
    "        \n",
    "        # ===== BOTTOM ROW: Run inference and draw Predictions =====\n",
    "        with torch.no_grad():\n",
    "            result = inference_detector(detector, img_path)\n",
    "        \n",
    "        # Extract predictions\n",
    "        pred_instances = result.pred_instances\n",
    "        pred_bboxes = pred_instances.bboxes.cpu().numpy()\n",
    "        pred_scores = pred_instances.scores.cpu().numpy()\n",
    "        pred_labels = pred_instances.labels.cpu().numpy()\n",
    "        \n",
    "        # Draw predictions\n",
    "        num_preds = 0\n",
    "        for bbox, score, label in zip(pred_bboxes, pred_scores, pred_labels):\n",
    "            if score >= score_threshold:\n",
    "                x1, y1, x2, y2 = bbox\n",
    "                w, h = x2 - x1, y2 - y1\n",
    "                \n",
    "                if 0 <= label < len(class_names):\n",
    "                    color = class_colors[label]\n",
    "                    rect = mpatches.Rectangle(\n",
    "                        (x1, y1), w, h,\n",
    "                        linewidth=2.5, edgecolor=color, facecolor='none',\n",
    "                        linestyle='-', alpha=0.9\n",
    "                    )\n",
    "                    ax_pred.add_patch(rect)\n",
    "                    \n",
    "                    # Prediction label with score\n",
    "                    ax_pred.text(x1, y1 - 5, f'{class_names[label]}: {score:.2f}',\n",
    "                               fontsize=8, color='white', weight='bold',\n",
    "                               bbox=dict(boxstyle='round,pad=0.3', \n",
    "                                       facecolor=color, alpha=0.8))\n",
    "                    num_preds += 1\n",
    "        \n",
    "        ax_pred.set_title(f'View {idx+1} (Crop {crop_num}) - Pred: {num_preds} boxes (‚â•{score_threshold})', \n",
    "                         fontsize=10, weight='bold', color='darkblue')\n",
    "        ax_pred.axis('off')\n",
    "    \n",
    "    # Hide extra subplots if less than 8\n",
    "    for idx in range(len(image_list_sorted), 8):\n",
    "        axes[0, idx].axis('off')\n",
    "        axes[1, idx].axis('off')\n",
    "    \n",
    "    # Create legend\n",
    "    legend_elements = []\n",
    "    \n",
    "    # Add row labels\n",
    "    from matplotlib.lines import Line2D\n",
    "    legend_elements.append(Line2D([0], [0], color='darkgreen', linewidth=0, \n",
    "                                  marker='s', markersize=10, \n",
    "                                  label='‚ñ† Top Row: Ground Truth'))\n",
    "    legend_elements.append(Line2D([0], [0], color='darkblue', linewidth=0, \n",
    "                                  marker='s', markersize=10,\n",
    "                                  label=f'‚ñ† Bottom Row: Predictions (score ‚â• {score_threshold})'))\n",
    "    legend_elements.append(Line2D([0], [0], color='white', linewidth=0, label=''))  # Spacer\n",
    "    \n",
    "    # Add class colors\n",
    "    for i, name in enumerate(class_names):\n",
    "        legend_elements.append(\n",
    "            mpatches.Patch(color=class_colors[i], label=name)\n",
    "        )\n",
    "    \n",
    "    fig.legend(handles=legend_elements, loc='lower center', ncol=8,\n",
    "              bbox_to_anchor=(0.5, -0.02), fontsize=11, frameon=True,\n",
    "              columnspacing=1.5)\n",
    "    \n",
    "    plt.suptitle(f'8-View Comparison: {base_name}\\n(Top: Ground Truth | Bottom: Predictions)', \n",
    "                fontsize=16, weight='bold', y=0.98)\n",
    "    plt.tight_layout(rect=[0, 0.05, 1, 0.95])\n",
    "    \n",
    "    return fig\n",
    "\n",
    "print(\"‚úÖ Updated visualization function ready! (GT on top, Predictions on bottom)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d32d673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize with predictions\n",
    "vis_pred_dir = f'{cfg.work_dir}/8view_predictions'\n",
    "os.makedirs(vis_pred_dir, exist_ok=True)\n",
    "\n",
    "num_samples = 3\n",
    "score_threshold = 0.3\n",
    "\n",
    "print(f\"Visualizing {num_samples} base images with predictions...\")\n",
    "print(f\"Score threshold: {score_threshold}\")\n",
    "print(f\"Output directory: {vis_pred_dir}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, (base_name, image_list) in enumerate(list(image_groups.items())[:num_samples]):\n",
    "    print(f\"\\n[{i+1}/{num_samples}] Processing: {base_name}\")\n",
    "    print(f\"  Number of views: {len(image_list)}\")\n",
    "    \n",
    "    try:\n",
    "        # Visualize with predictions\n",
    "        fig = visualize_8_views_with_predictions(\n",
    "            base_name, image_list, data_prefix, \n",
    "            detector, score_threshold\n",
    "        )\n",
    "        \n",
    "        # Save\n",
    "        output_path = f'{vis_pred_dir}/{base_name}_predictions.png'\n",
    "        plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"  ‚úÖ Saved to: {output_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error: {e}\")\n",
    "        continue\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"‚úÖ All prediction visualizations saved to: {vis_pred_dir}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcdfce8",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 15: Ch·ªçn base image c·ª• th·ªÉ ƒë·ªÉ visualize\n",
    "\n",
    "Nh·∫≠p base_name ho·∫∑c index ƒë·ªÉ xem chi ti·∫øt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52d91ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all available base images\n",
    "print(\"Available base images:\")\n",
    "print(\"=\"*80)\n",
    "base_names_list = list(image_groups.keys())\n",
    "for i, base_name in enumerate(base_names_list[:20]):  # Show first 20\n",
    "    num_views = len(image_groups[base_name])\n",
    "    num_boxes = sum(len(annotations_by_image.get(img['id'], [])) \n",
    "                   for img in image_groups[base_name])\n",
    "    print(f\"[{i:2d}] {base_name:<50} ‚Üí {num_views} views, {num_boxes} GT boxes\")\n",
    "\n",
    "if len(base_names_list) > 20:\n",
    "    print(f\"\\n... and {len(base_names_list) - 20} more\")\n",
    "\n",
    "print(f\"\\nTotal: {len(base_names_list)} base images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b8de30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize specific base image by index\n",
    "# Change this index to visualize different base images\n",
    "selected_index = 0  # Change this: 0, 1, 2, 3, ... \n",
    "\n",
    "if 0 <= selected_index < len(base_names_list):\n",
    "    selected_base = base_names_list[selected_index]\n",
    "    selected_images = image_groups[selected_base]\n",
    "    \n",
    "    print(f\"Selected base image: {selected_base}\")\n",
    "    print(f\"Number of views: {len(selected_images)}\")\n",
    "    \n",
    "    # Visualize\n",
    "    fig = visualize_8_views_with_predictions(\n",
    "        selected_base, selected_images, data_prefix,\n",
    "        detector, score_threshold=0.3\n",
    "    )\n",
    "    \n",
    "    # Save\n",
    "    output_path = f'{vis_pred_dir}/{selected_base}_selected.png'\n",
    "    plt.savefig(output_path, dpi=200, bbox_inches='tight')\n",
    "    print(f\"‚úÖ Saved to: {output_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"‚ùå Invalid index! Choose between 0 and {len(base_names_list)-1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
