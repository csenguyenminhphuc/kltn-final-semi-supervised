# üî¥ PH√ÇN T√çCH V·∫§N ƒê·ªÄ NGHI√äM TR·ªåNG - Multi-View Soft Teacher

**Ng√†y ph√¢n t√≠ch:** 03/12/2025  
**Tr·∫°ng th√°i:** Model train ƒë∆∞·ª£c (loss gi·∫£m) nh∆∞ng **mAP = 0.0** ho√†n to√†n

---

## üìä HI·ªÜN T∆Ø·ª¢NG QUAN S√ÅT

### 1. **K·∫øt Qu·∫£ ƒê√°nh Gi√° (Epoch 0)**
```
+-------------+-----+--------+--------+-------+-------+-------+
| category    | mAP | mAP_50 | mAP_75 | mAP_s | mAP_m | mAP_l |
+-------------+-----+--------+--------+-------+-------+-------+
| Broken      | 0.0 | 0.0    | 0.0    | nan   | nan   | 0.0   |
| Chipped     | 0.0 | 0.0    | 0.0    | nan   | 0.0   | 0.0   |
| Scratched   | 0.0 | 0.0    | 0.0    | nan   | 0.0   | 0.0   |
| Severe_Rust | 0.0 | 0.0    | 0.0    | nan   | 0.0   | 0.0   |
| Tip_Wear    | 0.0 | 0.0    | 0.0    | nan   | 0.0   | 0.0   |
+-------------+-----+--------+--------+-------+-------+-------+
```

### 2. **Teacher Predictions (B·∫•t Th∆∞·ªùng)**
```
[Teacher Predictions] Total boxes: 800
Score range: [0.999, 1.000], Mean: 1.000, Median: 1.000
[After Filtering] Threshold: 0.7, Kept: 120/800 (15.0%)
```

### 3. **Training Loss Values (Iter 50)**
```
loss: 4.6771

Supervised Branch:
  loss_rpn_cls: 0.2659
  loss_rpn_bbox: 0.5833
  loss_cls: 2.0261 ‚Üê R·∫§T CAO!
  loss_bbox: 1.3502
  acc_fg: 8.5714% ‚Üê C·ª∞C TH·∫§P!
  acc_bg: 60.0000%

Unsupervised Branch:
  loss_rpn_cls: 0.2000
  loss_rpn_bbox: 0.8338
  loss_cls: 1.5117
  loss_bbox: 0.6804
  acc_fg: 86.0360% ‚Üê CAO B·∫§T TH∆Ø·ªúNG!
  acc_bg: 48.6486%
```

---

## üîç PH√ÇN T√çCH G·ªêC R·ªÑ V·∫§N ƒê·ªÄ

### **V·∫§N ƒê·ªÄ 1: Teacher Model Output Confidence = 1.0**

**Hi·ªán t∆∞·ª£ng:**
```python
Score range: [0.999, 1.000], Mean: 1.000
```

**Nguy√™n nh√¢n:**
1. **Init kh√¥ng ƒë√∫ng:** Teacher ƒë∆∞·ª£c copy t·ª´ Student ban ƒë·∫ßu
2. **Activation function:** Sigmoid/Softmax v·ªõi logits qu√° l·ªõn ‚Üí confidence = 1.0
3. **EMA update kh√¥ng ho·∫°t ƒë·ªông ƒë√∫ng:** Teacher kh√¥ng ƒë∆∞·ª£c c·∫≠p nh·∫≠t d·∫ßn d·∫ßn

**H·∫≠u qu·∫£:**
- Pseudo-labels c√≥ confidence = 1.0 ‚Üí model overfit v√†o pseudo-labels SAI
- Unsupervised acc_fg = 86% cao b·∫•t th∆∞·ªùng v√¨ model "ch·∫Øc ch·∫Øn" v·ªÅ pseudo-labels sai

**C√°ch ki·ªÉm tra:**
```python
# In mmdet/models/detectors/soft_teacher.py
@torch.no_grad()
def get_pseudo_instances():
    # Add debug
    print(f"Teacher cls scores BEFORE sigmoid: {cls_logits.min():.3f} to {cls_logits.max():.3f}")
    print(f"Teacher cls scores AFTER sigmoid: {cls_scores.min():.3f} to {cls_scores.max():.3f}")
```

**FIX:**
```python
# Option 1: Temperature scaling for teacher predictions
teacher_temperature = 2.0  # Smooth confidence distribution
cls_scores = cls_logits.sigmoid() / teacher_temperature

# Option 2: Clip logits before sigmoid
cls_logits = torch.clamp(cls_logits, min=-10, max=10)
cls_scores = cls_logits.sigmoid()

# Option 3: Add noise to teacher predictions
cls_scores = cls_scores + torch.randn_like(cls_scores) * 0.05
cls_scores = torch.clamp(cls_scores, 0, 1)
```

---

### **V·∫§N ƒê·ªÄ 2: Supervised Accuracy C·ª±c Th·∫•p (8.57%)**

**Hi·ªán t∆∞·ª£ng:**
```
sup_acc_fg: 8.5714%  ‚Üê Ch·ªâ 8.57% foreground d·ª± ƒëo√°n ƒë√∫ng!
sup_loss_cls: 2.0261  ‚Üê Loss classification r·∫•t cao
```

**Nguy√™n nh√¢n c√≥ th·ªÉ:**

#### A. **Class Imbalance Nghi√™m Tr·ªçng**
```python
# Your config
num_classes = 5  # Broken, Chipped, Scratched, Severe_Rust, Tip_Wear
```

Ki·ªÉm tra distribution:
```bash
# Count labels in annotation file
grep -o '"category_id": [0-9]*' data_drill/anno_train/_annotations.coco.json | sort | uniq -c
```

N·∫øu c√≥ 1 class chi·∫øm >80% ‚Üí Focal Loss kh√¥ng ƒë·ªß m·∫°nh

**FIX:**
```python
# In config: TƒÉng gamma c·ªßa Focal Loss
loss_cls=dict(
    type='FocalLoss',
    use_sigmoid=True,
    gamma=3.0,  # TƒÉng t·ª´ 2.0 ‚Üí 3.0 (focus h∆°n v√†o hard examples)
    alpha=0.25,
    loss_weight=1.0
)

# Ho·∫∑c d√πng Class-Balanced Focal Loss
loss_cls=dict(
    type='CrossEntropyLoss',
    use_sigmoid=True,
    class_weight=[1.0, 2.0, 1.5, 3.0, 2.5],  # Weight theo inverse frequency
    loss_weight=1.0
)
```

#### B. **Learning Rate Qu√° Th·∫•p**
```python
lr: 1.9704e-05  # = 0.00001970 (R·∫§T TH·∫§P!)
```

**FIX:**
```python
# In config
optim_wrapper = dict(
    type='OptimWrapper',
    optimizer=dict(type='SGD', lr=0.001, momentum=0.9, weight_decay=0.0001)
    # Th·ª≠ lr=0.001 thay v√¨ 0.00002
)

# Ho·∫∑c d√πng AdamW
optim_wrapper = dict(
    optimizer=dict(type='AdamW', lr=0.0001, weight_decay=0.05)
)
```

#### C. **MVViT Attention L√†m Nhi·ªÖu Features**

**Gi·∫£ thuy·∫øt:** MVViT cross-view attention ƒëang "tr·ªôn l·∫´n" features t·ª´ 8 crops ‚Üí features b·ªã nhi·ªÖu

**Ki·ªÉm tra:**
```python
# Trong MultiViewBackbone.forward()
# Th√™m debug
if self.fusion == 'mvvit':
    feats_before = feats  # Features t·ª´ ResNet
    feats_refined = self.mvvit(...)  # Features sau MVViT
    
    # Check difference
    diff = (feats_refined - feats_before).abs().mean()
    print(f"[DEBUG] MVViT change magnitude: {diff:.6f}")
    
    # If diff > 0.5 ‚Üí MVViT ƒëang thay ƒë·ªïi qu√° m·∫°nh!
```

**FIX:**
```python
# Option 1: Gi·∫£m learning rate c·ªßa MVViT
mvvit=dict(
    type='MVViT',
    # ... other params ...
    lr_multiplier=0.1  # MVViT h·ªçc ch·∫≠m h∆°n backbone
)

# Option 2: Residual connection m·∫°nh h∆°n
# Trong multi_view_transformer.py
refined = 0.9 * original + 0.1 * refined  # Thay v√¨ 0.5/0.5

# Option 3: T·∫Øt MVViT t·∫°m th·ªùi ƒë·ªÉ test
detector.backbone = dict(
    type='MultiViewBackbone',
    backbone=dict(type='ResNet', ...),
    fusion='mean',  # Thay v√¨ 'mvvit'
    views_per_sample=8
)
# ‚Üí N·∫øu mAP tƒÉng ‚Üí MVViT l√† nguy√™n nh√¢n!
```

---

### **V·∫§N ƒê·ªÄ 3: mAP = 0.0 Ho√†n To√†n**

**Nguy√™n nh√¢n c√≥ th·ªÉ:**

#### A. **NMS/Score Threshold Qu√° Cao**
```python
# Check trong config
nms=dict(type='nms', iou_threshold=0.5),
score_thr=0.05  # C√≥ th·ªÉ qu√° cao?
```

**FIX:**
```python
# Gi·∫£m threshold xu·ªëng
test_cfg=dict(
    rcnn=dict(
        score_thr=0.001,  # T·ª´ 0.05 ‚Üí 0.001
        nms=dict(type='nms', iou_threshold=0.7),  # T·ª´ 0.5 ‚Üí 0.7
        max_per_img=100
    )
)
```

#### B. **Bounding Box ·ªü Sai Coordinate Space**

**V·∫•n ƒë·ªÅ:** Predictions ƒë∆∞·ª£c crop coordinates, nh∆∞ng evaluation ·ªü base image coordinates

**Ki·ªÉm tra:**
```python
# Trong evaluation log
[BBOX DEBUG] File: S110_bright_2_crop_1, Image shape: 720x256
  ‚Üí Loaded 1 boxes from COCO JSON
    Box 0: [25, 492, 69.894, 622.718], label: 1

# Check: Box width = 69.894-25 = 44.894, height = 622.718-492 = 130.718
# Nh∆∞ng image width = 720, height = 256
# ‚Üí Height 130.718 < 256 ‚úì, nh∆∞ng x-coord 622.718 > 720?! üî¥
```

**PH√ÅT HI·ªÜN:** Bounding box coordinates **V·ªöT KH·ªéI IMAGE**!

**Nguy√™n nh√¢n:**
- Annotation c√≥ th·ªÉ ·ªü base image coords (1080√ó2560)
- Nh∆∞ng crop size l√† 256√ó720
- Transform kh√¥ng ƒë√∫ng!

**FIX:**
```python
# Ki·ªÉm tra trong MultiViewFromFolder._load_annotations()
def _clip_bbox_to_image(self, bbox, img_width, img_height):
    """Clip bbox to image boundaries."""
    x1, y1, x2, y2 = bbox
    x1 = max(0, min(x1, img_width))
    x2 = max(0, min(x2, img_width))
    y1 = max(0, min(y1, img_height))
    y2 = max(0, min(y2, img_height))
    return [x1, y1, x2, y2]

# Apply in load_data_list()
for bbox in gt_bboxes:
    bbox = self._clip_bbox_to_image(bbox, img_width, img_height)
```

#### C. **Evaluation Mode Sai**

Log shows:
```
[Info] Evaluation mode: Per-view (no aggregation)
  - Total views evaluated: 640
```

**V·∫•n ƒë·ªÅ:** ƒêang evaluate t·ª´ng crop ri√™ng bi·ªát, kh√¥ng aggregate v·ªÅ base image!

**FIX:**
```python
# Trong config, thay ƒë·ªïi evaluation metric
val_evaluator = dict(
    type='MultiViewCocoMetric',
    views_per_sample=8,
    aggregate_predictions=True,  # ‚Üê PH·∫¢I B·∫¨T!
    aggregation_method='wbf',
    metric='bbox',
    format_only=False,
    ann_file='data_drill/anno_valid/_annotations_filtered.coco.json'
)
```

---

### **V·∫§N ƒê·ªÄ 4: Annotations C√≥ Th·ªÉ Sai**

**Observation t·ª´ log:**
```
[BBOX DEBUG] File: ...bright_4_crop_1.jpg, Image shape: 720x256
  ‚Üí Loaded 1 boxes
    Box 0: [89, 0, 153.76, 238.57], label: 2
```

**Ki·ªÉm tra:**
```bash
# Visualize 1 crop v·ªõi GT bbox
python tools/analysis_tools/browse_dataset.py \
  configs/soft_teacher/soft_teacher_custom_multi_view.py \
  --output-dir vis_check \
  --not-show

# Check xem bbox c√≥ ƒë√∫ng kh√¥ng
```

**V·∫•n ƒë·ªÅ ti·ªÅm ·∫©n:**
1. **Filtered annotations** (`_annotations_filtered.coco.json`) c√≥ th·ªÉ b·ªã l·ªói
2. **base_img_id mapping** c√≥ th·ªÉ sai
3. **Crop coordinates** kh√¥ng match v·ªõi bbox coordinates

**FIX:**
```bash
# Quay l·∫°i d√πng original annotations
ann_file='data_drill/anno_train/_annotations.coco.json'  # Kh√¥ng d√πng _filtered

# Ho·∫∑c regenerate filtered file
python tools/misc/regenerate_filtered_annotations.py
```

---

### **V·∫§N ƒê·ªÄ 5: MVViT Capacity Kh√¥ng Ph√π H·ª£p**

**Config hi·ªán t·∫°i:**
```python
mvvit=dict(
    embed_dim=256,
    num_heads=4,     # 4 heads
    num_layers=1,    # 1 layer - QU√Å √çT!
    mlp_ratio=2.0,   # MLP dim = 512
    spatial_attention='moderate'  # 512 tokens/view
)
```

**V·∫•n ƒë·ªÅ:**
- **1 layer qu√° √≠t** ‚Üí kh√¥ng ƒë·ªß capacity h·ªçc cross-view relationships
- **4 heads √≠t** ‚Üí attention patterns h·∫°n ch·∫ø
- **mlp_ratio=2.0 th·∫•p** ‚Üí bottleneck trong feedforward

**FIX:**
```python
mvvit=dict(
    embed_dim=256,
    num_heads=8,      # 4 ‚Üí 8 heads (more attention patterns)
    num_layers=2,     # 1 ‚Üí 2 layers (deeper learning)
    mlp_ratio=4.0,    # 2.0 ‚Üí 4.0 (standard transformer ratio)
    dropout=0.1,
    spatial_attention='moderate'
)

# Note: TƒÉng capacity c√≥ th·ªÉ c·∫ßn th√™m regularization
# Add dropout, weight decay
```

---

### **V·∫§N ƒê·ªÄ 6: Data Augmentation Qu√° M·∫°nh?**

**Supervised branch c√≥ accuracy th·∫•p** ‚Üí c√≥ th·ªÉ augmentation l√†m kh√≥ data qu√° m·ª©c

**Check config:**
```python
# Strong augmentation for student
dict(type='RandAugment', ...)
dict(type='RandomErasing', ...)
```

**FIX:**
```python
# Gi·∫£m augmentation trong supervised branch
sup_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(type='RandomFlip', prob=0.5),  # Ch·ªâ flip
    # B·ªè RandAugment, RandomErasing
    dict(type='PackDetInputs')
]
```

---

## üõ†Ô∏è H√ÄNH ƒê·ªòNG KH·∫ÆC PH·ª§C (∆ØU TI√äN)

### **Priority 1: FIX NGAY** (Critical)

#### 1.1. **Clip Bounding Boxes**
```python
# File: mmdet/datasets/wrappers/multi_view_from_folder.py
# Th√™m v√†o load_data_list()

def _clip_bbox(bbox, width, height):
    x1, y1, x2, y2 = bbox
    x1 = np.clip(x1, 0, width)
    x2 = np.clip(x2, 0, width)
    y1 = np.clip(y1, 0, height)
    y2 = np.clip(y2, 0, height)
    # Remove invalid boxes
    if x2 <= x1 or y2 <= y1:
        return None
    return [x1, y1, x2, y2]

# Apply to all GT boxes
gt_bboxes_filtered = []
for bbox in gt_bboxes:
    clipped = _clip_bbox(bbox, img_width, img_height)
    if clipped is not None:
        gt_bboxes_filtered.append(clipped)
```

#### 1.2. **Gi·∫£m Score Threshold**
```python
# In config
test_cfg=dict(
    rcnn=dict(
        score_thr=0.001,  # ‚Üê T·ª´ 0.05 xu·ªëng
        nms=dict(type='nms', iou_threshold=0.7),
        max_per_img=100
    )
)
```

#### 1.3. **B·∫≠t Aggregation trong Evaluation**
```python
val_evaluator = dict(
    type='MultiViewCocoMetric',
    views_per_sample=8,
    aggregate_predictions=True,  # ‚Üê B·∫¨T
    aggregation_method='wbf',
    # ...
)
```

### **Priority 2: EXPERIMENT** (Test individually)

#### 2.1. **Test WITHOUT MVViT**
```python
detector.backbone = dict(
    type='MultiViewBackbone',
    backbone=dict(type='ResNet', depth=50, ...),
    fusion='mean',  # ‚Üê T·∫Øt MVViT
    views_per_sample=8
)
```
‚Üí Train 1000 iterations, check mAP  
‚Üí N·∫øu mAP > 0 ‚Üí MVViT l√† v·∫•n ƒë·ªÅ  
‚Üí N·∫øu mAP = 0 ‚Üí V·∫•n ƒë·ªÅ ·ªü ch·ªó kh√°c

#### 2.2. **TƒÉng Learning Rate**
```python
optimizer=dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)
# T·ª´ 0.00002 ‚Üí 0.01 (x500)
```

#### 2.3. **Gi·∫£m Augmentation**
```python
# Supervised pipeline: ch·ªâ flip
# Unsupervised student: ch·ªâ ColorJitter + flip
```

#### 2.4. **Fix Teacher Temperature**
```python
# In soft_teacher.py, get_pseudo_instances()
cls_scores = (cls_logits / 2.0).sigmoid()  # Temperature = 2.0
```

### **Priority 3: DEEP DIVE** (N·∫øu v·∫´n mAP=0)

#### 3.1. **Visualize Predictions**
```python
python tools/analysis_tools/visualize_predictions.py \
  --config configs/soft_teacher/soft_teacher_custom_multi_view.py \
  --checkpoint work_dirs/.../latest.pth \
  --img-dir data_drill/valid \
  --output vis_pred
```

#### 3.2. **Check Annotation Correctness**
```python
python tools/analysis_tools/browse_dataset.py \
  configs/soft_teacher/soft_teacher_custom_multi_view.py \
  --phase val \
  --output vis_gt
```

#### 3.3. **Profile Forward Pass**
```python
# Check if MVViT nans/infs
with torch.autograd.detect_anomaly():
    loss = model(batch)
```

---

## üìã CHECKLIST DEBUG

```
‚ñ° Bbox coordinates clipped to image boundaries
‚ñ° Score threshold lowered (0.05 ‚Üí 0.001)
‚ñ° Evaluation aggregation enabled
‚ñ° Teacher confidence not = 1.0
‚ñ° Learning rate reasonable (>0.0001)
‚ñ° Visualized GT annotations (correct?)
‚ñ° Visualized predictions (exist?)
‚ñ° Test without MVViT (isolate issue)
‚ñ° Check for NaN/Inf in loss
‚ñ° Verify annotation file correctness
```

---

## üéØ K·∫æT LU·∫¨N

**V·∫•n ƒë·ªÅ ch√≠nh:**
1. **Bounding boxes v∆∞·ª£t kh·ªèi image boundaries** ‚Üí c·∫ßn clip
2. **Teacher predictions = 1.0** ‚Üí c·∫ßn temperature scaling
3. **Evaluation kh√¥ng aggregate** ‚Üí c·∫ßn b·∫≠t WBF
4. **Score threshold qu√° cao** ‚Üí c·∫ßn gi·∫£m
5. **MVViT c√≥ th·ªÉ l√†m nhi·ªÖu** ‚Üí c·∫ßn test without

**Kh·∫£ nƒÉng cao nh·∫•t:**
- **70%**: Bbox coordinates sai + score threshold cao
- **20%**: MVViT l√†m nhi·ªÖu features
- **10%**: Learning rate qu√° th·∫•p

**Next Steps:**
1. Fix bbox clipping (5 ph√∫t)
2. Gi·∫£m score threshold (1 ph√∫t)
3. B·∫≠t aggregation (1 ph√∫t)
4. Train th√™m 1000 iterations
5. Check mAP ‚Üí n·∫øu v·∫´n 0 ‚Üí test without MVViT

---

**Generated:** 2025-12-03  
**Author:** GitHub Copilot Analysis  
**Status:** CRITICAL - C·∫ßn fix ngay ƒë·ªÉ model c√≥ th·ªÉ detect ƒë∆∞·ª£c objects
