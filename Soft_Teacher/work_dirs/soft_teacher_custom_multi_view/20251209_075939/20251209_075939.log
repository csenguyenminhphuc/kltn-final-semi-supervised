2025/12/09 07:59:40 - mmengine - INFO - 
------------------------------------------------------------
System environment:
    sys.platform: linux
    Python: 3.9.19 (main, May  6 2024, 19:43:03) [GCC 11.2.0]
    CUDA available: True
    MUSA available: False
    numpy_random_seed: 1653551698
    GPU 0: NVIDIA RTX A6000
    CUDA_HOME: /usr/local/cuda
    NVCC: Cuda compilation tools, release 12.6, V12.6.85
    GCC: gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
    PyTorch: 2.0.0+cu117
    PyTorch compiling details: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

    TorchVision: 0.15.1+cu117
    OpenCV: 4.11.0
    MMEngine: 0.11.0rc0

Runtime environment:
    cudnn_benchmark: False
    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}
    dist_cfg: {'backend': 'nccl'}
    seed: 1653551698
    Distributed launcher: none
    Distributed training: False
    GPU number: 1
------------------------------------------------------------

2025/12/09 07:59:43 - mmengine - INFO - Config:
backend_args = None
batch_size = 2
branch_field = [
    'sup',
    'unsup_teacher',
    'unsup_student',
]
color_space = [
    [
        dict(prob=0.5, type='AutoContrast'),
    ],
    [
        dict(prob=0.4, type='Equalize'),
    ],
    [
        dict(level=7, max_mag=1.25, min_mag=0.8, prob=0.6, type='Brightness'),
    ],
    [
        dict(level=7, max_mag=1.3, min_mag=0.8, prob=0.6, type='Contrast'),
    ],
    [
        dict(level=6, max_mag=1.5, min_mag=0.8, prob=0.4, type='Sharpness'),
    ],
    [
        dict(level=5, max_mag=7.0, min_mag=5.0, prob=0.2, type='Posterize'),
    ],
]
custom_hooks = [
    dict(momentum=0.999, type='MeanTeacherHook'),
]
custom_imports = dict(
    allow_failed_imports=False,
    imports=[
        'mmdet.models.utils.multi_view',
        'mmdet.models.utils.multi_view_transformer',
        'mmdet.datasets.wrappers.multi_view_from_folder',
    ])
data_root = '/home/coder/data/trong/KLTN/Soft_Teacher/data_drill/'
dataset_type = 'CocoDataset'
default_hooks = dict(
    checkpoint=dict(
        interval=1,
        max_keep_ckpts=1,
        rule='greater',
        save_best='teacher/coco/bbox_mAP_50',
        type='CheckpointHook'),
    logger=dict(interval=50, type='LoggerHook'),
    param_scheduler=dict(type='ParamSchedulerHook'),
    sampler_seed=dict(type='DistSamplerSeedHook'),
    timer=dict(type='IterTimerHook'),
    visualization=dict(type='DetVisualizationHook'))
default_scope = 'mmdet'
detector = dict(
    backbone=dict(
        backbone=dict(
            depth=50,
            frozen_stages=1,
            init_cfg=dict(
                checkpoint='open-mmlab://detectron2/resnet50_caffe',
                type='Pretrained'),
            norm_cfg=dict(requires_grad=False, type='BN'),
            norm_eval=True,
            num_stages=4,
            out_indices=(
                0,
                1,
                2,
                3,
            ),
            style='caffe',
            type='ResNet'),
        concat_reduce=False,
        fusion='mvvit',
        mvvit=dict(
            dropout=0.2,
            embed_dim=256,
            mlp_ratio=2.0,
            num_heads=4,
            num_layers=1,
            spatial_attention='moderate',
            type='MVViT',
            use_gradient_checkpointing=True,
            use_layer_norm=True),
        type='MultiViewBackbone',
        views_per_sample=8),
    data_preprocessor=dict(
        bgr_to_rgb=False,
        mean=[
            103.53,
            116.28,
            123.675,
        ],
        pad_size_divisor=8,
        std=[
            1.0,
            1.0,
            1.0,
        ],
        type='DetDataPreprocessor'),
    neck=dict(
        in_channels=[
            256,
            512,
            1024,
            2048,
        ],
        num_outs=5,
        out_channels=256,
        type='FPN'),
    roi_head=dict(
        bbox_head=dict(
            bbox_coder=dict(
                target_means=[
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                ],
                target_stds=[
                    0.1,
                    0.1,
                    0.2,
                    0.2,
                ],
                type='DeltaXYWHBBoxCoder'),
            fc_out_channels=1024,
            in_channels=256,
            loss_bbox=dict(loss_weight=1.0, type='L1Loss'),
            loss_cls=dict(
                alpha=0.5,
                gamma=2.2,
                loss_weight=1.5,
                type='FocalLoss',
                use_sigmoid=True),
            num_classes=5,
            reg_class_agnostic=False,
            roi_feat_size=7,
            type='Shared2FCBBoxHead'),
        bbox_roi_extractor=dict(
            featmap_strides=[
                4,
                8,
                16,
                32,
            ],
            out_channels=256,
            roi_layer=dict(output_size=7, sampling_ratio=0, type='RoIAlign'),
            type='SingleRoIExtractor'),
        type='StandardRoIHead'),
    rpn_head=dict(
        anchor_generator=dict(
            ratios=[
                0.3,
                0.45,
                0.6,
                1.0,
                2.0,
            ],
            scales=[
                8,
                16,
                32,
            ],
            strides=[
                4,
                8,
                16,
                32,
                64,
            ],
            type='AnchorGenerator'),
        bbox_coder=dict(
            target_means=[
                0.0,
                0.0,
                0.0,
                0.0,
            ],
            target_stds=[
                1.0,
                1.0,
                1.0,
                1.0,
            ],
            type='DeltaXYWHBBoxCoder'),
        feat_channels=256,
        in_channels=256,
        loss_bbox=dict(loss_weight=1.0, type='L1Loss'),
        loss_cls=dict(
            alpha=0.5,
            gamma=2.0,
            loss_weight=2.0,
            type='FocalLoss',
            use_sigmoid=True),
        type='RPNHead'),
    test_cfg=dict(
        rcnn=dict(
            max_per_img=20,
            nms=dict(iou_threshold=0.5, type='nms'),
            score_thr=0.5),
        rpn=dict(
            max_per_img=1000,
            min_bbox_size=8,
            nms=dict(iou_threshold=0.7, type='nms'),
            nms_pre=2000)),
    train_cfg=dict(
        rcnn=dict(
            assigner=dict(
                ignore_iof_thr=-1,
                match_low_quality=False,
                min_pos_iou=0.5,
                neg_iou_thr=0.5,
                pos_iou_thr=0.5,
                type='MaxIoUAssigner'),
            debug=False,
            pos_weight=-1,
            sampler=dict(
                add_gt_as_proposals=True,
                neg_pos_ub=2,
                num=256,
                pos_fraction=0.6,
                type='RandomSampler')),
        rpn=dict(
            allowed_border=-1,
            assigner=dict(
                ignore_iof_thr=-1,
                match_low_quality=True,
                min_pos_iou=0.3,
                neg_iou_thr=0.3,
                pos_iou_thr=0.5,
                type='MaxIoUAssigner'),
            debug=False,
            pos_weight=-1,
            sampler=dict(
                add_gt_as_proposals=False,
                neg_pos_ub=2,
                num=256,
                pos_fraction=0.6,
                type='RandomSampler')),
        rpn_proposal=dict(
            max_per_img=1000,
            min_bbox_size=0,
            nms=dict(iou_threshold=0.7, type='nms'),
            nms_pre=2000)),
    type='FasterRCNN')
env_cfg = dict(
    cudnn_benchmark=False,
    dist_cfg=dict(backend='nccl'),
    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))
geometric = [
    [
        dict(level=7, max_mag=0.05, min_mag=0.0, prob=0.3, type='TranslateX'),
    ],
    [
        dict(level=7, max_mag=0.05, min_mag=0.0, prob=0.3, type='TranslateY'),
    ],
]
img_scale = (
    256,
    720,
)
labeled_dataset = dict(
    ann_file=
    '/home/coder/data/trong/KLTN/Soft_Teacher/data_drill/semi_anno_multiview/_annotations.coco.labeled.grouped@40.bright.json',
    backend_args=None,
    data_prefix=dict(img='train/'),
    data_root='/home/coder/data/trong/KLTN/Soft_Teacher/data_drill/',
    filter_cfg=dict(filter_empty_gt=True, min_size=4),
    metainfo=dict(
        classes=(
            'Broken',
            'Chipped',
            'Scratched',
            'Severe_Rust',
            'Tip_Wear',
        ),
        palette=[
            (
                134,
                34,
                255,
            ),
            (
                0,
                255,
                206,
            ),
            (
                255,
                128,
                0,
            ),
            (
                254,
                0,
                86,
            ),
            (
                199,
                252,
                0,
            ),
        ]),
    pipeline=[
        dict(
            backend_args=None,
            imdecode_backend='cv2',
            to_float32=False,
            type='LoadImageFromFile'),
        dict(type='LoadAnnotations', with_bbox=True),
        dict(keep_ratio=True, scale=(
            256,
            720,
        ), type='Resize'),
        dict(prob=0.5, type='RandomFlip'),
        dict(
            aug_num=1,
            aug_space=[
                [
                    dict(prob=0.5, type='AutoContrast'),
                ],
                [
                    dict(prob=0.4, type='Equalize'),
                ],
                [
                    dict(
                        level=7,
                        max_mag=1.25,
                        min_mag=0.8,
                        prob=0.6,
                        type='Brightness'),
                ],
                [
                    dict(
                        level=7,
                        max_mag=1.3,
                        min_mag=0.8,
                        prob=0.6,
                        type='Contrast'),
                ],
                [
                    dict(
                        level=6,
                        max_mag=1.5,
                        min_mag=0.8,
                        prob=0.4,
                        type='Sharpness'),
                ],
                [
                    dict(
                        level=5,
                        max_mag=7.0,
                        min_mag=5.0,
                        prob=0.2,
                        type='Posterize'),
                ],
            ],
            type='RandAugment'),
        dict(min_gt_bbox_wh=(
            4,
            4,
        ), type='FilterAnnotations'),
        dict(
            branch_field=[
                'sup',
                'unsup_teacher',
                'unsup_student',
            ],
            sup=dict(type='PackDetInputs'),
            type='MultiBranch'),
    ],
    type='MultiViewFromFolder',
    views_per_sample=8)
launcher = 'none'
load_from = None
log_level = 'INFO'
log_processor = dict(by_epoch=True, type='LogProcessor', window_size=50)
metainfo = dict(
    classes=(
        'Broken',
        'Chipped',
        'Scratched',
        'Severe_Rust',
        'Tip_Wear',
    ),
    palette=[
        (
            134,
            34,
            255,
        ),
        (
            0,
            255,
            206,
        ),
        (
            255,
            128,
            0,
        ),
        (
            254,
            0,
            86,
        ),
        (
            199,
            252,
            0,
        ),
    ])
model = dict(
    aggregate_views='mean',
    data_preprocessor=dict(
        data_preprocessor=dict(
            bgr_to_rgb=False,
            mean=[
                103.53,
                116.28,
                123.675,
            ],
            pad_size_divisor=8,
            std=[
                1.0,
                1.0,
                1.0,
            ],
            type='DetDataPreprocessor'),
        type='MultiBranchDataPreprocessor'),
    detector=dict(
        backbone=dict(
            backbone=dict(
                depth=50,
                frozen_stages=1,
                init_cfg=dict(
                    checkpoint='open-mmlab://detectron2/resnet50_caffe',
                    type='Pretrained'),
                norm_cfg=dict(requires_grad=False, type='BN'),
                norm_eval=True,
                num_stages=4,
                out_indices=(
                    0,
                    1,
                    2,
                    3,
                ),
                style='caffe',
                type='ResNet'),
            concat_reduce=False,
            fusion='mvvit',
            mvvit=dict(
                dropout=0.2,
                embed_dim=256,
                mlp_ratio=2.0,
                num_heads=4,
                num_layers=1,
                spatial_attention='moderate',
                type='MVViT',
                use_gradient_checkpointing=True,
                use_layer_norm=True),
            type='MultiViewBackbone',
            views_per_sample=8),
        data_preprocessor=dict(
            bgr_to_rgb=False,
            mean=[
                103.53,
                116.28,
                123.675,
            ],
            pad_size_divisor=8,
            std=[
                1.0,
                1.0,
                1.0,
            ],
            type='DetDataPreprocessor'),
        neck=dict(
            in_channels=[
                256,
                512,
                1024,
                2048,
            ],
            num_outs=5,
            out_channels=256,
            type='FPN'),
        roi_head=dict(
            bbox_head=dict(
                bbox_coder=dict(
                    target_means=[
                        0.0,
                        0.0,
                        0.0,
                        0.0,
                    ],
                    target_stds=[
                        0.1,
                        0.1,
                        0.2,
                        0.2,
                    ],
                    type='DeltaXYWHBBoxCoder'),
                fc_out_channels=1024,
                in_channels=256,
                loss_bbox=dict(loss_weight=1.0, type='L1Loss'),
                loss_cls=dict(
                    alpha=0.5,
                    gamma=2.2,
                    loss_weight=1.5,
                    type='FocalLoss',
                    use_sigmoid=True),
                num_classes=5,
                reg_class_agnostic=False,
                roi_feat_size=7,
                type='Shared2FCBBoxHead'),
            bbox_roi_extractor=dict(
                featmap_strides=[
                    4,
                    8,
                    16,
                    32,
                ],
                out_channels=256,
                roi_layer=dict(
                    output_size=7, sampling_ratio=0, type='RoIAlign'),
                type='SingleRoIExtractor'),
            type='StandardRoIHead'),
        rpn_head=dict(
            anchor_generator=dict(
                ratios=[
                    0.3,
                    0.45,
                    0.6,
                    1.0,
                    2.0,
                ],
                scales=[
                    8,
                    16,
                    32,
                ],
                strides=[
                    4,
                    8,
                    16,
                    32,
                    64,
                ],
                type='AnchorGenerator'),
            bbox_coder=dict(
                target_means=[
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                ],
                target_stds=[
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                ],
                type='DeltaXYWHBBoxCoder'),
            feat_channels=256,
            in_channels=256,
            loss_bbox=dict(loss_weight=1.0, type='L1Loss'),
            loss_cls=dict(
                alpha=0.5,
                gamma=2.0,
                loss_weight=2.0,
                type='FocalLoss',
                use_sigmoid=True),
            type='RPNHead'),
        test_cfg=dict(
            rcnn=dict(
                max_per_img=20,
                nms=dict(iou_threshold=0.5, type='nms'),
                score_thr=0.5),
            rpn=dict(
                max_per_img=1000,
                min_bbox_size=8,
                nms=dict(iou_threshold=0.7, type='nms'),
                nms_pre=2000)),
        train_cfg=dict(
            rcnn=dict(
                assigner=dict(
                    ignore_iof_thr=-1,
                    match_low_quality=False,
                    min_pos_iou=0.5,
                    neg_iou_thr=0.5,
                    pos_iou_thr=0.5,
                    type='MaxIoUAssigner'),
                debug=False,
                pos_weight=-1,
                sampler=dict(
                    add_gt_as_proposals=True,
                    neg_pos_ub=2,
                    num=256,
                    pos_fraction=0.6,
                    type='RandomSampler')),
            rpn=dict(
                allowed_border=-1,
                assigner=dict(
                    ignore_iof_thr=-1,
                    match_low_quality=True,
                    min_pos_iou=0.3,
                    neg_iou_thr=0.3,
                    pos_iou_thr=0.5,
                    type='MaxIoUAssigner'),
                debug=False,
                pos_weight=-1,
                sampler=dict(
                    add_gt_as_proposals=False,
                    neg_pos_ub=2,
                    num=256,
                    pos_fraction=0.6,
                    type='RandomSampler')),
            rpn_proposal=dict(
                max_per_img=1000,
                min_bbox_size=0,
                nms=dict(iou_threshold=0.7, type='nms'),
                nms_pre=2000)),
        type='FasterRCNN'),
    semi_test_cfg=dict(predict_on='teacher'),
    semi_train_cfg=dict(
        cls_pseudo_thr=0.35,
        enable_consensus=True,
        freeze_teacher=True,
        jitter_scale=0.03,
        jitter_times=5,
        min_pseudo_bbox_wh=(
            8,
            8,
        ),
        pseudo_label_initial_score_thr=0.4,
        reg_pseudo_thr=0.1,
        rpn_pseudo_thr=0.35,
        sup_weight=1.0,
        unsup_weight=0.5),
    type='MultiViewSoftTeacher',
    views_per_sample=8)
num_workers = 4
optim_wrapper = dict(
    optimizer=dict(lr=0.0005, momentum=0.9, type='SGD', weight_decay=0.0001),
    type='OptimWrapper')
param_scheduler = [
    dict(
        begin=0, by_epoch=False, end=10000, start_factor=0.01,
        type='LinearLR'),
    dict(
        begin=10000, by_epoch=False, end=70000, factor=1.0, type='ConstantLR'),
    dict(
        begin=70000,
        by_epoch=False,
        end=100000,
        eta_min=5e-05,
        type='CosineAnnealingLR'),
]
resume = False
strong_pipeline = [
    dict(keep_ratio=True, scale=(
        256,
        720,
    ), type='Resize'),
    dict(prob=0.5, type='RandomFlip'),
    dict(
        transforms=[
            dict(
                aug_num=2,
                aug_space=[
                    [
                        dict(prob=0.5, type='AutoContrast'),
                    ],
                    [
                        dict(prob=0.4, type='Equalize'),
                    ],
                    [
                        dict(
                            level=7,
                            max_mag=1.25,
                            min_mag=0.8,
                            prob=0.6,
                            type='Brightness'),
                    ],
                    [
                        dict(
                            level=7,
                            max_mag=1.3,
                            min_mag=0.8,
                            prob=0.6,
                            type='Contrast'),
                    ],
                    [
                        dict(
                            level=6,
                            max_mag=1.5,
                            min_mag=0.8,
                            prob=0.4,
                            type='Sharpness'),
                    ],
                    [
                        dict(
                            level=5,
                            max_mag=7.0,
                            min_mag=5.0,
                            prob=0.2,
                            type='Posterize'),
                    ],
                ],
                type='RandAugment'),
            dict(
                aug_num=1,
                aug_space=[
                    [
                        dict(
                            level=7,
                            max_mag=0.05,
                            min_mag=0.0,
                            prob=0.3,
                            type='TranslateX'),
                    ],
                    [
                        dict(
                            level=7,
                            max_mag=0.05,
                            min_mag=0.0,
                            prob=0.3,
                            type='TranslateY'),
                    ],
                ],
                type='RandAugment'),
        ],
        type='RandomOrder'),
    dict(n_patches=(
        1,
        3,
    ), ratio=(
        0,
        0.15,
    ), type='RandomErasing'),
    dict(min_gt_bbox_wh=(
        4,
        4,
    ), type='FilterAnnotations'),
    dict(
        meta_keys=(
            'img_id',
            'img_path',
            'ori_shape',
            'img_shape',
            'scale_factor',
            'flip',
            'flip_direction',
            'homography_matrix',
        ),
        type='PackDetInputs'),
]
sup_pipeline = [
    dict(
        backend_args=None,
        imdecode_backend='cv2',
        to_float32=False,
        type='LoadImageFromFile'),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(keep_ratio=True, scale=(
        256,
        720,
    ), type='Resize'),
    dict(prob=0.5, type='RandomFlip'),
    dict(
        aug_num=1,
        aug_space=[
            [
                dict(prob=0.5, type='AutoContrast'),
            ],
            [
                dict(prob=0.4, type='Equalize'),
            ],
            [
                dict(
                    level=7,
                    max_mag=1.25,
                    min_mag=0.8,
                    prob=0.6,
                    type='Brightness'),
            ],
            [
                dict(
                    level=7,
                    max_mag=1.3,
                    min_mag=0.8,
                    prob=0.6,
                    type='Contrast'),
            ],
            [
                dict(
                    level=6,
                    max_mag=1.5,
                    min_mag=0.8,
                    prob=0.4,
                    type='Sharpness'),
            ],
            [
                dict(
                    level=5,
                    max_mag=7.0,
                    min_mag=5.0,
                    prob=0.2,
                    type='Posterize'),
            ],
        ],
        type='RandAugment'),
    dict(min_gt_bbox_wh=(
        4,
        4,
    ), type='FilterAnnotations'),
    dict(
        branch_field=[
            'sup',
            'unsup_teacher',
            'unsup_student',
        ],
        sup=dict(type='PackDetInputs'),
        type='MultiBranch'),
]
test_cfg = dict(type='TestLoop')
test_dataloader = dict(
    batch_size=1,
    collate_fn=dict(
        type=
        'mmdet.datasets.wrappers.multi_view_from_folder.multi_view_collate_val'
    ),
    dataset=dict(
        ann_file=
        '/home/coder/data/trong/KLTN/Soft_Teacher/data_drill/anno_valid/_annotations_filtered.bright.coco.json',
        backend_args=None,
        data_prefix=dict(img='valid/'),
        data_root='/home/coder/data/trong/KLTN/Soft_Teacher/data_drill/',
        metainfo=dict(
            classes=(
                'Broken',
                'Chipped',
                'Scratched',
                'Severe_Rust',
                'Tip_Wear',
            ),
            palette=[
                (
                    134,
                    34,
                    255,
                ),
                (
                    0,
                    255,
                    206,
                ),
                (
                    255,
                    128,
                    0,
                ),
                (
                    254,
                    0,
                    86,
                ),
                (
                    199,
                    252,
                    0,
                ),
            ]),
        pipeline=[
            dict(
                backend_args=None,
                imdecode_backend='cv2',
                to_float32=False,
                type='LoadImageFromFile'),
            dict(keep_ratio=True, scale=(
                256,
                720,
            ), type='Resize'),
            dict(
                meta_keys=(
                    'img_id',
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                ),
                type='PackDetInputs'),
        ],
        test_mode=True,
        type='MultiViewFromFolder',
        views_per_sample=8),
    drop_last=False,
    num_workers=2,
    persistent_workers=True,
    pin_memory=True,
    sampler=dict(shuffle=False, type='DefaultSampler'))
test_evaluator = dict(
    aggregation='none',
    ann_file=
    '/home/coder/data/trong/KLTN/Soft_Teacher/data_drill/anno_valid/_annotations_filtered.bright.coco.json',
    backend_args=None,
    classwise=True,
    enable_aggregation=False,
    extract_base_name=True,
    format_only=False,
    metric='bbox',
    nms_iou_thr=0.4,
    type='MultiViewCocoMetric',
    views_per_sample=8)
test_pipeline = [
    dict(
        backend_args=None,
        imdecode_backend='cv2',
        to_float32=False,
        type='LoadImageFromFile'),
    dict(keep_ratio=True, scale=(
        256,
        720,
    ), type='Resize'),
    dict(
        meta_keys=(
            'img_id',
            'img_path',
            'ori_shape',
            'img_shape',
            'scale_factor',
        ),
        type='PackDetInputs'),
]
train_cfg = dict(
    max_iters=100000, type='IterBasedTrainLoop', val_interval=10000)
train_dataloader = dict(
    batch_size=2,
    collate_fn=dict(
        type=
        'mmdet.datasets.wrappers.multi_view_from_folder.multi_view_collate_flatten'
    ),
    dataset=dict(
        datasets=[
            dict(
                ann_file=
                '/home/coder/data/trong/KLTN/Soft_Teacher/data_drill/semi_anno_multiview/_annotations.coco.labeled.grouped@40.bright.json',
                backend_args=None,
                data_prefix=dict(img='train/'),
                data_root=
                '/home/coder/data/trong/KLTN/Soft_Teacher/data_drill/',
                filter_cfg=dict(filter_empty_gt=True, min_size=4),
                metainfo=dict(
                    classes=(
                        'Broken',
                        'Chipped',
                        'Scratched',
                        'Severe_Rust',
                        'Tip_Wear',
                    ),
                    palette=[
                        (
                            134,
                            34,
                            255,
                        ),
                        (
                            0,
                            255,
                            206,
                        ),
                        (
                            255,
                            128,
                            0,
                        ),
                        (
                            254,
                            0,
                            86,
                        ),
                        (
                            199,
                            252,
                            0,
                        ),
                    ]),
                pipeline=[
                    dict(
                        backend_args=None,
                        imdecode_backend='cv2',
                        to_float32=False,
                        type='LoadImageFromFile'),
                    dict(type='LoadAnnotations', with_bbox=True),
                    dict(keep_ratio=True, scale=(
                        256,
                        720,
                    ), type='Resize'),
                    dict(prob=0.5, type='RandomFlip'),
                    dict(
                        aug_num=1,
                        aug_space=[
                            [
                                dict(prob=0.5, type='AutoContrast'),
                            ],
                            [
                                dict(prob=0.4, type='Equalize'),
                            ],
                            [
                                dict(
                                    level=7,
                                    max_mag=1.25,
                                    min_mag=0.8,
                                    prob=0.6,
                                    type='Brightness'),
                            ],
                            [
                                dict(
                                    level=7,
                                    max_mag=1.3,
                                    min_mag=0.8,
                                    prob=0.6,
                                    type='Contrast'),
                            ],
                            [
                                dict(
                                    level=6,
                                    max_mag=1.5,
                                    min_mag=0.8,
                                    prob=0.4,
                                    type='Sharpness'),
                            ],
                            [
                                dict(
                                    level=5,
                                    max_mag=7.0,
                                    min_mag=5.0,
                                    prob=0.2,
                                    type='Posterize'),
                            ],
                        ],
                        type='RandAugment'),
                    dict(min_gt_bbox_wh=(
                        4,
                        4,
                    ), type='FilterAnnotations'),
                    dict(
                        branch_field=[
                            'sup',
                            'unsup_teacher',
                            'unsup_student',
                        ],
                        sup=dict(type='PackDetInputs'),
                        type='MultiBranch'),
                ],
                type='MultiViewFromFolder',
                views_per_sample=8),
            dict(
                ann_file=
                '/home/coder/data/trong/KLTN/Soft_Teacher/data_drill/semi_anno_multiview/_annotations.coco.unlabeled.grouped@40.bright.json',
                backend_args=None,
                data_prefix=dict(img='train/'),
                data_root=
                '/home/coder/data/trong/KLTN/Soft_Teacher/data_drill/',
                filter_cfg=dict(filter_empty_gt=False, min_size=4),
                metainfo=dict(
                    classes=(
                        'Broken',
                        'Chipped',
                        'Scratched',
                        'Severe_Rust',
                        'Tip_Wear',
                    ),
                    palette=[
                        (
                            134,
                            34,
                            255,
                        ),
                        (
                            0,
                            255,
                            206,
                        ),
                        (
                            255,
                            128,
                            0,
                        ),
                        (
                            254,
                            0,
                            86,
                        ),
                        (
                            199,
                            252,
                            0,
                        ),
                    ]),
                pipeline=[
                    dict(
                        backend_args=None,
                        imdecode_backend='cv2',
                        to_float32=False,
                        type='LoadImageFromFile'),
                    dict(type='LoadEmptyAnnotations'),
                    dict(
                        branch_field=[
                            'sup',
                            'unsup_teacher',
                            'unsup_student',
                        ],
                        type='MultiBranch',
                        unsup_student=[
                            dict(
                                keep_ratio=True,
                                scale=(
                                    256,
                                    720,
                                ),
                                type='Resize'),
                            dict(prob=0.5, type='RandomFlip'),
                            dict(
                                transforms=[
                                    dict(
                                        aug_num=2,
                                        aug_space=[
                                            [
                                                dict(
                                                    prob=0.5,
                                                    type='AutoContrast'),
                                            ],
                                            [
                                                dict(
                                                    prob=0.4, type='Equalize'),
                                            ],
                                            [
                                                dict(
                                                    level=7,
                                                    max_mag=1.25,
                                                    min_mag=0.8,
                                                    prob=0.6,
                                                    type='Brightness'),
                                            ],
                                            [
                                                dict(
                                                    level=7,
                                                    max_mag=1.3,
                                                    min_mag=0.8,
                                                    prob=0.6,
                                                    type='Contrast'),
                                            ],
                                            [
                                                dict(
                                                    level=6,
                                                    max_mag=1.5,
                                                    min_mag=0.8,
                                                    prob=0.4,
                                                    type='Sharpness'),
                                            ],
                                            [
                                                dict(
                                                    level=5,
                                                    max_mag=7.0,
                                                    min_mag=5.0,
                                                    prob=0.2,
                                                    type='Posterize'),
                                            ],
                                        ],
                                        type='RandAugment'),
                                    dict(
                                        aug_num=1,
                                        aug_space=[
                                            [
                                                dict(
                                                    level=7,
                                                    max_mag=0.05,
                                                    min_mag=0.0,
                                                    prob=0.3,
                                                    type='TranslateX'),
                                            ],
                                            [
                                                dict(
                                                    level=7,
                                                    max_mag=0.05,
                                                    min_mag=0.0,
                                                    prob=0.3,
                                                    type='TranslateY'),
                                            ],
                                        ],
                                        type='RandAugment'),
                                ],
                                type='RandomOrder'),
                            dict(
                                n_patches=(
                                    1,
                                    3,
                                ),
                                ratio=(
                                    0,
                                    0.15,
                                ),
                                type='RandomErasing'),
                            dict(
                                min_gt_bbox_wh=(
                                    4,
                                    4,
                                ),
                                type='FilterAnnotations'),
                            dict(
                                meta_keys=(
                                    'img_id',
                                    'img_path',
                                    'ori_shape',
                                    'img_shape',
                                    'scale_factor',
                                    'flip',
                                    'flip_direction',
                                    'homography_matrix',
                                ),
                                type='PackDetInputs'),
                        ],
                        unsup_teacher=[
                            dict(
                                keep_ratio=True,
                                scale=(
                                    256,
                                    720,
                                ),
                                type='Resize'),
                            dict(prob=0.5, type='RandomFlip'),
                            dict(
                                meta_keys=(
                                    'img_id',
                                    'img_path',
                                    'ori_shape',
                                    'img_shape',
                                    'scale_factor',
                                    'flip',
                                    'flip_direction',
                                    'homography_matrix',
                                ),
                                type='PackDetInputs'),
                        ]),
                ],
                type='MultiViewFromFolder',
                views_per_sample=8),
        ],
        type='ConcatDataset'),
    num_workers=4,
    persistent_workers=True,
    pin_memory=True,
    prefetch_factor=2,
    sampler=dict(
        batch_size=2, source_ratio=[
            1,
            1,
        ], type='GroupMultiSourceSampler'))
unlabeled_dataset = dict(
    ann_file=
    '/home/coder/data/trong/KLTN/Soft_Teacher/data_drill/semi_anno_multiview/_annotations.coco.unlabeled.grouped@40.bright.json',
    backend_args=None,
    data_prefix=dict(img='train/'),
    data_root='/home/coder/data/trong/KLTN/Soft_Teacher/data_drill/',
    filter_cfg=dict(filter_empty_gt=False, min_size=4),
    metainfo=dict(
        classes=(
            'Broken',
            'Chipped',
            'Scratched',
            'Severe_Rust',
            'Tip_Wear',
        ),
        palette=[
            (
                134,
                34,
                255,
            ),
            (
                0,
                255,
                206,
            ),
            (
                255,
                128,
                0,
            ),
            (
                254,
                0,
                86,
            ),
            (
                199,
                252,
                0,
            ),
        ]),
    pipeline=[
        dict(
            backend_args=None,
            imdecode_backend='cv2',
            to_float32=False,
            type='LoadImageFromFile'),
        dict(type='LoadEmptyAnnotations'),
        dict(
            branch_field=[
                'sup',
                'unsup_teacher',
                'unsup_student',
            ],
            type='MultiBranch',
            unsup_student=[
                dict(keep_ratio=True, scale=(
                    256,
                    720,
                ), type='Resize'),
                dict(prob=0.5, type='RandomFlip'),
                dict(
                    transforms=[
                        dict(
                            aug_num=2,
                            aug_space=[
                                [
                                    dict(prob=0.5, type='AutoContrast'),
                                ],
                                [
                                    dict(prob=0.4, type='Equalize'),
                                ],
                                [
                                    dict(
                                        level=7,
                                        max_mag=1.25,
                                        min_mag=0.8,
                                        prob=0.6,
                                        type='Brightness'),
                                ],
                                [
                                    dict(
                                        level=7,
                                        max_mag=1.3,
                                        min_mag=0.8,
                                        prob=0.6,
                                        type='Contrast'),
                                ],
                                [
                                    dict(
                                        level=6,
                                        max_mag=1.5,
                                        min_mag=0.8,
                                        prob=0.4,
                                        type='Sharpness'),
                                ],
                                [
                                    dict(
                                        level=5,
                                        max_mag=7.0,
                                        min_mag=5.0,
                                        prob=0.2,
                                        type='Posterize'),
                                ],
                            ],
                            type='RandAugment'),
                        dict(
                            aug_num=1,
                            aug_space=[
                                [
                                    dict(
                                        level=7,
                                        max_mag=0.05,
                                        min_mag=0.0,
                                        prob=0.3,
                                        type='TranslateX'),
                                ],
                                [
                                    dict(
                                        level=7,
                                        max_mag=0.05,
                                        min_mag=0.0,
                                        prob=0.3,
                                        type='TranslateY'),
                                ],
                            ],
                            type='RandAugment'),
                    ],
                    type='RandomOrder'),
                dict(
                    n_patches=(
                        1,
                        3,
                    ),
                    ratio=(
                        0,
                        0.15,
                    ),
                    type='RandomErasing'),
                dict(min_gt_bbox_wh=(
                    4,
                    4,
                ), type='FilterAnnotations'),
                dict(
                    meta_keys=(
                        'img_id',
                        'img_path',
                        'ori_shape',
                        'img_shape',
                        'scale_factor',
                        'flip',
                        'flip_direction',
                        'homography_matrix',
                    ),
                    type='PackDetInputs'),
            ],
            unsup_teacher=[
                dict(keep_ratio=True, scale=(
                    256,
                    720,
                ), type='Resize'),
                dict(prob=0.5, type='RandomFlip'),
                dict(
                    meta_keys=(
                        'img_id',
                        'img_path',
                        'ori_shape',
                        'img_shape',
                        'scale_factor',
                        'flip',
                        'flip_direction',
                        'homography_matrix',
                    ),
                    type='PackDetInputs'),
            ]),
    ],
    type='MultiViewFromFolder',
    views_per_sample=8)
unsup_pipeline = [
    dict(
        backend_args=None,
        imdecode_backend='cv2',
        to_float32=False,
        type='LoadImageFromFile'),
    dict(type='LoadEmptyAnnotations'),
    dict(
        branch_field=[
            'sup',
            'unsup_teacher',
            'unsup_student',
        ],
        type='MultiBranch',
        unsup_student=[
            dict(keep_ratio=True, scale=(
                256,
                720,
            ), type='Resize'),
            dict(prob=0.5, type='RandomFlip'),
            dict(
                transforms=[
                    dict(
                        aug_num=2,
                        aug_space=[
                            [
                                dict(prob=0.5, type='AutoContrast'),
                            ],
                            [
                                dict(prob=0.4, type='Equalize'),
                            ],
                            [
                                dict(
                                    level=7,
                                    max_mag=1.25,
                                    min_mag=0.8,
                                    prob=0.6,
                                    type='Brightness'),
                            ],
                            [
                                dict(
                                    level=7,
                                    max_mag=1.3,
                                    min_mag=0.8,
                                    prob=0.6,
                                    type='Contrast'),
                            ],
                            [
                                dict(
                                    level=6,
                                    max_mag=1.5,
                                    min_mag=0.8,
                                    prob=0.4,
                                    type='Sharpness'),
                            ],
                            [
                                dict(
                                    level=5,
                                    max_mag=7.0,
                                    min_mag=5.0,
                                    prob=0.2,
                                    type='Posterize'),
                            ],
                        ],
                        type='RandAugment'),
                    dict(
                        aug_num=1,
                        aug_space=[
                            [
                                dict(
                                    level=7,
                                    max_mag=0.05,
                                    min_mag=0.0,
                                    prob=0.3,
                                    type='TranslateX'),
                            ],
                            [
                                dict(
                                    level=7,
                                    max_mag=0.05,
                                    min_mag=0.0,
                                    prob=0.3,
                                    type='TranslateY'),
                            ],
                        ],
                        type='RandAugment'),
                ],
                type='RandomOrder'),
            dict(n_patches=(
                1,
                3,
            ), ratio=(
                0,
                0.15,
            ), type='RandomErasing'),
            dict(min_gt_bbox_wh=(
                4,
                4,
            ), type='FilterAnnotations'),
            dict(
                meta_keys=(
                    'img_id',
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                    'flip',
                    'flip_direction',
                    'homography_matrix',
                ),
                type='PackDetInputs'),
        ],
        unsup_teacher=[
            dict(keep_ratio=True, scale=(
                256,
                720,
            ), type='Resize'),
            dict(prob=0.5, type='RandomFlip'),
            dict(
                meta_keys=(
                    'img_id',
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                    'flip',
                    'flip_direction',
                    'homography_matrix',
                ),
                type='PackDetInputs'),
        ]),
]
val_cfg = dict(type='TeacherStudentValLoop')
val_dataloader = dict(
    batch_size=1,
    collate_fn=dict(
        type=
        'mmdet.datasets.wrappers.multi_view_from_folder.multi_view_collate_val'
    ),
    dataset=dict(
        ann_file=
        '/home/coder/data/trong/KLTN/Soft_Teacher/data_drill/anno_valid/_annotations_filtered.bright.coco.json',
        backend_args=None,
        data_prefix=dict(img='valid/'),
        data_root='/home/coder/data/trong/KLTN/Soft_Teacher/data_drill/',
        metainfo=dict(
            classes=(
                'Broken',
                'Chipped',
                'Scratched',
                'Severe_Rust',
                'Tip_Wear',
            ),
            palette=[
                (
                    134,
                    34,
                    255,
                ),
                (
                    0,
                    255,
                    206,
                ),
                (
                    255,
                    128,
                    0,
                ),
                (
                    254,
                    0,
                    86,
                ),
                (
                    199,
                    252,
                    0,
                ),
            ]),
        pipeline=[
            dict(
                backend_args=None,
                imdecode_backend='cv2',
                to_float32=False,
                type='LoadImageFromFile'),
            dict(keep_ratio=True, scale=(
                256,
                720,
            ), type='Resize'),
            dict(
                meta_keys=(
                    'img_id',
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                ),
                type='PackDetInputs'),
        ],
        test_mode=True,
        type='MultiViewFromFolder',
        views_per_sample=8),
    drop_last=False,
    num_workers=2,
    persistent_workers=True,
    pin_memory=True,
    sampler=dict(shuffle=False, type='DefaultSampler'))
val_evaluator = dict(
    aggregation='none',
    ann_file=
    '/home/coder/data/trong/KLTN/Soft_Teacher/data_drill/anno_valid/_annotations_filtered.bright.coco.json',
    backend_args=None,
    classwise=True,
    enable_aggregation=False,
    extract_base_name=True,
    format_only=False,
    metric='bbox',
    nms_iou_thr=0.4,
    type='MultiViewCocoMetric',
    views_per_sample=8)
views_per_sample = 8
vis_backends = [
    dict(type='LocalVisBackend'),
]
visualizer = dict(
    name='visualizer',
    type='DetLocalVisualizer',
    vis_backends=[
        dict(type='LocalVisBackend'),
    ])
weak_pipeline = [
    dict(keep_ratio=True, scale=(
        256,
        720,
    ), type='Resize'),
    dict(prob=0.5, type='RandomFlip'),
    dict(
        meta_keys=(
            'img_id',
            'img_path',
            'ori_shape',
            'img_shape',
            'scale_factor',
            'flip',
            'flip_direction',
            'homography_matrix',
        ),
        type='PackDetInputs'),
]
work_dir = './work_dirs/soft_teacher_custom_multi_view'

2025/12/09 07:59:46 - mmengine - INFO - MultiViewBackbone: Initialized with MVViT fusion, views_per_sample=8
2025/12/09 07:59:46 - mmengine - INFO - MultiViewBackbone: Initialized with MVViT fusion, views_per_sample=8
2025/12/09 07:59:47 - mmengine - INFO - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.
2025/12/09 07:59:47 - mmengine - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) RuntimeInfoHook                    
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
before_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) MeanTeacherHook                    
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DistSamplerSeedHook                
 -------------------- 
before_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) MeanTeacherHook                    
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) IterTimerHook                      
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_val_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_val_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_val_iter:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DetVisualizationHook               
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_val_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_test_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_test_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_test_iter:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DetVisualizationHook               
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_run:
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
2025/12/09 07:59:50 - mmengine - INFO - [COCO ANNO] Loaded from: /home/coder/data/trong/KLTN/Soft_Teacher/data_drill/semi_anno_multiview/_annotations.coco.labeled.grouped@40.bright.json
2025/12/09 07:59:50 - mmengine - INFO -    1208 images, 1448 annotations
2025/12/09 07:59:50 - mmengine - INFO -    946 files have annotations
2025/12/09 07:59:50 - mmengine - INFO -    1208 files have base_img_id
2025/12/09 07:59:50 - mmengine - INFO -    Categories: [0, 1, 2, 3, 4, 5]
2025/12/09 07:59:50 - mmengine - INFO -    cat2label mapping: {1: 0, 2: 1, 3: 2, 4: 3, 5: 4}
2025/12/09 07:59:50 - mmengine - INFO -    File 'S136_Image__2025-09-23__16-42-55_bright_4_crop_8_jpg.rf.8d03a89b2a2643c02b4733eb858e68be.jpg': 1 boxes, base_img_id=S136_bright_4, crop_num=8
2025/12/09 07:59:50 - mmengine - INFO -    File 'S40_Image__2025-09-04__16-02-50_bright_2_crop_6_jpg.rf.0006368956fc4380b51ac33ff3fbee7c.jpg': 1 boxes, base_img_id=S40_bright_2, crop_num=6
2025/12/09 07:59:50 - mmengine - INFO -    File 'S124_Image__2025-09-23__10-26-03_bright_3_crop_5_jpg.rf.eff0c11d31452dcd01ccb1959ba342f5.jpg': 2 boxes, base_img_id=S124_bright_3, crop_num=5
2025/12/09 07:59:50 - mmengine - INFO - [COCO] Built 151 groups from base_img_id
2025/12/09 07:59:50 - mmengine - INFO - [COCO] Total files in COCO JSON: 1208
2025/12/09 07:59:50 - mmengine - INFO - [COCO] Unique base_img_ids: 151
2025/12/09 07:59:50 - mmengine - INFO -    Group 0: base_img_id=S101_bright_2, 8/8 views
2025/12/09 07:59:50 - mmengine - INFO -    Group 1: base_img_id=S111_bright_3, 8/8 views
2025/12/09 07:59:50 - mmengine - INFO -    Group 2: base_img_id=S111_bright_4, 8/8 views
2025/12/09 07:59:50 - mmengine - INFO - [COCO ANNO] Loaded from: /home/coder/data/trong/KLTN/Soft_Teacher/data_drill/semi_anno_multiview/_annotations.coco.unlabeled.grouped@40.bright.json
2025/12/09 07:59:50 - mmengine - INFO -    784 images, 956 annotations
2025/12/09 07:59:50 - mmengine - INFO -    649 files have annotations
2025/12/09 07:59:50 - mmengine - INFO -    784 files have base_img_id
2025/12/09 07:59:50 - mmengine - INFO -    Categories: [0, 1, 2, 3, 4, 5]
2025/12/09 07:59:50 - mmengine - INFO -    cat2label mapping: {1: 0, 2: 1, 3: 2, 4: 3, 5: 4}
2025/12/09 07:59:50 - mmengine - INFO -    File 'S4_than_Image__2025-09-03__14-00-16_bright_2_crop_3_jpg.rf.93c6a9e3d563bf395cd5c9ef7519b742.jpg': 3 boxes, base_img_id=S4_bright_2, crop_num=3
2025/12/09 07:59:50 - mmengine - INFO -    File 'S225_Image__2025-07-01__14-30-51_bright_2_crop_7_jpg.rf.6b508c0dd7ed86aff7e31273da30c5e4.jpg': 2 boxes, base_img_id=S225_bright_2, crop_num=7
2025/12/09 07:59:50 - mmengine - INFO -    File 'S78_Image__2025-09-08__16-35-36_bright_3_crop_8_jpg.rf.84d34f89fe2af65f5adacb8437ed2ee6.jpg': 3 boxes, base_img_id=S78_bright_3, crop_num=8
2025/12/09 07:59:50 - mmengine - INFO - [COCO] Built 98 groups from base_img_id
2025/12/09 07:59:50 - mmengine - INFO - [COCO] Total files in COCO JSON: 784
2025/12/09 07:59:50 - mmengine - INFO - [COCO] Unique base_img_ids: 98
2025/12/09 07:59:50 - mmengine - INFO -    Group 0: base_img_id=S111_bright_2, 8/8 views
2025/12/09 07:59:50 - mmengine - INFO -    Group 1: base_img_id=S114_bright_2, 8/8 views
2025/12/09 07:59:50 - mmengine - INFO -    Group 2: base_img_id=S117_bright_2, 8/8 views
2025/12/09 07:59:50 - mmengine - INFO - [COCO ANNO] Loaded from: /home/coder/data/trong/KLTN/Soft_Teacher/data_drill/anno_valid/_annotations_filtered.bright.coco.json
2025/12/09 07:59:50 - mmengine - INFO -    320 images, 442 annotations
2025/12/09 07:59:50 - mmengine - INFO -    284 files have annotations
2025/12/09 07:59:50 - mmengine - INFO -    320 files have base_img_id
2025/12/09 07:59:50 - mmengine - INFO -    Categories: [0, 1, 2, 3, 4, 5]
2025/12/09 07:59:50 - mmengine - INFO -    cat2label mapping: {1: 0, 2: 1, 3: 2, 4: 3, 5: 4}
2025/12/09 07:59:50 - mmengine - INFO -    File 'S245_Image__2025-11-11__12-09-08_bright_2_crop_5_jpg.rf.b0924b8796050c8c09b6c63180a8df2a.jpg': 1 boxes, base_img_id=S245_bright_2, crop_num=5
2025/12/09 07:59:50 - mmengine - INFO -    File 'S135_Image__2025-09-23__16-23-02_bright_4_crop_7_jpg.rf.1b8c818ab9826dbcd27a64c2e38414e1.jpg': 2 boxes, base_img_id=S135_bright_4, crop_num=7
2025/12/09 07:59:50 - mmengine - INFO -    File 'S242_Image__2025-11-11__11-56-30_bright_8_crop_1_jpg.rf.06e5607648d1e41e35c7bec4fc2eeb57.jpg': 1 boxes, base_img_id=S242_bright_8, crop_num=1
2025/12/09 07:59:50 - mmengine - INFO - [COCO] Built 40 groups from base_img_id
2025/12/09 07:59:50 - mmengine - INFO - [COCO] Total files in COCO JSON: 320
2025/12/09 07:59:50 - mmengine - INFO - [COCO] Unique base_img_ids: 40
2025/12/09 07:59:50 - mmengine - INFO -    Group 0: base_img_id=S110_bright_2, 8/8 views
2025/12/09 07:59:50 - mmengine - INFO -    Group 1: base_img_id=S110_bright_3, 8/8 views
2025/12/09 07:59:50 - mmengine - INFO -    Group 2: base_img_id=S110_bright_4, 8/8 views
2025/12/09 07:59:50 - mmengine - INFO - MultiViewCocoMetric initialized: PER-CROP evaluation mode (no aggregation, 8 crops evaluated independently)
2025/12/09 07:59:50 - mmengine - INFO - [BBOX DEBUG 1] File: S76_Image__2025-09-08__15-44-13_bright_2_crop_1_jpg.rf.58478a7f65d7717b15be6110556e7e8a.jpg, Image shape: 720x256
2025/12/09 07:59:50 - mmengine - INFO -    Loaded 1 boxes from COCO JSON
2025/12/09 07:59:50 - mmengine - INFO -     Box 0: [38, 44, 142.34, 147.74], label: 4
2025/12/09 07:59:50 - mmengine - INFO - [BBOX DEBUG 1] File: S82_Image__2025-09-08__18-14-09_bright_3_crop_1_jpg.rf.424af8b75c6b21792f39c8a52f2d73ac.jpg, Image shape: 720x256
2025/12/09 07:59:50 - mmengine - INFO -    Loaded 1 boxes from COCO JSON
2025/12/09 07:59:50 - mmengine - INFO -     Box 0: [27, 106, 74.89, 319.1], label: 2
2025/12/09 07:59:50 - mmengine - INFO - [BBOX DEBUG 1] File: S8_than_Image__2025-09-03__14-39-32_bright_3_crop_1_jpg.rf.6cfe7f4254a5989033bff6ad8dd359ae.jpg, Image shape: 720x256
2025/12/09 07:59:50 - mmengine - INFO -    Loaded 1 boxes from COCO JSON
2025/12/09 07:59:50 - mmengine - INFO -     Box 0: [155, 42, 204.3, 283.87], label: 2
2025/12/09 07:59:50 - mmengine - INFO - [BBOX DEBUG 2] File: S76_Image__2025-09-08__15-44-13_bright_2_crop_3_jpg.rf.221fb19f403d8221a4c72ff07f9c7dde.jpg, Image shape: 720x256
2025/12/09 07:59:50 - mmengine - INFO -    Loaded 1 boxes from COCO JSON
2025/12/09 07:59:50 - mmengine - INFO -     Box 0: [125, 0, 165.01, 72.29], label: 4
2025/12/09 07:59:50 - mmengine - INFO - [BBOX DEBUG 3] File: S76_Image__2025-09-08__15-44-13_bright_2_crop_4_jpg.rf.44b8ef1ce969a74685938bcce791ebae.jpg, Image shape: 720x256
2025/12/09 07:59:50 - mmengine - INFO -    Loaded 1 boxes from COCO JSON
2025/12/09 07:59:50 - mmengine - INFO -     Box 0: [57, 10, 140.45, 103.8], label: 4
2025/12/09 07:59:50 - mmengine - INFO - [BBOX DEBUG 2] File: S8_than_Image__2025-09-03__14-39-32_bright_3_crop_2_jpg.rf.b7c8a0a30eb9a439fb5a0e5db3b23113.jpg, Image shape: 720x256
2025/12/09 07:59:50 - mmengine - INFO -    Loaded 1 boxes from COCO JSON
2025/12/09 07:59:50 - mmengine - INFO -     Box 0: [70, 0, 131.577, 241.921], label: 2
2025/12/09 07:59:50 - mmengine - INFO - [BBOX DEBUG 2] File: S82_Image__2025-09-08__18-14-09_bright_3_crop_3_jpg.rf.18b4a5c87396a2387c97919acb399cca.jpg, Image shape: 720x256
2025/12/09 07:59:50 - mmengine - INFO -    Loaded 1 boxes from COCO JSON
2025/12/09 07:59:50 - mmengine - INFO -     Box 0: [141, 141, 216.29000000000002, 397.45], label: 2
2025/12/09 07:59:50 - mmengine - INFO - [BBOX DEBUG 3] File: S82_Image__2025-09-08__18-14-09_bright_3_crop_4_jpg.rf.721f1ede7bce2dd70095ca67b2c28834.jpg, Image shape: 720x256
2025/12/09 07:59:50 - mmengine - INFO -    Loaded 1 boxes from COCO JSON
2025/12/09 07:59:50 - mmengine - INFO -     Box 0: [64, 158, 183.82, 403.69], label: 2
2025/12/09 07:59:50 - mmengine - INFO - [BBOX DEBUG 3] File: S8_than_Image__2025-09-03__14-39-32_bright_3_crop_3_jpg.rf.2db64fde3d4bd15a8e5c0ce2705f1b22.jpg, Image shape: 720x256
2025/12/09 07:59:50 - mmengine - INFO -    Loaded 1 boxes from COCO JSON
2025/12/09 07:59:50 - mmengine - INFO -     Box 0: [21, 87, 60.18, 256.2], label: 2
2025/12/09 07:59:50 - mmengine - INFO - [BBOX DEBUG 1] File: S142_Image__2025-09-24__11-11-21_bright_2_crop_4_jpg.rf.1140640339faad33dc360aca4eb28832.jpg, Image shape: 720x256
2025/12/09 07:59:50 - mmengine - INFO -    Loaded 2 boxes from COCO JSON
2025/12/09 07:59:50 - mmengine - INFO -     Box 0: [159, 325, 207.32999999999998, 474.65999999999997], label: 2
2025/12/09 07:59:50 - mmengine - INFO -     Box 1: [51, 116, 165.9, 202.63], label: 4
2025/12/09 07:59:50 - mmengine - INFO - [BBOX DEBUG 2] File: S142_Image__2025-09-24__11-11-21_bright_2_crop_5_jpg.rf.f5cb875c5d53567ebd01702b6badb42e.jpg, Image shape: 720x256
2025/12/09 07:59:50 - mmengine - INFO -    Loaded 1 boxes from COCO JSON
2025/12/09 07:59:50 - mmengine - INFO -     Box 0: [99, 320, 171.227, 523.107], label: 2
2025/12/09 07:59:50 - mmengine - INFO - [BBOX DEBUG 3] File: S142_Image__2025-09-24__11-11-21_bright_2_crop_6_jpg.rf.487abc696967ba514f0cf9eeab49c53c.jpg, Image shape: 720x256
2025/12/09 07:59:50 - mmengine - INFO -    Loaded 1 boxes from COCO JSON
2025/12/09 07:59:50 - mmengine - INFO -     Box 0: [27, 338, 98.54, 515.1759999999999], label: 2
2025/12/09 07:59:50 - mmengine - INFO - [BBOX DEBUG 1] File: S71_Image__2025-09-22__11-06-14_bright_6_crop_1_jpg.rf.582d6f166d2a942bb556f253394da9f5.jpg, Image shape: 720x256
2025/12/09 07:59:50 - mmengine - INFO -    Loaded 1 boxes from COCO JSON
2025/12/09 07:59:50 - mmengine - INFO -     Box 0: [50, 348, 239.258, 671.328], label: 3
2025/12/09 07:59:50 - mmengine - INFO - [BBOX DEBUG 2] File: S71_Image__2025-09-22__11-06-14_bright_6_crop_2_jpg.rf.44326343fc27a305413a4f3ce7ef73f1.jpg, Image shape: 720x256
2025/12/09 07:59:50 - mmengine - INFO -    Loaded 3 boxes from COCO JSON
2025/12/09 07:59:50 - mmengine - INFO -     Box 0: [121, 115, 254.841, 461.563], label: 2
2025/12/09 07:59:50 - mmengine - INFO -     Box 1: [21, 459, 117.092, 675.0699999999999], label: 3
2025/12/09 07:59:50 - mmengine - INFO -     Box 2: [119, 467, 207.434, 645.656], label: 3
2025/12/09 07:59:50 - mmengine - INFO - [BBOX DEBUG 1] File: S131_Image__2025-09-23__14-18-52_bright_2_crop_1_jpg.rf.5e83c12fd7b7da426b97baf12b369887.jpg, Image shape: 720x256
2025/12/09 07:59:50 - mmengine - INFO -    Loaded 2 boxes from COCO JSON
2025/12/09 07:59:50 - mmengine - INFO -     Box 0: [28, 60, 252.395, 181.918], label: 0
2025/12/09 07:59:50 - mmengine - INFO -     Box 1: [33, 204, 80.774, 291.574], label: 1
2025/12/09 07:59:50 - mmengine - INFO - [BBOX DEBUG 2] File: S131_Image__2025-09-23__14-18-52_bright_2_crop_2_jpg.rf.e2eb062ad9dca5c3f3b104cf20b2fe3c.jpg, Image shape: 720x256
2025/12/09 07:59:50 - mmengine - INFO -    Loaded 3 boxes from COCO JSON
2025/12/09 07:59:50 - mmengine - INFO -     Box 0: [39, 187, 84.417, 259.837], label: 1
2025/12/09 07:59:50 - mmengine - INFO -     Box 1: [55, 15, 255.455, 175.959], label: 0
2025/12/09 07:59:50 - mmengine - INFO -     Box 2: [205, 94, 250.971, 172.89499999999998], label: 1
2025/12/09 07:59:50 - mmengine - INFO - [BBOX DEBUG 3] File: S71_Image__2025-09-22__11-06-14_bright_6_crop_3_jpg.rf.c9f1fd3e4181504e86a2b75dbdd0cbf0.jpg, Image shape: 720x256
2025/12/09 07:59:50 - mmengine - INFO -    Loaded 2 boxes from COCO JSON
2025/12/09 07:59:50 - mmengine - INFO -     Box 0: [89, 109, 209.494, 488.883], label: 2
2025/12/09 07:59:50 - mmengine - INFO -     Box 1: [145, 0, 235.24, 65.349], label: 2
2025/12/09 07:59:50 - mmengine - INFO - [BBOX DEBUG 1] File: S117_Image__2025-09-22__16-28-07_bright_3_crop_1_jpg.rf.90db94cca0ae0ea208c961e4d9d2c130.jpg, Image shape: 720x256
2025/12/09 07:59:50 - mmengine - INFO -    Loaded 1 boxes from COCO JSON
2025/12/09 07:59:50 - mmengine - INFO -     Box 0: [67, 379, 138.22, 598.331], label: 2
2025/12/09 07:59:50 - mmengine - INFO - [BBOX DEBUG 3] File: S131_Image__2025-09-23__14-18-52_bright_2_crop_3_jpg.rf.87a9e5de688eaddc69c109e55784d33c.jpg, Image shape: 720x256
2025/12/09 07:59:50 - mmengine - INFO -    Loaded 3 boxes from COCO JSON
2025/12/09 07:59:50 - mmengine - INFO -     Box 0: [41, 193, 63.677, 241.602], label: 1
2025/12/09 07:59:50 - mmengine - INFO -     Box 1: [133, 93, 188.609, 174.445], label: 1
2025/12/09 07:59:50 - mmengine - INFO -     Box 2: [30, 40, 235.217, 171.881], label: 0
2025/12/09 07:59:50 - mmengine - INFO - [BBOX DEBUG 1] File: S56_Image__2025-09-05__14-16-25_bright_2_crop_1_jpg.rf.0c08d634147a6892148e3764fc061522.jpg, Image shape: 720x256
2025/12/09 07:59:50 - mmengine - INFO -    Loaded 2 boxes from COCO JSON
2025/12/09 07:59:50 - mmengine - INFO -     Box 0: [54, 84, 215.71, 257.97], label: 0
2025/12/09 07:59:50 - mmengine - INFO -     Box 1: [52, 299, 144.12, 552.56], label: 3
2025/12/09 07:59:50 - mmengine - INFO - [BBOX DEBUG 2] File: S56_Image__2025-09-05__14-16-25_bright_2_crop_2_jpg.rf.74e6e619f3b947de6e1bbaa73475706a.jpg, Image shape: 720x256
2025/12/09 07:59:50 - mmengine - INFO -    Loaded 2 boxes from COCO JSON
2025/12/09 07:59:50 - mmengine - INFO -     Box 0: [68, 378, 197.6, 618.1], label: 3
2025/12/09 07:59:50 - mmengine - INFO -     Box 1: [52, 80, 251.38, 316.7], label: 0
2025/12/09 07:59:50 - mmengine - INFO - [BBOX DEBUG 3] File: S56_Image__2025-09-05__14-16-25_bright_2_crop_3_jpg.rf.32afa02f05ce946a147d405d81370b56.jpg, Image shape: 720x256
2025/12/09 07:59:50 - mmengine - INFO -    Loaded 2 boxes from COCO JSON
2025/12/09 07:59:50 - mmengine - INFO -     Box 0: [47, 474, 154.6, 622.54], label: 3
2025/12/09 07:59:50 - mmengine - INFO -     Box 1: [63, 50, 237.8, 274.65], label: 0
2025/12/09 07:59:50 - mmengine - INFO - [BBOX DEBUG 2] File: S117_Image__2025-09-22__16-28-07_bright_3_crop_7_jpg.rf.ece2f9b94d606f992eef6b8664fceda3.jpg, Image shape: 720x256
2025/12/09 07:59:50 - mmengine - INFO -    Loaded 1 boxes from COCO JSON
2025/12/09 07:59:50 - mmengine - INFO -     Box 0: [171, 363, 222.42000000000002, 554.938], label: 2
2025/12/09 07:59:50 - mmengine - INFO - [BBOX DEBUG 3] File: S117_Image__2025-09-22__16-28-07_bright_3_crop_8_jpg.rf.911f240f8b10cf1a0b6ded45d448603a.jpg, Image shape: 720x256
2025/12/09 07:59:50 - mmengine - INFO -    Loaded 1 boxes from COCO JSON
2025/12/09 07:59:50 - mmengine - INFO -     Box 0: [100, 381, 185.82999999999998, 580.34], label: 2
Name of parameter - Initialization information

student.backbone.backbone.conv1.weight - torch.Size([64, 3, 7, 7]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.0.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.0.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.0.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.0.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.0.bn3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.0.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.0.downsample.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.0.downsample.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.1.bn3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.1.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.2.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.2.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.2.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.2.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.2.bn3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.2.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.0.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.0.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.0.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.0.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.0.bn3.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.0.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.0.downsample.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.0.downsample.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.1.bn3.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.1.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.2.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.2.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.2.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.2.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.2.bn3.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.2.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.3.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.3.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.3.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.3.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.3.bn3.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.3.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.0.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.0.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.0.downsample.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.0.downsample.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.1.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.1.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.2.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.2.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.2.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.2.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.2.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.2.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.3.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.3.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.3.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.3.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.3.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.3.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.4.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.4.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.4.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.4.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.4.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.4.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.5.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.5.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.5.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.5.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.5.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.5.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.0.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.0.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.0.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.0.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.0.bn3.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.0.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.0.downsample.1.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.0.downsample.1.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.1.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.1.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.1.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.1.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.1.bn3.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.1.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.2.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.2.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.2.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.2.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.2.bn3.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.2.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.residual_alpha - torch.Size([]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.layers.0.self_attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.layers.0.self_attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.layers.0.self_attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.layers.0.self_attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.layers.0.linear1.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.layers.0.linear1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.layers.0.linear2.weight - torch.Size([256, 512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.layers.0.linear2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.layers.0.norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.layers.0.norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.layers.0.norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.layers.0.norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.pooling_queries.1024 - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.pooling_queries.2048 - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.pooling_queries.256 - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.pooling_queries.512 - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.upsample_queries.128x128 - torch.Size([16384, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.upsample_queries.32x32 - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.upsample_queries.64x64 - torch.Size([4096, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.neck.lateral_convs.0.conv.weight - torch.Size([256, 256, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

student.neck.lateral_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.neck.lateral_convs.1.conv.weight - torch.Size([256, 512, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

student.neck.lateral_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.neck.lateral_convs.2.conv.weight - torch.Size([256, 1024, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

student.neck.lateral_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.neck.lateral_convs.3.conv.weight - torch.Size([256, 2048, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

student.neck.lateral_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

student.neck.fpn_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

student.neck.fpn_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

student.neck.fpn_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.neck.fpn_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

student.neck.fpn_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.rpn_head.rpn_conv.weight - torch.Size([256, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=0 

student.rpn_head.rpn_conv.bias - torch.Size([256]): 
NormalInit: mean=0, std=0.01, bias=0 

student.rpn_head.rpn_cls.weight - torch.Size([15, 256, 1, 1]): 
NormalInit: mean=0, std=0.01, bias=0 

student.rpn_head.rpn_cls.bias - torch.Size([15]): 
NormalInit: mean=0, std=0.01, bias=0 

student.rpn_head.rpn_reg.weight - torch.Size([60, 256, 1, 1]): 
NormalInit: mean=0, std=0.01, bias=0 

student.rpn_head.rpn_reg.bias - torch.Size([60]): 
NormalInit: mean=0, std=0.01, bias=0 

student.roi_head.bbox_head.fc_cls.weight - torch.Size([6, 1024]): 
NormalInit: mean=0, std=0.01, bias=0 

student.roi_head.bbox_head.fc_cls.bias - torch.Size([6]): 
NormalInit: mean=0, std=0.01, bias=0 

student.roi_head.bbox_head.fc_reg.weight - torch.Size([20, 1024]): 
NormalInit: mean=0, std=0.001, bias=0 

student.roi_head.bbox_head.fc_reg.bias - torch.Size([20]): 
NormalInit: mean=0, std=0.001, bias=0 

student.roi_head.bbox_head.shared_fcs.0.weight - torch.Size([1024, 12544]): 
XavierInit: gain=1, distribution=uniform, bias=0 

student.roi_head.bbox_head.shared_fcs.0.bias - torch.Size([1024]): 
XavierInit: gain=1, distribution=uniform, bias=0 

student.roi_head.bbox_head.shared_fcs.1.weight - torch.Size([1024, 1024]): 
XavierInit: gain=1, distribution=uniform, bias=0 

student.roi_head.bbox_head.shared_fcs.1.bias - torch.Size([1024]): 
XavierInit: gain=1, distribution=uniform, bias=0 

teacher.backbone.backbone.conv1.weight - torch.Size([64, 3, 7, 7]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.0.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.0.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.0.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.0.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.0.bn3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.0.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.0.downsample.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.0.downsample.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.1.bn3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.1.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.2.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.2.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.2.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.2.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.2.bn3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.2.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.0.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.0.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.0.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.0.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.0.bn3.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.0.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.0.downsample.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.0.downsample.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.1.bn3.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.1.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.2.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.2.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.2.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.2.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.2.bn3.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.2.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.3.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.3.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.3.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.3.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.3.bn3.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.3.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.0.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.0.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.0.downsample.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.0.downsample.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.1.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.1.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.2.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.2.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.2.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.2.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.2.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.2.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.3.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.3.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.3.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.3.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.3.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.3.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.4.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.4.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.4.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.4.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.4.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.4.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.5.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.5.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.5.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.5.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.5.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.5.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.0.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.0.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.0.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.0.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.0.bn3.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.0.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.0.downsample.1.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.0.downsample.1.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.1.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.1.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.1.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.1.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.1.bn3.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.1.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.2.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.2.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.2.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.2.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.2.bn3.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.2.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.residual_alpha - torch.Size([]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.layers.0.self_attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.layers.0.self_attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.layers.0.self_attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.layers.0.self_attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.layers.0.linear1.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.layers.0.linear1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.layers.0.linear2.weight - torch.Size([256, 512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.layers.0.linear2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.layers.0.norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.layers.0.norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.layers.0.norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.layers.0.norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.pooling_queries.1024 - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.pooling_queries.2048 - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.pooling_queries.256 - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.pooling_queries.512 - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.upsample_queries.128x128 - torch.Size([16384, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.upsample_queries.32x32 - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.upsample_queries.64x64 - torch.Size([4096, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.neck.lateral_convs.0.conv.weight - torch.Size([256, 256, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

teacher.neck.lateral_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.neck.lateral_convs.1.conv.weight - torch.Size([256, 512, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

teacher.neck.lateral_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.neck.lateral_convs.2.conv.weight - torch.Size([256, 1024, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

teacher.neck.lateral_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.neck.lateral_convs.3.conv.weight - torch.Size([256, 2048, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

teacher.neck.lateral_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

teacher.neck.fpn_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

teacher.neck.fpn_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

teacher.neck.fpn_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.neck.fpn_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

teacher.neck.fpn_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.rpn_head.rpn_conv.weight - torch.Size([256, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=0 

teacher.rpn_head.rpn_conv.bias - torch.Size([256]): 
NormalInit: mean=0, std=0.01, bias=0 

teacher.rpn_head.rpn_cls.weight - torch.Size([15, 256, 1, 1]): 
NormalInit: mean=0, std=0.01, bias=0 

teacher.rpn_head.rpn_cls.bias - torch.Size([15]): 
NormalInit: mean=0, std=0.01, bias=0 

teacher.rpn_head.rpn_reg.weight - torch.Size([60, 256, 1, 1]): 
NormalInit: mean=0, std=0.01, bias=0 

teacher.rpn_head.rpn_reg.bias - torch.Size([60]): 
NormalInit: mean=0, std=0.01, bias=0 

teacher.roi_head.bbox_head.fc_cls.weight - torch.Size([6, 1024]): 
NormalInit: mean=0, std=0.01, bias=0 

teacher.roi_head.bbox_head.fc_cls.bias - torch.Size([6]): 
NormalInit: mean=0, std=0.01, bias=0 

teacher.roi_head.bbox_head.fc_reg.weight - torch.Size([20, 1024]): 
NormalInit: mean=0, std=0.001, bias=0 

teacher.roi_head.bbox_head.fc_reg.bias - torch.Size([20]): 
NormalInit: mean=0, std=0.001, bias=0 

teacher.roi_head.bbox_head.shared_fcs.0.weight - torch.Size([1024, 12544]): 
XavierInit: gain=1, distribution=uniform, bias=0 

teacher.roi_head.bbox_head.shared_fcs.0.bias - torch.Size([1024]): 
XavierInit: gain=1, distribution=uniform, bias=0 

teacher.roi_head.bbox_head.shared_fcs.1.weight - torch.Size([1024, 1024]): 
XavierInit: gain=1, distribution=uniform, bias=0 

teacher.roi_head.bbox_head.shared_fcs.1.bias - torch.Size([1024]): 
XavierInit: gain=1, distribution=uniform, bias=0 
2025/12/09 07:59:53 - mmengine - WARNING - "FileClient" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io
2025/12/09 07:59:53 - mmengine - WARNING - "HardDiskBackend" is the alias of "LocalBackend" and the former will be deprecated in future.
2025/12/09 07:59:53 - mmengine - INFO - Checkpoints will be saved to /home/coder/data/trong/KLTN/Soft_Teacher/work_dirs/soft_teacher_custom_multi_view.
2025/12/09 07:59:53 - mmengine - INFO - [Supervised Step 0] Groups: 2 , G0:7box | G1:14box
2025/12/09 07:59:59 - mmengine - INFO - [Supervised Step 1] Groups: 2 , G0:9box | G1:21box
2025/12/09 08:00:01 - mmengine - INFO - [Supervised Step 2] Groups: 2 , G0:5box | G1:3box
2025/12/09 08:00:04 - mmengine - INFO - [Supervised Step 3] Groups: 2 , G0:9box | G1:12box
2025/12/09 08:00:08 - mmengine - INFO - [Supervised Step 4] Groups: 2 , G0:13box | G1:13box
2025/12/09 08:00:11 - mmengine - INFO - [Supervised Step 5] Groups: 2 , G0:5box | G1:8box
2025/12/09 08:00:14 - mmengine - INFO - [Supervised Step 6] Groups: 2 , G0:8box | G1:3box
2025/12/09 08:00:17 - mmengine - INFO - [Supervised Step 7] Groups: 2 , G0:11box | G1:14box
2025/12/09 08:00:20 - mmengine - INFO - [Supervised Step 8] Groups: 2 , G0:3box | G1:4box
2025/12/09 08:00:24 - mmengine - INFO - [Supervised Step 9] Groups: 2 , G0:2box | G1:20box
2025/12/09 08:00:56 - mmengine - INFO - [Teacher Predictions] Total boxes: 160, Score range: [0.946, 1.000], Mean: 0.981, Median: 0.982
2025/12/09 08:00:56 - mmengine - INFO - [After Filtering] Threshold: 0.4, Kept: 80/160 (50.0%), Avg: 10.0 boxes/img
2025/12/09 08:00:56 - mmengine - INFO - [Before uncertainty] Box counts per view: [10, 10, 10, 10, 10, 10, 10, 10]
2025/12/09 08:00:56 - mmengine - INFO - [After reg_uncs] Box counts: [10, 10, 10, 10, 10, 10, 10, 10], reg_uncs sizes: [10, 10, 10, 10, 10, 10, 10, 10]
2025/12/09 08:00:56 - mmengine - INFO - [After reg_uncs] reg_uncs shapes: ['(10,)', '(10,)', '(10,)', '(10,)', '(10,)', '(10,)', '(10,)', '(10,)']
2025/12/09 08:00:57 - mmengine - INFO - [After cv_uncs] cv_uncs sizes: [10, 10, 10, 10, 10, 10, 10, 10]
2025/12/09 08:00:57 - mmengine - INFO - [After cv_uncs] cv_uncs shapes: ['(10,)', '(10,)', '(10,)', '(10,)', '(10,)', '(10,)', '(10,)', '(10,)']
2025/12/09 08:00:58 - mmengine - INFO - [Cross-View Uncertainty] Groups: 1, G0:80box(unc=0.072)
2025/12/09 08:02:38 - mmengine - INFO - Epoch(train) [1][ 50/125]  lr: 7.4257e-06  eta: 3 days, 19:45:05  time: 3.3047  data_time: 0.0289  memory: 25318  loss: 2.2490  sup_loss_rpn_cls: 0.2113  sup_loss_rpn_bbox: 0.8510  sup_loss_cls: 0.8030  sup_acc: 84.2105  sup_num_pred_fg: 5.0000  sup_num_pred_bg: 33.0000  sup_num_gt_fg: 6.0000  sup_num_gt_bg: 32.0000  sup_acc_fg: 0.0000  sup_acc_bg: 100.0000  sup_loss_bbox: 0.2868  unsup_loss_rpn_cls: 0.1275  unsup_loss_rpn_bbox: 0.9244  unsup_loss_cls: 3.3797  unsup_acc: 66.4303  unsup_num_pred_fg: 1.0000  unsup_num_pred_bg: 422.0000  unsup_num_gt_fg: 141.0000  unsup_num_gt_bg: 282.0000  unsup_acc_fg: 0.0000  unsup_acc_bg: 99.6454  unsup_loss_bbox: 0.4156
2025/12/09 08:02:38 - mmengine - INFO - [Supervised Step 50] Groups: 2 , G0:10box | G1:10box
2025/12/09 08:05:17 - mmengine - INFO - Epoch(train) [1][100/125]  lr: 9.9010e-06  eta: 3 days, 18:02:15  time: 3.1845  data_time: 0.0396  memory: 25318  loss: 1.8778  sup_loss_rpn_cls: 0.1720  sup_loss_rpn_bbox: 0.5446  sup_loss_cls: 0.3690  sup_acc: 61.2676  sup_num_pred_fg: 92.0000  sup_num_pred_bg: 334.0000  sup_num_gt_fg: 142.0000  sup_num_gt_bg: 284.0000  sup_acc_fg: 10.5634  sup_acc_bg: 86.6197  sup_loss_bbox: 0.7922  unsup_loss_rpn_cls: 0.1275  unsup_loss_rpn_bbox: 0.9244  unsup_loss_cls: 3.3797  unsup_acc: 66.4303  unsup_num_pred_fg: 1.0000  unsup_num_pred_bg: 422.0000  unsup_num_gt_fg: 141.0000  unsup_num_gt_bg: 282.0000  unsup_acc_fg: 0.0000  unsup_acc_bg: 99.6454  unsup_loss_bbox: 0.4156
2025/12/09 08:05:17 - mmengine - INFO - [Supervised Step 100] Groups: 2 , G0:13box | G1:7box
2025/12/09 08:06:41 - mmengine - INFO - Exp name: soft_teacher_custom_multi_view_20251209_075939
2025/12/09 08:08:00 - mmengine - INFO - Epoch(train) [1][150/125]  lr: 1.2376e-05  eta: 3 days, 18:11:25  time: 3.2660  data_time: 0.0431  memory: 25317  loss: 2.1656  sup_loss_rpn_cls: 0.1528  sup_loss_rpn_bbox: 0.4845  sup_loss_cls: 0.3817  sup_acc: 64.8968  sup_num_pred_fg: 40.0000  sup_num_pred_bg: 299.0000  sup_num_gt_fg: 111.0000  sup_num_gt_bg: 228.0000  sup_acc_fg: 6.3063  sup_acc_bg: 93.4211  sup_loss_bbox: 1.0725  unsup_loss_rpn_cls: 0.1074  unsup_loss_rpn_bbox: 0.8976  unsup_loss_cls: 2.8021  unsup_acc: 60.3741  unsup_num_pred_fg: 43.0000  unsup_num_pred_bg: 545.0000  unsup_num_gt_fg: 196.0000  unsup_num_gt_bg: 392.0000  unsup_acc_fg: 1.5306  unsup_acc_bg: 89.7959  unsup_loss_bbox: 0.4682
2025/12/09 08:08:00 - mmengine - INFO - [Supervised Step 150] Groups: 2 , G0:14box | G1:7box
2025/12/09 08:10:39 - mmengine - INFO - Epoch(train) [1][200/125]  lr: 1.4851e-05  eta: 3 days, 17:40:36  time: 3.1842  data_time: 0.0419  memory: 25317  loss: 2.1936  sup_loss_rpn_cls: 0.1443  sup_loss_rpn_bbox: 0.4770  sup_loss_cls: 0.3774  sup_acc: 66.3830  sup_num_pred_fg: 16.0000  sup_num_pred_bg: 219.0000  sup_num_gt_fg: 77.0000  sup_num_gt_bg: 158.0000  sup_acc_fg: 3.8961  sup_acc_bg: 96.8354  sup_loss_bbox: 1.1531  unsup_loss_rpn_cls: 0.1096  unsup_loss_rpn_bbox: 0.8065  unsup_loss_cls: 2.1924  unsup_acc: 63.7778  unsup_num_pred_fg: 17.0000  unsup_num_pred_bg: 433.0000  unsup_num_gt_fg: 150.0000  unsup_num_gt_bg: 300.0000  unsup_acc_fg: 0.0000  unsup_acc_bg: 95.6667  unsup_loss_bbox: 0.4396
2025/12/09 08:10:40 - mmengine - INFO - [Supervised Step 200] Groups: 2 , G0:12box | G1:9box
2025/12/09 08:13:18 - mmengine - INFO - Epoch(train) [1][250/125]  lr: 1.7327e-05  eta: 3 days, 17:14:28  time: 3.1644  data_time: 0.0407  memory: 25318  loss: 2.2115  sup_loss_rpn_cls: 0.1393  sup_loss_rpn_bbox: 0.4610  sup_loss_cls: 0.3530  sup_acc: 64.4860  sup_num_pred_fg: 64.0000  sup_num_pred_bg: 364.0000  sup_num_gt_fg: 140.0000  sup_num_gt_bg: 288.0000  sup_acc_fg: 9.2857  sup_acc_bg: 91.3194  sup_loss_bbox: 1.2178  unsup_loss_rpn_cls: 0.1124  unsup_loss_rpn_bbox: 0.7545  unsup_loss_cls: 1.8802  unsup_acc: 62.6667  unsup_num_pred_fg: 23.0000  unsup_num_pred_bg: 352.0000  unsup_num_gt_fg: 125.0000  unsup_num_gt_bg: 250.0000  unsup_acc_fg: 0.0000  unsup_acc_bg: 94.0000  unsup_loss_bbox: 0.4173
2025/12/09 08:13:18 - mmengine - INFO - [Supervised Step 250] Groups: 2 , G0:10box | G1:14box
2025/12/09 08:15:58 - mmengine - INFO - Epoch(train) [1][300/125]  lr: 1.9802e-05  eta: 3 days, 17:07:13  time: 3.2042  data_time: 0.0419  memory: 25318  loss: 2.1954  sup_loss_rpn_cls: 0.1352  sup_loss_rpn_bbox: 0.4381  sup_loss_cls: 0.3483  sup_acc: 65.1685  sup_num_pred_fg: 20.0000  sup_num_pred_bg: 158.0000  sup_num_gt_fg: 56.0000  sup_num_gt_bg: 122.0000  sup_acc_fg: 8.9286  sup_acc_bg: 90.9836  sup_loss_bbox: 1.2289  unsup_loss_rpn_cls: 0.1147  unsup_loss_rpn_bbox: 0.7608  unsup_loss_cls: 1.6580  unsup_acc: 62.2066  unsup_num_pred_fg: 19.0000  unsup_num_pred_bg: 407.0000  unsup_num_gt_fg: 142.0000  unsup_num_gt_bg: 284.0000  unsup_acc_fg: 0.0000  unsup_acc_bg: 93.3099  unsup_loss_bbox: 0.4474
2025/12/09 08:15:58 - mmengine - INFO - [Supervised Step 300] Groups: 2 , G0:8box | G1:4box
2025/12/09 08:18:42 - mmengine - INFO - Epoch(train) [1][350/125]  lr: 2.2277e-05  eta: 3 days, 17:19:58  time: 3.2831  data_time: 0.0474  memory: 25318  loss: 2.1789  sup_loss_rpn_cls: 0.1337  sup_loss_rpn_bbox: 0.4384  sup_loss_cls: 0.3389  sup_acc: 63.6785  sup_num_pred_fg: 75.0000  sup_num_pred_bg: 572.0000  sup_num_gt_fg: 215.0000  sup_num_gt_bg: 432.0000  sup_acc_fg: 7.9070  sup_acc_bg: 91.4352  sup_loss_bbox: 1.2408  unsup_loss_rpn_cls: 0.1154  unsup_loss_rpn_bbox: 0.7207  unsup_loss_cls: 1.4511  unsup_acc: 68.8073  unsup_num_pred_fg: 47.0000  unsup_num_pred_bg: 280.0000  unsup_num_gt_fg: 109.0000  unsup_num_gt_bg: 218.0000  unsup_acc_fg: 22.0183  unsup_acc_bg: 92.2018  unsup_loss_bbox: 0.4235
2025/12/09 08:18:42 - mmengine - INFO - [Supervised Step 350] Groups: 2 , G0:2box | G1:12box
2025/12/09 08:21:22 - mmengine - INFO - Epoch(train) [1][400/125]  lr: 2.4752e-05  eta: 3 days, 17:10:48  time: 3.1961  data_time: 0.0475  memory: 25318  loss: 2.1552  sup_loss_rpn_cls: 0.1325  sup_loss_rpn_bbox: 0.4339  sup_loss_cls: 0.3444  sup_acc: 66.5323  sup_num_pred_fg: 14.0000  sup_num_pred_bg: 730.0000  sup_num_gt_fg: 246.0000  sup_num_gt_bg: 498.0000  sup_acc_fg: 2.0325  sup_acc_bg: 98.3936  sup_loss_bbox: 1.2444  unsup_loss_rpn_cls: 0.1154  unsup_loss_rpn_bbox: 0.7207  unsup_loss_cls: 1.4511  unsup_acc: 68.8073  unsup_num_pred_fg: 47.0000  unsup_num_pred_bg: 280.0000  unsup_num_gt_fg: 109.0000  unsup_num_gt_bg: 218.0000  unsup_acc_fg: 22.0183  unsup_acc_bg: 92.2018  unsup_loss_bbox: 0.4235
2025/12/09 08:21:22 - mmengine - INFO - [Supervised Step 400] Groups: 2 , G0:10box | G1:7box
2025/12/09 08:24:08 - mmengine - INFO - Epoch(train) [1][450/125]  lr: 2.7228e-05  eta: 3 days, 17:25:58  time: 3.3201  data_time: 0.0432  memory: 25318  loss: 2.2080  sup_loss_rpn_cls: 0.1322  sup_loss_rpn_bbox: 0.4240  sup_loss_cls: 0.3401  sup_acc: 45.5172  sup_num_pred_fg: 257.0000  sup_num_pred_bg: 468.0000  sup_num_gt_fg: 241.0000  sup_num_gt_bg: 484.0000  sup_acc_fg: 12.0332  sup_acc_bg: 62.1901  sup_loss_bbox: 1.2196  unsup_loss_rpn_cls: 0.1200  unsup_loss_rpn_bbox: 0.6858  unsup_loss_cls: 1.2883  unsup_acc: 64.7679  unsup_num_pred_fg: 10.0000  unsup_num_pred_bg: 464.0000  unsup_num_gt_fg: 158.0000  unsup_num_gt_bg: 316.0000  unsup_acc_fg: 0.0000  unsup_acc_bg: 97.1519  unsup_loss_bbox: 0.5146
2025/12/09 08:24:08 - mmengine - INFO - [Supervised Step 450] Groups: 2 , G0:7box | G1:12box
2025/12/09 08:26:51 - mmengine - INFO - Epoch(train) [1][500/125]  lr: 2.9703e-05  eta: 3 days, 17:27:22  time: 3.2588  data_time: 0.0419  memory: 25318  loss: 2.1567  sup_loss_rpn_cls: 0.1258  sup_loss_rpn_bbox: 0.4233  sup_loss_cls: 0.3291  sup_acc: 69.8462  sup_num_pred_fg: 22.0000  sup_num_pred_bg: 303.0000  sup_num_gt_fg: 105.0000  sup_num_gt_bg: 220.0000  sup_acc_fg: 13.3333  sup_acc_bg: 96.8182  sup_loss_bbox: 1.2451  unsup_loss_rpn_cls: 0.1202  unsup_loss_rpn_bbox: 0.6684  unsup_loss_cls: 1.1979  unsup_acc: 66.2005  unsup_num_pred_fg: 24.0000  unsup_num_pred_bg: 405.0000  unsup_num_gt_fg: 143.0000  unsup_num_gt_bg: 286.0000  unsup_acc_fg: 6.2937  unsup_acc_bg: 96.1538  unsup_loss_bbox: 0.5177
2025/12/09 08:26:51 - mmengine - INFO - [Supervised Step 500] Groups: 2 , G0:4box | G1:10box
2025/12/09 08:29:36 - mmengine - INFO - Epoch(train) [1][550/125]  lr: 3.2178e-05  eta: 3 days, 17:33:26  time: 3.2949  data_time: 0.0403  memory: 25318  loss: 2.1794  sup_loss_rpn_cls: 0.1298  sup_loss_rpn_bbox: 0.4225  sup_loss_cls: 0.3556  sup_acc: 66.4587  sup_num_pred_fg: 12.0000  sup_num_pred_bg: 629.0000  sup_num_gt_fg: 213.0000  sup_num_gt_bg: 428.0000  sup_acc_fg: 1.4085  sup_acc_bg: 98.8318  sup_loss_bbox: 1.2248  unsup_loss_rpn_cls: 0.1218  unsup_loss_rpn_bbox: 0.6469  unsup_loss_cls: 1.2038  unsup_acc: 61.1621  unsup_num_pred_fg: 35.0000  unsup_num_pred_bg: 292.0000  unsup_num_gt_fg: 109.0000  unsup_num_gt_bg: 218.0000  unsup_acc_fg: 0.0000  unsup_acc_bg: 91.7431  unsup_loss_bbox: 0.5150
2025/12/09 08:29:36 - mmengine - INFO - [Supervised Step 550] Groups: 2 , G0:3box | G1:14box
2025/12/09 08:32:17 - mmengine - INFO - Epoch(train) [1][600/125]  lr: 3.4653e-05  eta: 3 days, 17:28:53  time: 3.2284  data_time: 0.0359  memory: 25318  loss: 2.1116  sup_loss_rpn_cls: 0.1277  sup_loss_rpn_bbox: 0.4208  sup_loss_cls: 0.3244  sup_acc: 66.3265  sup_num_pred_fg: 5.0000  sup_num_pred_bg: 877.0000  sup_num_gt_fg: 294.0000  sup_num_gt_bg: 588.0000  sup_acc_fg: 0.3401  sup_acc_bg: 99.3197  sup_loss_bbox: 1.2386  unsup_loss_rpn_cls: 0.1218  unsup_loss_rpn_bbox: 0.6469  unsup_loss_cls: 1.2038  unsup_acc: 61.1621  unsup_num_pred_fg: 35.0000  unsup_num_pred_bg: 292.0000  unsup_num_gt_fg: 109.0000  unsup_num_gt_bg: 218.0000  unsup_acc_fg: 0.0000  unsup_acc_bg: 91.7431  unsup_loss_bbox: 0.5150
2025/12/09 08:32:17 - mmengine - INFO - [Supervised Step 600] Groups: 2 , G0:12box | G1:6box
2025/12/09 08:35:02 - mmengine - INFO - Epoch(train) [1][650/125]  lr: 3.7129e-05  eta: 3 days, 17:34:19  time: 3.3046  data_time: 0.0412  memory: 25352  loss: 2.0855  sup_loss_rpn_cls: 0.1292  sup_loss_rpn_bbox: 0.4144  sup_loss_cls: 0.3375  sup_acc: 64.2183  sup_num_pred_fg: 47.0000  sup_num_pred_bg: 906.0000  sup_num_gt_fg: 317.0000  sup_num_gt_bg: 636.0000  sup_acc_fg: 2.8391  sup_acc_bg: 94.8113  sup_loss_bbox: 1.2169  unsup_loss_rpn_cls: 0.1228  unsup_loss_rpn_bbox: 0.6285  unsup_loss_cls: 1.1317  unsup_acc: 67.1463  unsup_num_pred_fg: 56.0000  unsup_num_pred_bg: 778.0000  unsup_num_gt_fg: 278.0000  unsup_num_gt_bg: 556.0000  unsup_acc_fg: 7.9137  unsup_acc_bg: 96.7626  unsup_loss_bbox: 0.5132
2025/12/09 08:35:05 - mmengine - INFO - [Supervised Step 650] Groups: 2 , G0:6box | G1:3box
