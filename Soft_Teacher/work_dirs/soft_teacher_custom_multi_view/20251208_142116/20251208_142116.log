2025/12/08 14:21:17 - mmengine - INFO - 
------------------------------------------------------------
System environment:
    sys.platform: linux
    Python: 3.9.19 (main, May  6 2024, 19:43:03) [GCC 11.2.0]
    CUDA available: True
    MUSA available: False
    numpy_random_seed: 1093373644
    GPU 0: NVIDIA RTX A6000
    CUDA_HOME: /usr/local/cuda
    NVCC: Cuda compilation tools, release 12.6, V12.6.85
    GCC: gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
    PyTorch: 2.0.0+cu117
    PyTorch compiling details: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

    TorchVision: 0.15.1+cu117
    OpenCV: 4.11.0
    MMEngine: 0.11.0rc0

Runtime environment:
    cudnn_benchmark: False
    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}
    dist_cfg: {'backend': 'nccl'}
    seed: 1093373644
    Distributed launcher: none
    Distributed training: False
    GPU number: 1
------------------------------------------------------------

2025/12/08 14:21:20 - mmengine - INFO - Config:
backend_args = None
batch_size = 2
branch_field = [
    'sup',
    'unsup_teacher',
    'unsup_student',
]
color_space = [
    [
        dict(prob=0.5, type='AutoContrast'),
    ],
    [
        dict(prob=0.4, type='Equalize'),
    ],
    [
        dict(level=7, max_mag=1.25, min_mag=0.8, prob=0.6, type='Brightness'),
    ],
    [
        dict(level=7, max_mag=1.3, min_mag=0.8, prob=0.6, type='Contrast'),
    ],
    [
        dict(level=6, max_mag=1.5, min_mag=0.8, prob=0.4, type='Sharpness'),
    ],
    [
        dict(level=5, max_mag=7.0, min_mag=5.0, prob=0.2, type='Posterize'),
    ],
]
custom_hooks = [
    dict(momentum=0.999, type='MeanTeacherHook'),
]
custom_imports = dict(
    allow_failed_imports=False,
    imports=[
        'mmdet.models.utils.multi_view',
        'mmdet.models.utils.multi_view_transformer',
        'mmdet.datasets.wrappers.multi_view_from_folder',
    ])
data_root = '/home/coder/data/trong/KLTN/Soft_Teacher/data_drill/'
dataset_type = 'CocoDataset'
default_hooks = dict(
    checkpoint=dict(
        interval=1,
        max_keep_ckpts=1,
        rule='greater',
        save_best='teacher/coco/bbox_mAP_50',
        type='CheckpointHook'),
    logger=dict(interval=50, type='LoggerHook'),
    param_scheduler=dict(type='ParamSchedulerHook'),
    sampler_seed=dict(type='DistSamplerSeedHook'),
    timer=dict(type='IterTimerHook'),
    visualization=dict(type='DetVisualizationHook'))
default_scope = 'mmdet'
detector = dict(
    backbone=dict(
        backbone=dict(
            depth=50,
            frozen_stages=1,
            init_cfg=dict(
                checkpoint='open-mmlab://detectron2/resnet50_caffe',
                type='Pretrained'),
            norm_cfg=dict(requires_grad=False, type='BN'),
            norm_eval=True,
            num_stages=4,
            out_indices=(
                0,
                1,
                2,
                3,
            ),
            style='caffe',
            type='ResNet'),
        concat_reduce=False,
        fusion='mvvit',
        mvvit=dict(
            dropout=0.2,
            embed_dim=256,
            mlp_ratio=2.0,
            num_heads=4,
            num_layers=1,
            spatial_attention='moderate',
            type='MVViT',
            use_gradient_checkpointing=True,
            use_layer_norm=True),
        type='MultiViewBackbone',
        views_per_sample=8),
    data_preprocessor=dict(
        bgr_to_rgb=False,
        mean=[
            103.53,
            116.28,
            123.675,
        ],
        pad_size_divisor=8,
        std=[
            1.0,
            1.0,
            1.0,
        ],
        type='DetDataPreprocessor'),
    neck=dict(
        in_channels=[
            256,
            512,
            1024,
            2048,
        ],
        num_outs=5,
        out_channels=256,
        type='FPN'),
    roi_head=dict(
        bbox_head=dict(
            bbox_coder=dict(
                target_means=[
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                ],
                target_stds=[
                    0.1,
                    0.1,
                    0.2,
                    0.2,
                ],
                type='DeltaXYWHBBoxCoder'),
            fc_out_channels=1024,
            in_channels=256,
            loss_bbox=dict(loss_weight=1.0, type='L1Loss'),
            loss_cls=dict(
                alpha=0.5,
                gamma=2.2,
                loss_weight=1.5,
                type='FocalLoss',
                use_sigmoid=True),
            num_classes=5,
            reg_class_agnostic=False,
            roi_feat_size=7,
            type='Shared2FCBBoxHead'),
        bbox_roi_extractor=dict(
            featmap_strides=[
                4,
                8,
                16,
                32,
            ],
            out_channels=256,
            roi_layer=dict(output_size=7, sampling_ratio=0, type='RoIAlign'),
            type='SingleRoIExtractor'),
        type='StandardRoIHead'),
    rpn_head=dict(
        anchor_generator=dict(
            ratios=[
                0.3,
                0.45,
                0.6,
                1.0,
                2.0,
            ],
            scales=[
                8,
                16,
                32,
            ],
            strides=[
                4,
                8,
                16,
                32,
                64,
            ],
            type='AnchorGenerator'),
        bbox_coder=dict(
            target_means=[
                0.0,
                0.0,
                0.0,
                0.0,
            ],
            target_stds=[
                1.0,
                1.0,
                1.0,
                1.0,
            ],
            type='DeltaXYWHBBoxCoder'),
        feat_channels=256,
        in_channels=256,
        loss_bbox=dict(loss_weight=1.0, type='L1Loss'),
        loss_cls=dict(
            alpha=0.5,
            gamma=2.0,
            loss_weight=2.0,
            type='FocalLoss',
            use_sigmoid=True),
        type='RPNHead'),
    test_cfg=dict(
        rcnn=dict(
            max_per_img=20,
            nms=dict(iou_threshold=0.5, type='nms'),
            score_thr=0.5),
        rpn=dict(
            max_per_img=1000,
            min_bbox_size=8,
            nms=dict(iou_threshold=0.7, type='nms'),
            nms_pre=2000)),
    train_cfg=dict(
        rcnn=dict(
            assigner=dict(
                ignore_iof_thr=-1,
                match_low_quality=False,
                min_pos_iou=0.5,
                neg_iou_thr=0.5,
                pos_iou_thr=0.5,
                type='MaxIoUAssigner'),
            debug=False,
            pos_weight=-1,
            sampler=dict(
                add_gt_as_proposals=True,
                neg_pos_ub=2,
                num=256,
                pos_fraction=0.6,
                type='RandomSampler')),
        rpn=dict(
            allowed_border=-1,
            assigner=dict(
                ignore_iof_thr=-1,
                match_low_quality=True,
                min_pos_iou=0.3,
                neg_iou_thr=0.3,
                pos_iou_thr=0.5,
                type='MaxIoUAssigner'),
            debug=False,
            pos_weight=-1,
            sampler=dict(
                add_gt_as_proposals=False,
                neg_pos_ub=2,
                num=256,
                pos_fraction=0.6,
                type='RandomSampler')),
        rpn_proposal=dict(
            max_per_img=1000,
            min_bbox_size=0,
            nms=dict(iou_threshold=0.7, type='nms'),
            nms_pre=2000)),
    type='FasterRCNN')
env_cfg = dict(
    cudnn_benchmark=False,
    dist_cfg=dict(backend='nccl'),
    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))
geometric = [
    [
        dict(level=7, max_mag=0.05, min_mag=0.0, prob=0.3, type='TranslateX'),
    ],
    [
        dict(level=7, max_mag=0.05, min_mag=0.0, prob=0.3, type='TranslateY'),
    ],
]
img_scale = (
    256,
    720,
)
labeled_dataset = dict(
    ann_file=
    '/home/coder/data/trong/KLTN/Soft_Teacher/data_drill/semi_anno_multiview/_annotations.coco.labeled.grouped@40.bright.json',
    backend_args=None,
    data_prefix=dict(img='train/'),
    data_root='/home/coder/data/trong/KLTN/Soft_Teacher/data_drill/',
    filter_cfg=dict(filter_empty_gt=True, min_size=4),
    metainfo=dict(
        classes=(
            'Broken',
            'Chipped',
            'Scratched',
            'Severe_Rust',
            'Tip_Wear',
        ),
        palette=[
            (
                134,
                34,
                255,
            ),
            (
                0,
                255,
                206,
            ),
            (
                255,
                128,
                0,
            ),
            (
                254,
                0,
                86,
            ),
            (
                199,
                252,
                0,
            ),
        ]),
    pipeline=[
        dict(
            backend_args=None,
            imdecode_backend='cv2',
            to_float32=False,
            type='LoadImageFromFile'),
        dict(type='LoadAnnotations', with_bbox=True),
        dict(keep_ratio=True, scale=(
            256,
            720,
        ), type='Resize'),
        dict(prob=0.5, type='RandomFlip'),
        dict(
            aug_num=1,
            aug_space=[
                [
                    dict(prob=0.5, type='AutoContrast'),
                ],
                [
                    dict(prob=0.4, type='Equalize'),
                ],
                [
                    dict(
                        level=7,
                        max_mag=1.25,
                        min_mag=0.8,
                        prob=0.6,
                        type='Brightness'),
                ],
                [
                    dict(
                        level=7,
                        max_mag=1.3,
                        min_mag=0.8,
                        prob=0.6,
                        type='Contrast'),
                ],
                [
                    dict(
                        level=6,
                        max_mag=1.5,
                        min_mag=0.8,
                        prob=0.4,
                        type='Sharpness'),
                ],
                [
                    dict(
                        level=5,
                        max_mag=7.0,
                        min_mag=5.0,
                        prob=0.2,
                        type='Posterize'),
                ],
            ],
            type='RandAugment'),
        dict(min_gt_bbox_wh=(
            4,
            4,
        ), type='FilterAnnotations'),
        dict(
            branch_field=[
                'sup',
                'unsup_teacher',
                'unsup_student',
            ],
            sup=dict(type='PackDetInputs'),
            type='MultiBranch'),
    ],
    type='MultiViewFromFolder',
    views_per_sample=8)
launcher = 'none'
load_from = None
log_level = 'INFO'
log_processor = dict(by_epoch=True, type='LogProcessor', window_size=50)
metainfo = dict(
    classes=(
        'Broken',
        'Chipped',
        'Scratched',
        'Severe_Rust',
        'Tip_Wear',
    ),
    palette=[
        (
            134,
            34,
            255,
        ),
        (
            0,
            255,
            206,
        ),
        (
            255,
            128,
            0,
        ),
        (
            254,
            0,
            86,
        ),
        (
            199,
            252,
            0,
        ),
    ])
model = dict(
    aggregate_views='mean',
    data_preprocessor=dict(
        data_preprocessor=dict(
            bgr_to_rgb=False,
            mean=[
                103.53,
                116.28,
                123.675,
            ],
            pad_size_divisor=8,
            std=[
                1.0,
                1.0,
                1.0,
            ],
            type='DetDataPreprocessor'),
        type='MultiBranchDataPreprocessor'),
    detector=dict(
        backbone=dict(
            backbone=dict(
                depth=50,
                frozen_stages=1,
                init_cfg=dict(
                    checkpoint='open-mmlab://detectron2/resnet50_caffe',
                    type='Pretrained'),
                norm_cfg=dict(requires_grad=False, type='BN'),
                norm_eval=True,
                num_stages=4,
                out_indices=(
                    0,
                    1,
                    2,
                    3,
                ),
                style='caffe',
                type='ResNet'),
            concat_reduce=False,
            fusion='mvvit',
            mvvit=dict(
                dropout=0.2,
                embed_dim=256,
                mlp_ratio=2.0,
                num_heads=4,
                num_layers=1,
                spatial_attention='moderate',
                type='MVViT',
                use_gradient_checkpointing=True,
                use_layer_norm=True),
            type='MultiViewBackbone',
            views_per_sample=8),
        data_preprocessor=dict(
            bgr_to_rgb=False,
            mean=[
                103.53,
                116.28,
                123.675,
            ],
            pad_size_divisor=8,
            std=[
                1.0,
                1.0,
                1.0,
            ],
            type='DetDataPreprocessor'),
        neck=dict(
            in_channels=[
                256,
                512,
                1024,
                2048,
            ],
            num_outs=5,
            out_channels=256,
            type='FPN'),
        roi_head=dict(
            bbox_head=dict(
                bbox_coder=dict(
                    target_means=[
                        0.0,
                        0.0,
                        0.0,
                        0.0,
                    ],
                    target_stds=[
                        0.1,
                        0.1,
                        0.2,
                        0.2,
                    ],
                    type='DeltaXYWHBBoxCoder'),
                fc_out_channels=1024,
                in_channels=256,
                loss_bbox=dict(loss_weight=1.0, type='L1Loss'),
                loss_cls=dict(
                    alpha=0.5,
                    gamma=2.2,
                    loss_weight=1.5,
                    type='FocalLoss',
                    use_sigmoid=True),
                num_classes=5,
                reg_class_agnostic=False,
                roi_feat_size=7,
                type='Shared2FCBBoxHead'),
            bbox_roi_extractor=dict(
                featmap_strides=[
                    4,
                    8,
                    16,
                    32,
                ],
                out_channels=256,
                roi_layer=dict(
                    output_size=7, sampling_ratio=0, type='RoIAlign'),
                type='SingleRoIExtractor'),
            type='StandardRoIHead'),
        rpn_head=dict(
            anchor_generator=dict(
                ratios=[
                    0.3,
                    0.45,
                    0.6,
                    1.0,
                    2.0,
                ],
                scales=[
                    8,
                    16,
                    32,
                ],
                strides=[
                    4,
                    8,
                    16,
                    32,
                    64,
                ],
                type='AnchorGenerator'),
            bbox_coder=dict(
                target_means=[
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                ],
                target_stds=[
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                ],
                type='DeltaXYWHBBoxCoder'),
            feat_channels=256,
            in_channels=256,
            loss_bbox=dict(loss_weight=1.0, type='L1Loss'),
            loss_cls=dict(
                alpha=0.5,
                gamma=2.0,
                loss_weight=2.0,
                type='FocalLoss',
                use_sigmoid=True),
            type='RPNHead'),
        test_cfg=dict(
            rcnn=dict(
                max_per_img=20,
                nms=dict(iou_threshold=0.5, type='nms'),
                score_thr=0.5),
            rpn=dict(
                max_per_img=1000,
                min_bbox_size=8,
                nms=dict(iou_threshold=0.7, type='nms'),
                nms_pre=2000)),
        train_cfg=dict(
            rcnn=dict(
                assigner=dict(
                    ignore_iof_thr=-1,
                    match_low_quality=False,
                    min_pos_iou=0.5,
                    neg_iou_thr=0.5,
                    pos_iou_thr=0.5,
                    type='MaxIoUAssigner'),
                debug=False,
                pos_weight=-1,
                sampler=dict(
                    add_gt_as_proposals=True,
                    neg_pos_ub=2,
                    num=256,
                    pos_fraction=0.6,
                    type='RandomSampler')),
            rpn=dict(
                allowed_border=-1,
                assigner=dict(
                    ignore_iof_thr=-1,
                    match_low_quality=True,
                    min_pos_iou=0.3,
                    neg_iou_thr=0.3,
                    pos_iou_thr=0.5,
                    type='MaxIoUAssigner'),
                debug=False,
                pos_weight=-1,
                sampler=dict(
                    add_gt_as_proposals=False,
                    neg_pos_ub=2,
                    num=256,
                    pos_fraction=0.6,
                    type='RandomSampler')),
            rpn_proposal=dict(
                max_per_img=1000,
                min_bbox_size=0,
                nms=dict(iou_threshold=0.7, type='nms'),
                nms_pre=2000)),
        type='FasterRCNN'),
    semi_test_cfg=dict(predict_on='teacher'),
    semi_train_cfg=dict(
        cls_pseudo_thr=0.35,
        enable_consensus=True,
        freeze_teacher=True,
        jitter_scale=0.03,
        jitter_times=5,
        min_pseudo_bbox_wh=(
            8,
            8,
        ),
        pseudo_label_initial_score_thr=0.4,
        reg_pseudo_thr=0.02,
        rpn_pseudo_thr=0.3,
        sup_weight=1.0,
        unsup_weight=0.5),
    type='MultiViewSoftTeacher',
    views_per_sample=8)
num_workers = 4
optim_wrapper = dict(
    optimizer=dict(lr=0.0005, momentum=0.9, type='SGD', weight_decay=0.0001),
    type='OptimWrapper')
param_scheduler = [
    dict(
        begin=0, by_epoch=False, end=10000, start_factor=0.01,
        type='LinearLR'),
    dict(
        begin=10000, by_epoch=False, end=70000, factor=1.0, type='ConstantLR'),
    dict(
        begin=70000,
        by_epoch=False,
        end=100000,
        eta_min=5e-05,
        type='CosineAnnealingLR'),
]
resume = False
strong_pipeline = [
    dict(keep_ratio=True, scale=(
        256,
        720,
    ), type='Resize'),
    dict(prob=0.5, type='RandomFlip'),
    dict(
        transforms=[
            dict(
                aug_num=2,
                aug_space=[
                    [
                        dict(prob=0.5, type='AutoContrast'),
                    ],
                    [
                        dict(prob=0.4, type='Equalize'),
                    ],
                    [
                        dict(
                            level=7,
                            max_mag=1.25,
                            min_mag=0.8,
                            prob=0.6,
                            type='Brightness'),
                    ],
                    [
                        dict(
                            level=7,
                            max_mag=1.3,
                            min_mag=0.8,
                            prob=0.6,
                            type='Contrast'),
                    ],
                    [
                        dict(
                            level=6,
                            max_mag=1.5,
                            min_mag=0.8,
                            prob=0.4,
                            type='Sharpness'),
                    ],
                    [
                        dict(
                            level=5,
                            max_mag=7.0,
                            min_mag=5.0,
                            prob=0.2,
                            type='Posterize'),
                    ],
                ],
                type='RandAugment'),
            dict(
                aug_num=1,
                aug_space=[
                    [
                        dict(
                            level=7,
                            max_mag=0.05,
                            min_mag=0.0,
                            prob=0.3,
                            type='TranslateX'),
                    ],
                    [
                        dict(
                            level=7,
                            max_mag=0.05,
                            min_mag=0.0,
                            prob=0.3,
                            type='TranslateY'),
                    ],
                ],
                type='RandAugment'),
        ],
        type='RandomOrder'),
    dict(n_patches=(
        1,
        3,
    ), ratio=(
        0,
        0.15,
    ), type='RandomErasing'),
    dict(min_gt_bbox_wh=(
        4,
        4,
    ), type='FilterAnnotations'),
    dict(
        meta_keys=(
            'img_id',
            'img_path',
            'ori_shape',
            'img_shape',
            'scale_factor',
            'flip',
            'flip_direction',
            'homography_matrix',
        ),
        type='PackDetInputs'),
]
sup_pipeline = [
    dict(
        backend_args=None,
        imdecode_backend='cv2',
        to_float32=False,
        type='LoadImageFromFile'),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(keep_ratio=True, scale=(
        256,
        720,
    ), type='Resize'),
    dict(prob=0.5, type='RandomFlip'),
    dict(
        aug_num=1,
        aug_space=[
            [
                dict(prob=0.5, type='AutoContrast'),
            ],
            [
                dict(prob=0.4, type='Equalize'),
            ],
            [
                dict(
                    level=7,
                    max_mag=1.25,
                    min_mag=0.8,
                    prob=0.6,
                    type='Brightness'),
            ],
            [
                dict(
                    level=7,
                    max_mag=1.3,
                    min_mag=0.8,
                    prob=0.6,
                    type='Contrast'),
            ],
            [
                dict(
                    level=6,
                    max_mag=1.5,
                    min_mag=0.8,
                    prob=0.4,
                    type='Sharpness'),
            ],
            [
                dict(
                    level=5,
                    max_mag=7.0,
                    min_mag=5.0,
                    prob=0.2,
                    type='Posterize'),
            ],
        ],
        type='RandAugment'),
    dict(min_gt_bbox_wh=(
        4,
        4,
    ), type='FilterAnnotations'),
    dict(
        branch_field=[
            'sup',
            'unsup_teacher',
            'unsup_student',
        ],
        sup=dict(type='PackDetInputs'),
        type='MultiBranch'),
]
test_cfg = dict(type='TestLoop')
test_dataloader = dict(
    batch_size=1,
    collate_fn=dict(
        type=
        'mmdet.datasets.wrappers.multi_view_from_folder.multi_view_collate_val'
    ),
    dataset=dict(
        ann_file=
        '/home/coder/data/trong/KLTN/Soft_Teacher/data_drill/anno_valid/_annotations_filtered.bright.coco.json',
        backend_args=None,
        data_prefix=dict(img='valid/'),
        data_root='/home/coder/data/trong/KLTN/Soft_Teacher/data_drill/',
        metainfo=dict(
            classes=(
                'Broken',
                'Chipped',
                'Scratched',
                'Severe_Rust',
                'Tip_Wear',
            ),
            palette=[
                (
                    134,
                    34,
                    255,
                ),
                (
                    0,
                    255,
                    206,
                ),
                (
                    255,
                    128,
                    0,
                ),
                (
                    254,
                    0,
                    86,
                ),
                (
                    199,
                    252,
                    0,
                ),
            ]),
        pipeline=[
            dict(
                backend_args=None,
                imdecode_backend='cv2',
                to_float32=False,
                type='LoadImageFromFile'),
            dict(keep_ratio=True, scale=(
                256,
                720,
            ), type='Resize'),
            dict(
                meta_keys=(
                    'img_id',
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                ),
                type='PackDetInputs'),
        ],
        test_mode=True,
        type='MultiViewFromFolder',
        views_per_sample=8),
    drop_last=False,
    num_workers=2,
    persistent_workers=True,
    pin_memory=True,
    sampler=dict(shuffle=False, type='DefaultSampler'))
test_evaluator = dict(
    aggregation='none',
    ann_file=
    '/home/coder/data/trong/KLTN/Soft_Teacher/data_drill/anno_valid/_annotations_filtered.bright.coco.json',
    backend_args=None,
    classwise=True,
    enable_aggregation=False,
    extract_base_name=True,
    format_only=False,
    metric='bbox',
    nms_iou_thr=0.4,
    type='MultiViewCocoMetric',
    views_per_sample=8)
test_pipeline = [
    dict(
        backend_args=None,
        imdecode_backend='cv2',
        to_float32=False,
        type='LoadImageFromFile'),
    dict(keep_ratio=True, scale=(
        256,
        720,
    ), type='Resize'),
    dict(
        meta_keys=(
            'img_id',
            'img_path',
            'ori_shape',
            'img_shape',
            'scale_factor',
        ),
        type='PackDetInputs'),
]
train_cfg = dict(
    max_iters=100000, type='IterBasedTrainLoop', val_interval=10000)
train_dataloader = dict(
    batch_size=2,
    collate_fn=dict(
        type=
        'mmdet.datasets.wrappers.multi_view_from_folder.multi_view_collate_flatten'
    ),
    dataset=dict(
        datasets=[
            dict(
                ann_file=
                '/home/coder/data/trong/KLTN/Soft_Teacher/data_drill/semi_anno_multiview/_annotations.coco.labeled.grouped@40.bright.json',
                backend_args=None,
                data_prefix=dict(img='train/'),
                data_root=
                '/home/coder/data/trong/KLTN/Soft_Teacher/data_drill/',
                filter_cfg=dict(filter_empty_gt=True, min_size=4),
                metainfo=dict(
                    classes=(
                        'Broken',
                        'Chipped',
                        'Scratched',
                        'Severe_Rust',
                        'Tip_Wear',
                    ),
                    palette=[
                        (
                            134,
                            34,
                            255,
                        ),
                        (
                            0,
                            255,
                            206,
                        ),
                        (
                            255,
                            128,
                            0,
                        ),
                        (
                            254,
                            0,
                            86,
                        ),
                        (
                            199,
                            252,
                            0,
                        ),
                    ]),
                pipeline=[
                    dict(
                        backend_args=None,
                        imdecode_backend='cv2',
                        to_float32=False,
                        type='LoadImageFromFile'),
                    dict(type='LoadAnnotations', with_bbox=True),
                    dict(keep_ratio=True, scale=(
                        256,
                        720,
                    ), type='Resize'),
                    dict(prob=0.5, type='RandomFlip'),
                    dict(
                        aug_num=1,
                        aug_space=[
                            [
                                dict(prob=0.5, type='AutoContrast'),
                            ],
                            [
                                dict(prob=0.4, type='Equalize'),
                            ],
                            [
                                dict(
                                    level=7,
                                    max_mag=1.25,
                                    min_mag=0.8,
                                    prob=0.6,
                                    type='Brightness'),
                            ],
                            [
                                dict(
                                    level=7,
                                    max_mag=1.3,
                                    min_mag=0.8,
                                    prob=0.6,
                                    type='Contrast'),
                            ],
                            [
                                dict(
                                    level=6,
                                    max_mag=1.5,
                                    min_mag=0.8,
                                    prob=0.4,
                                    type='Sharpness'),
                            ],
                            [
                                dict(
                                    level=5,
                                    max_mag=7.0,
                                    min_mag=5.0,
                                    prob=0.2,
                                    type='Posterize'),
                            ],
                        ],
                        type='RandAugment'),
                    dict(min_gt_bbox_wh=(
                        4,
                        4,
                    ), type='FilterAnnotations'),
                    dict(
                        branch_field=[
                            'sup',
                            'unsup_teacher',
                            'unsup_student',
                        ],
                        sup=dict(type='PackDetInputs'),
                        type='MultiBranch'),
                ],
                type='MultiViewFromFolder',
                views_per_sample=8),
            dict(
                ann_file=
                '/home/coder/data/trong/KLTN/Soft_Teacher/data_drill/semi_anno_multiview/_annotations.coco.unlabeled.grouped@40.bright.json',
                backend_args=None,
                data_prefix=dict(img='train/'),
                data_root=
                '/home/coder/data/trong/KLTN/Soft_Teacher/data_drill/',
                filter_cfg=dict(filter_empty_gt=False, min_size=4),
                metainfo=dict(
                    classes=(
                        'Broken',
                        'Chipped',
                        'Scratched',
                        'Severe_Rust',
                        'Tip_Wear',
                    ),
                    palette=[
                        (
                            134,
                            34,
                            255,
                        ),
                        (
                            0,
                            255,
                            206,
                        ),
                        (
                            255,
                            128,
                            0,
                        ),
                        (
                            254,
                            0,
                            86,
                        ),
                        (
                            199,
                            252,
                            0,
                        ),
                    ]),
                pipeline=[
                    dict(
                        backend_args=None,
                        imdecode_backend='cv2',
                        to_float32=False,
                        type='LoadImageFromFile'),
                    dict(type='LoadEmptyAnnotations'),
                    dict(
                        branch_field=[
                            'sup',
                            'unsup_teacher',
                            'unsup_student',
                        ],
                        type='MultiBranch',
                        unsup_student=[
                            dict(
                                keep_ratio=True,
                                scale=(
                                    256,
                                    720,
                                ),
                                type='Resize'),
                            dict(prob=0.5, type='RandomFlip'),
                            dict(
                                transforms=[
                                    dict(
                                        aug_num=2,
                                        aug_space=[
                                            [
                                                dict(
                                                    prob=0.5,
                                                    type='AutoContrast'),
                                            ],
                                            [
                                                dict(
                                                    prob=0.4, type='Equalize'),
                                            ],
                                            [
                                                dict(
                                                    level=7,
                                                    max_mag=1.25,
                                                    min_mag=0.8,
                                                    prob=0.6,
                                                    type='Brightness'),
                                            ],
                                            [
                                                dict(
                                                    level=7,
                                                    max_mag=1.3,
                                                    min_mag=0.8,
                                                    prob=0.6,
                                                    type='Contrast'),
                                            ],
                                            [
                                                dict(
                                                    level=6,
                                                    max_mag=1.5,
                                                    min_mag=0.8,
                                                    prob=0.4,
                                                    type='Sharpness'),
                                            ],
                                            [
                                                dict(
                                                    level=5,
                                                    max_mag=7.0,
                                                    min_mag=5.0,
                                                    prob=0.2,
                                                    type='Posterize'),
                                            ],
                                        ],
                                        type='RandAugment'),
                                    dict(
                                        aug_num=1,
                                        aug_space=[
                                            [
                                                dict(
                                                    level=7,
                                                    max_mag=0.05,
                                                    min_mag=0.0,
                                                    prob=0.3,
                                                    type='TranslateX'),
                                            ],
                                            [
                                                dict(
                                                    level=7,
                                                    max_mag=0.05,
                                                    min_mag=0.0,
                                                    prob=0.3,
                                                    type='TranslateY'),
                                            ],
                                        ],
                                        type='RandAugment'),
                                ],
                                type='RandomOrder'),
                            dict(
                                n_patches=(
                                    1,
                                    3,
                                ),
                                ratio=(
                                    0,
                                    0.15,
                                ),
                                type='RandomErasing'),
                            dict(
                                min_gt_bbox_wh=(
                                    4,
                                    4,
                                ),
                                type='FilterAnnotations'),
                            dict(
                                meta_keys=(
                                    'img_id',
                                    'img_path',
                                    'ori_shape',
                                    'img_shape',
                                    'scale_factor',
                                    'flip',
                                    'flip_direction',
                                    'homography_matrix',
                                ),
                                type='PackDetInputs'),
                        ],
                        unsup_teacher=[
                            dict(
                                keep_ratio=True,
                                scale=(
                                    256,
                                    720,
                                ),
                                type='Resize'),
                            dict(prob=0.5, type='RandomFlip'),
                            dict(
                                meta_keys=(
                                    'img_id',
                                    'img_path',
                                    'ori_shape',
                                    'img_shape',
                                    'scale_factor',
                                    'flip',
                                    'flip_direction',
                                    'homography_matrix',
                                ),
                                type='PackDetInputs'),
                        ]),
                ],
                type='MultiViewFromFolder',
                views_per_sample=8),
        ],
        type='ConcatDataset'),
    num_workers=4,
    persistent_workers=True,
    pin_memory=True,
    prefetch_factor=2,
    sampler=dict(
        batch_size=2, source_ratio=[
            1,
            3,
        ], type='GroupMultiSourceSampler'))
unlabeled_dataset = dict(
    ann_file=
    '/home/coder/data/trong/KLTN/Soft_Teacher/data_drill/semi_anno_multiview/_annotations.coco.unlabeled.grouped@40.bright.json',
    backend_args=None,
    data_prefix=dict(img='train/'),
    data_root='/home/coder/data/trong/KLTN/Soft_Teacher/data_drill/',
    filter_cfg=dict(filter_empty_gt=False, min_size=4),
    metainfo=dict(
        classes=(
            'Broken',
            'Chipped',
            'Scratched',
            'Severe_Rust',
            'Tip_Wear',
        ),
        palette=[
            (
                134,
                34,
                255,
            ),
            (
                0,
                255,
                206,
            ),
            (
                255,
                128,
                0,
            ),
            (
                254,
                0,
                86,
            ),
            (
                199,
                252,
                0,
            ),
        ]),
    pipeline=[
        dict(
            backend_args=None,
            imdecode_backend='cv2',
            to_float32=False,
            type='LoadImageFromFile'),
        dict(type='LoadEmptyAnnotations'),
        dict(
            branch_field=[
                'sup',
                'unsup_teacher',
                'unsup_student',
            ],
            type='MultiBranch',
            unsup_student=[
                dict(keep_ratio=True, scale=(
                    256,
                    720,
                ), type='Resize'),
                dict(prob=0.5, type='RandomFlip'),
                dict(
                    transforms=[
                        dict(
                            aug_num=2,
                            aug_space=[
                                [
                                    dict(prob=0.5, type='AutoContrast'),
                                ],
                                [
                                    dict(prob=0.4, type='Equalize'),
                                ],
                                [
                                    dict(
                                        level=7,
                                        max_mag=1.25,
                                        min_mag=0.8,
                                        prob=0.6,
                                        type='Brightness'),
                                ],
                                [
                                    dict(
                                        level=7,
                                        max_mag=1.3,
                                        min_mag=0.8,
                                        prob=0.6,
                                        type='Contrast'),
                                ],
                                [
                                    dict(
                                        level=6,
                                        max_mag=1.5,
                                        min_mag=0.8,
                                        prob=0.4,
                                        type='Sharpness'),
                                ],
                                [
                                    dict(
                                        level=5,
                                        max_mag=7.0,
                                        min_mag=5.0,
                                        prob=0.2,
                                        type='Posterize'),
                                ],
                            ],
                            type='RandAugment'),
                        dict(
                            aug_num=1,
                            aug_space=[
                                [
                                    dict(
                                        level=7,
                                        max_mag=0.05,
                                        min_mag=0.0,
                                        prob=0.3,
                                        type='TranslateX'),
                                ],
                                [
                                    dict(
                                        level=7,
                                        max_mag=0.05,
                                        min_mag=0.0,
                                        prob=0.3,
                                        type='TranslateY'),
                                ],
                            ],
                            type='RandAugment'),
                    ],
                    type='RandomOrder'),
                dict(
                    n_patches=(
                        1,
                        3,
                    ),
                    ratio=(
                        0,
                        0.15,
                    ),
                    type='RandomErasing'),
                dict(min_gt_bbox_wh=(
                    4,
                    4,
                ), type='FilterAnnotations'),
                dict(
                    meta_keys=(
                        'img_id',
                        'img_path',
                        'ori_shape',
                        'img_shape',
                        'scale_factor',
                        'flip',
                        'flip_direction',
                        'homography_matrix',
                    ),
                    type='PackDetInputs'),
            ],
            unsup_teacher=[
                dict(keep_ratio=True, scale=(
                    256,
                    720,
                ), type='Resize'),
                dict(prob=0.5, type='RandomFlip'),
                dict(
                    meta_keys=(
                        'img_id',
                        'img_path',
                        'ori_shape',
                        'img_shape',
                        'scale_factor',
                        'flip',
                        'flip_direction',
                        'homography_matrix',
                    ),
                    type='PackDetInputs'),
            ]),
    ],
    type='MultiViewFromFolder',
    views_per_sample=8)
unsup_pipeline = [
    dict(
        backend_args=None,
        imdecode_backend='cv2',
        to_float32=False,
        type='LoadImageFromFile'),
    dict(type='LoadEmptyAnnotations'),
    dict(
        branch_field=[
            'sup',
            'unsup_teacher',
            'unsup_student',
        ],
        type='MultiBranch',
        unsup_student=[
            dict(keep_ratio=True, scale=(
                256,
                720,
            ), type='Resize'),
            dict(prob=0.5, type='RandomFlip'),
            dict(
                transforms=[
                    dict(
                        aug_num=2,
                        aug_space=[
                            [
                                dict(prob=0.5, type='AutoContrast'),
                            ],
                            [
                                dict(prob=0.4, type='Equalize'),
                            ],
                            [
                                dict(
                                    level=7,
                                    max_mag=1.25,
                                    min_mag=0.8,
                                    prob=0.6,
                                    type='Brightness'),
                            ],
                            [
                                dict(
                                    level=7,
                                    max_mag=1.3,
                                    min_mag=0.8,
                                    prob=0.6,
                                    type='Contrast'),
                            ],
                            [
                                dict(
                                    level=6,
                                    max_mag=1.5,
                                    min_mag=0.8,
                                    prob=0.4,
                                    type='Sharpness'),
                            ],
                            [
                                dict(
                                    level=5,
                                    max_mag=7.0,
                                    min_mag=5.0,
                                    prob=0.2,
                                    type='Posterize'),
                            ],
                        ],
                        type='RandAugment'),
                    dict(
                        aug_num=1,
                        aug_space=[
                            [
                                dict(
                                    level=7,
                                    max_mag=0.05,
                                    min_mag=0.0,
                                    prob=0.3,
                                    type='TranslateX'),
                            ],
                            [
                                dict(
                                    level=7,
                                    max_mag=0.05,
                                    min_mag=0.0,
                                    prob=0.3,
                                    type='TranslateY'),
                            ],
                        ],
                        type='RandAugment'),
                ],
                type='RandomOrder'),
            dict(n_patches=(
                1,
                3,
            ), ratio=(
                0,
                0.15,
            ), type='RandomErasing'),
            dict(min_gt_bbox_wh=(
                4,
                4,
            ), type='FilterAnnotations'),
            dict(
                meta_keys=(
                    'img_id',
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                    'flip',
                    'flip_direction',
                    'homography_matrix',
                ),
                type='PackDetInputs'),
        ],
        unsup_teacher=[
            dict(keep_ratio=True, scale=(
                256,
                720,
            ), type='Resize'),
            dict(prob=0.5, type='RandomFlip'),
            dict(
                meta_keys=(
                    'img_id',
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                    'flip',
                    'flip_direction',
                    'homography_matrix',
                ),
                type='PackDetInputs'),
        ]),
]
val_cfg = dict(type='TeacherStudentValLoop')
val_dataloader = dict(
    batch_size=1,
    collate_fn=dict(
        type=
        'mmdet.datasets.wrappers.multi_view_from_folder.multi_view_collate_val'
    ),
    dataset=dict(
        ann_file=
        '/home/coder/data/trong/KLTN/Soft_Teacher/data_drill/anno_valid/_annotations_filtered.bright.coco.json',
        backend_args=None,
        data_prefix=dict(img='valid/'),
        data_root='/home/coder/data/trong/KLTN/Soft_Teacher/data_drill/',
        metainfo=dict(
            classes=(
                'Broken',
                'Chipped',
                'Scratched',
                'Severe_Rust',
                'Tip_Wear',
            ),
            palette=[
                (
                    134,
                    34,
                    255,
                ),
                (
                    0,
                    255,
                    206,
                ),
                (
                    255,
                    128,
                    0,
                ),
                (
                    254,
                    0,
                    86,
                ),
                (
                    199,
                    252,
                    0,
                ),
            ]),
        pipeline=[
            dict(
                backend_args=None,
                imdecode_backend='cv2',
                to_float32=False,
                type='LoadImageFromFile'),
            dict(keep_ratio=True, scale=(
                256,
                720,
            ), type='Resize'),
            dict(
                meta_keys=(
                    'img_id',
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                ),
                type='PackDetInputs'),
        ],
        test_mode=True,
        type='MultiViewFromFolder',
        views_per_sample=8),
    drop_last=False,
    num_workers=2,
    persistent_workers=True,
    pin_memory=True,
    sampler=dict(shuffle=False, type='DefaultSampler'))
val_evaluator = dict(
    aggregation='none',
    ann_file=
    '/home/coder/data/trong/KLTN/Soft_Teacher/data_drill/anno_valid/_annotations_filtered.bright.coco.json',
    backend_args=None,
    classwise=True,
    enable_aggregation=False,
    extract_base_name=True,
    format_only=False,
    metric='bbox',
    nms_iou_thr=0.4,
    type='MultiViewCocoMetric',
    views_per_sample=8)
views_per_sample = 8
vis_backends = [
    dict(type='LocalVisBackend'),
]
visualizer = dict(
    name='visualizer',
    type='DetLocalVisualizer',
    vis_backends=[
        dict(type='LocalVisBackend'),
    ])
weak_pipeline = [
    dict(keep_ratio=True, scale=(
        256,
        720,
    ), type='Resize'),
    dict(prob=0.5, type='RandomFlip'),
    dict(
        meta_keys=(
            'img_id',
            'img_path',
            'ori_shape',
            'img_shape',
            'scale_factor',
            'flip',
            'flip_direction',
            'homography_matrix',
        ),
        type='PackDetInputs'),
]
work_dir = './work_dirs/soft_teacher_custom_multi_view'

2025/12/08 14:21:23 - mmengine - INFO - MultiViewBackbone: Initialized with MVViT fusion, views_per_sample=8
2025/12/08 14:21:24 - mmengine - INFO - MultiViewBackbone: Initialized with MVViT fusion, views_per_sample=8
2025/12/08 14:21:26 - mmengine - INFO - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.
2025/12/08 14:21:26 - mmengine - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) RuntimeInfoHook                    
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
before_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) MeanTeacherHook                    
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DistSamplerSeedHook                
 -------------------- 
before_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) MeanTeacherHook                    
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) IterTimerHook                      
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_val_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_val_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_val_iter:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DetVisualizationHook               
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_val_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_test_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_test_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_test_iter:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DetVisualizationHook               
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_run:
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
2025/12/08 14:21:28 - mmengine - INFO - [COCO ANNO] Loaded from: /home/coder/data/trong/KLTN/Soft_Teacher/data_drill/semi_anno_multiview/_annotations.coco.labeled.grouped@40.bright.json
2025/12/08 14:21:28 - mmengine - INFO -    1208 images, 1448 annotations
2025/12/08 14:21:28 - mmengine - INFO -    946 files have annotations
2025/12/08 14:21:28 - mmengine - INFO -    1208 files have base_img_id
2025/12/08 14:21:28 - mmengine - INFO -    Categories: [0, 1, 2, 3, 4, 5]
2025/12/08 14:21:28 - mmengine - INFO -    cat2label mapping: {1: 0, 2: 1, 3: 2, 4: 3, 5: 4}
2025/12/08 14:21:28 - mmengine - INFO -    File 'S136_Image__2025-09-23__16-42-55_bright_4_crop_8_jpg.rf.8d03a89b2a2643c02b4733eb858e68be.jpg': 1 boxes, base_img_id=S136_bright_4, crop_num=8
2025/12/08 14:21:28 - mmengine - INFO -    File 'S40_Image__2025-09-04__16-02-50_bright_2_crop_6_jpg.rf.0006368956fc4380b51ac33ff3fbee7c.jpg': 1 boxes, base_img_id=S40_bright_2, crop_num=6
2025/12/08 14:21:28 - mmengine - INFO -    File 'S124_Image__2025-09-23__10-26-03_bright_3_crop_5_jpg.rf.eff0c11d31452dcd01ccb1959ba342f5.jpg': 2 boxes, base_img_id=S124_bright_3, crop_num=5
2025/12/08 14:21:28 - mmengine - INFO - [COCO] Built 151 groups from base_img_id
2025/12/08 14:21:28 - mmengine - INFO - [COCO] Total files in COCO JSON: 1208
2025/12/08 14:21:28 - mmengine - INFO - [COCO] Unique base_img_ids: 151
2025/12/08 14:21:28 - mmengine - INFO -    Group 0: base_img_id=S101_bright_2, 8/8 views
2025/12/08 14:21:28 - mmengine - INFO -    Group 1: base_img_id=S111_bright_3, 8/8 views
2025/12/08 14:21:28 - mmengine - INFO -    Group 2: base_img_id=S111_bright_4, 8/8 views
2025/12/08 14:21:28 - mmengine - INFO - [COCO ANNO] Loaded from: /home/coder/data/trong/KLTN/Soft_Teacher/data_drill/semi_anno_multiview/_annotations.coco.unlabeled.grouped@40.bright.json
2025/12/08 14:21:28 - mmengine - INFO -    784 images, 956 annotations
2025/12/08 14:21:28 - mmengine - INFO -    649 files have annotations
2025/12/08 14:21:28 - mmengine - INFO -    784 files have base_img_id
2025/12/08 14:21:28 - mmengine - INFO -    Categories: [0, 1, 2, 3, 4, 5]
2025/12/08 14:21:28 - mmengine - INFO -    cat2label mapping: {1: 0, 2: 1, 3: 2, 4: 3, 5: 4}
2025/12/08 14:21:28 - mmengine - INFO -    File 'S4_than_Image__2025-09-03__14-00-16_bright_2_crop_3_jpg.rf.93c6a9e3d563bf395cd5c9ef7519b742.jpg': 3 boxes, base_img_id=S4_bright_2, crop_num=3
2025/12/08 14:21:28 - mmengine - INFO -    File 'S225_Image__2025-07-01__14-30-51_bright_2_crop_7_jpg.rf.6b508c0dd7ed86aff7e31273da30c5e4.jpg': 2 boxes, base_img_id=S225_bright_2, crop_num=7
2025/12/08 14:21:28 - mmengine - INFO -    File 'S78_Image__2025-09-08__16-35-36_bright_3_crop_8_jpg.rf.84d34f89fe2af65f5adacb8437ed2ee6.jpg': 3 boxes, base_img_id=S78_bright_3, crop_num=8
2025/12/08 14:21:28 - mmengine - INFO - [COCO] Built 98 groups from base_img_id
2025/12/08 14:21:28 - mmengine - INFO - [COCO] Total files in COCO JSON: 784
2025/12/08 14:21:28 - mmengine - INFO - [COCO] Unique base_img_ids: 98
2025/12/08 14:21:28 - mmengine - INFO -    Group 0: base_img_id=S111_bright_2, 8/8 views
2025/12/08 14:21:28 - mmengine - INFO -    Group 1: base_img_id=S114_bright_2, 8/8 views
2025/12/08 14:21:28 - mmengine - INFO -    Group 2: base_img_id=S117_bright_2, 8/8 views
2025/12/08 14:21:29 - mmengine - INFO - [COCO ANNO] Loaded from: /home/coder/data/trong/KLTN/Soft_Teacher/data_drill/anno_valid/_annotations_filtered.bright.coco.json
2025/12/08 14:21:29 - mmengine - INFO -    320 images, 442 annotations
2025/12/08 14:21:29 - mmengine - INFO -    284 files have annotations
2025/12/08 14:21:29 - mmengine - INFO -    320 files have base_img_id
2025/12/08 14:21:29 - mmengine - INFO -    Categories: [0, 1, 2, 3, 4, 5]
2025/12/08 14:21:29 - mmengine - INFO -    cat2label mapping: {1: 0, 2: 1, 3: 2, 4: 3, 5: 4}
2025/12/08 14:21:29 - mmengine - INFO -    File 'S245_Image__2025-11-11__12-09-08_bright_2_crop_5_jpg.rf.b0924b8796050c8c09b6c63180a8df2a.jpg': 1 boxes, base_img_id=S245_bright_2, crop_num=5
2025/12/08 14:21:29 - mmengine - INFO -    File 'S135_Image__2025-09-23__16-23-02_bright_4_crop_7_jpg.rf.1b8c818ab9826dbcd27a64c2e38414e1.jpg': 2 boxes, base_img_id=S135_bright_4, crop_num=7
2025/12/08 14:21:29 - mmengine - INFO -    File 'S242_Image__2025-11-11__11-56-30_bright_8_crop_1_jpg.rf.06e5607648d1e41e35c7bec4fc2eeb57.jpg': 1 boxes, base_img_id=S242_bright_8, crop_num=1
2025/12/08 14:21:29 - mmengine - INFO - [COCO] Built 40 groups from base_img_id
2025/12/08 14:21:29 - mmengine - INFO - [COCO] Total files in COCO JSON: 320
2025/12/08 14:21:29 - mmengine - INFO - [COCO] Unique base_img_ids: 40
2025/12/08 14:21:29 - mmengine - INFO -    Group 0: base_img_id=S110_bright_2, 8/8 views
2025/12/08 14:21:29 - mmengine - INFO -    Group 1: base_img_id=S110_bright_3, 8/8 views
2025/12/08 14:21:29 - mmengine - INFO -    Group 2: base_img_id=S110_bright_4, 8/8 views
2025/12/08 14:21:29 - mmengine - INFO - [BBOX DEBUG 1] File: S185_Image__2025-07-15__15-20-59_bright_2_crop_1_jpg.rf.5cb583ed03219c63cb71ef9f4d015924.jpg, Image shape: 720x256
2025/12/08 14:21:29 - mmengine - INFO -    Loaded 1 boxes from COCO JSON
2025/12/08 14:21:29 - mmengine - INFO -     Box 0: [0, 202, 243.093, 360.86400000000003], label: 0
2025/12/08 14:21:29 - mmengine - INFO - MultiViewCocoMetric initialized: PER-CROP evaluation mode (no aggregation, 8 crops evaluated independently)
2025/12/08 14:21:29 - mmengine - INFO - [BBOX DEBUG 1] File: S213_Image__2025-06-05__10-33-38_bright_3_crop_1_jpg.rf.2597dcda47db622003b1e9cad57d6599.jpg, Image shape: 720x256
2025/12/08 14:21:29 - mmengine - INFO -    Loaded 1 boxes from COCO JSON
2025/12/08 14:21:29 - mmengine - INFO -     Box 0: [160, 295, 215.895, 399.693], label: 1
2025/12/08 14:21:29 - mmengine - INFO - [BBOX DEBUG 1] File: S21_Image__2025-09-04__09-32-36_bright_3_crop_1_jpg.rf.f0f51f12a158565115637d0ed6f00f0b.jpg, Image shape: 720x256
2025/12/08 14:21:29 - mmengine - INFO -    Loaded 2 boxes from COCO JSON
2025/12/08 14:21:29 - mmengine - INFO -     Box 0: [36, 222, 138.32, 478.35], label: 2
2025/12/08 14:21:29 - mmengine - INFO -     Box 1: [82, 292, 116.61, 356.33], label: 1
2025/12/08 14:21:29 - mmengine - INFO - [BBOX DEBUG 2] File: S213_Image__2025-06-05__10-33-38_bright_3_crop_2_jpg.rf.549d0c706a7f19890e796e7c2b573aa9.jpg, Image shape: 720x256
2025/12/08 14:21:29 - mmengine - INFO -    Loaded 1 boxes from COCO JSON
2025/12/08 14:21:29 - mmengine - INFO -     Box 0: [78, 243, 154.83499999999998, 381.227], label: 1
2025/12/08 14:21:29 - mmengine - INFO - [BBOX DEBUG 2] File: S21_Image__2025-09-04__09-32-36_bright_3_crop_2_jpg.rf.b0ba9bbd664ab1349adfc03725978cfa.jpg, Image shape: 720x256
2025/12/08 14:21:29 - mmengine - INFO -    Loaded 2 boxes from COCO JSON
2025/12/08 14:21:29 - mmengine - INFO -     Box 0: [35, 267, 65.243, 354.606], label: 1
2025/12/08 14:21:29 - mmengine - INFO -     Box 1: [180, 0, 245.82999999999998, 127.504], label: 2
2025/12/08 14:21:29 - mmengine - INFO - [BBOX DEBUG 3] File: S21_Image__2025-09-04__09-32-36_bright_3_crop_3_jpg.rf.efeddaa0c8a82bce28fe29646702de27.jpg, Image shape: 720x256
2025/12/08 14:21:29 - mmengine - INFO -    Loaded 3 boxes from COCO JSON
2025/12/08 14:21:29 - mmengine - INFO -     Box 0: [40, 290, 72.03999999999999, 359.93], label: 1
2025/12/08 14:21:29 - mmengine - INFO -     Box 1: [176, 167, 249.67000000000002, 515.96], label: 2
2025/12/08 14:21:29 - mmengine - INFO -     Box 2: [104, 0, 191.35, 155.88], label: 2
2025/12/08 14:21:29 - mmengine - INFO - [BBOX DEBUG 2] File: S185_Image__2025-07-15__15-20-59_bright_2_crop_2_jpg.rf.9a94053ade2637de03f7042f7eed1abe.jpg, Image shape: 720x256
2025/12/08 14:21:29 - mmengine - INFO -    Loaded 1 boxes from COCO JSON
2025/12/08 14:21:29 - mmengine - INFO -     Box 0: [29, 162, 254.865, 358.773], label: 0
2025/12/08 14:21:29 - mmengine - INFO - [BBOX DEBUG 3] File: S185_Image__2025-07-15__15-20-59_bright_2_crop_3_jpg.rf.26d0ae2aa1f484215f11b4e08da18e0f.jpg, Image shape: 720x256
2025/12/08 14:21:29 - mmengine - INFO -    Loaded 1 boxes from COCO JSON
2025/12/08 14:21:29 - mmengine - INFO -     Box 0: [7, 193, 235.549, 317.115], label: 0
2025/12/08 14:21:29 - mmengine - INFO - [BBOX DEBUG 1] File: S71_Image__2025-09-22__11-02-44_bright_5_crop_2_jpg.rf.365c357e818750ac6de13d29bf0e767c.jpg, Image shape: 720x256
2025/12/08 14:21:29 - mmengine - INFO -    Loaded 1 boxes from COCO JSON
2025/12/08 14:21:29 - mmengine - INFO -     Box 0: [143, 347, 217.909, 576.573], label: 2
2025/12/08 14:21:29 - mmengine - INFO - [BBOX DEBUG 2] File: S71_Image__2025-09-22__11-02-44_bright_5_crop_3_jpg.rf.5860fd1b32d77388fba15f24a6e2c09e.jpg, Image shape: 720x256
2025/12/08 14:21:29 - mmengine - INFO -    Loaded 2 boxes from COCO JSON
2025/12/08 14:21:29 - mmengine - INFO -     Box 0: [94, 335, 206.576, 682.5070000000001], label: 2
2025/12/08 14:21:29 - mmengine - INFO -     Box 1: [155, 120, 232.47899999999998, 328.794], label: 2
2025/12/08 14:21:29 - mmengine - INFO - [BBOX DEBUG 3] File: S213_Image__2025-06-05__10-33-38_bright_3_crop_3_jpg.rf.32f1d6b5f3437a470188313b23416451.jpg, Image shape: 720x256
2025/12/08 14:21:29 - mmengine - INFO -    Loaded 2 boxes from COCO JSON
2025/12/08 14:21:29 - mmengine - INFO -     Box 0: [32, 263, 86.35499999999999, 394.14], label: 1
2025/12/08 14:21:29 - mmengine - INFO -     Box 1: [90, 491, 131.7, 695.823], label: 2
2025/12/08 14:21:29 - mmengine - INFO - [BBOX DEBUG 3] File: S71_Image__2025-09-22__11-02-44_bright_5_crop_4_jpg.rf.3d57cd6c942cbf960807066aa64319ca.jpg, Image shape: 720x256
2025/12/08 14:21:29 - mmengine - INFO -    Loaded 1 boxes from COCO JSON
2025/12/08 14:21:29 - mmengine - INFO -     Box 0: [55, 354, 115.767, 649.914], label: 2
2025/12/08 14:21:29 - mmengine - INFO - [BBOX DEBUG 1] File: S4_than_Image__2025-09-03__14-00-16_bright_2_crop_1_jpg.rf.49284f02b82f2b3d1ea483148bcba547.jpg, Image shape: 720x256
2025/12/08 14:21:29 - mmengine - INFO -    Loaded 2 boxes from COCO JSON
2025/12/08 14:21:29 - mmengine - INFO -     Box 0: [35, 37, 225.939, 196.592], label: 0
2025/12/08 14:21:29 - mmengine - INFO -     Box 1: [128, 191, 187.442, 313.994], label: 1
2025/12/08 14:21:29 - mmengine - INFO - [BBOX DEBUG 2] File: S4_than_Image__2025-09-03__14-00-16_bright_2_crop_2_jpg.rf.08069e169864b4d6f535bc1e5a0dac9d.jpg, Image shape: 720x256
2025/12/08 14:21:29 - mmengine - INFO -    Loaded 2 boxes from COCO JSON
2025/12/08 14:21:29 - mmengine - INFO -     Box 0: [25, 0, 255.332, 175.216], label: 0
2025/12/08 14:21:29 - mmengine - INFO - [BBOX DEBUG 1] File: S241_Image__2025-11-11__11-31-46_bright_3_crop_1_jpg.rf.fda7caa8ae78f09efec5f6bb133f219a.jpg, Image shape: 720x256
2025/12/08 14:21:29 - mmengine - INFO -    Loaded 3 boxes from COCO JSON
2025/12/08 14:21:29 - mmengine - INFO -     Box 0: [44, 40, 233.91, 470.56], label: 3
2025/12/08 14:21:29 - mmengine - INFO -     Box 1: [157, 535, 247.29000000000002, 707.8199999999999], label: 3
2025/12/08 14:21:29 - mmengine - INFO -     Box 2: [36, 579, 87.14, 701.8199999999999], label: 3
2025/12/08 14:21:29 - mmengine - INFO -     Box 1: [27, 157, 111.161, 292.603], label: 1
2025/12/08 14:21:29 - mmengine - INFO - [BBOX DEBUG 3] File: S4_than_Image__2025-09-03__14-00-16_bright_2_crop_3_jpg.rf.93c6a9e3d563bf395cd5c9ef7519b742.jpg, Image shape: 720x256
2025/12/08 14:21:29 - mmengine - INFO -    Loaded 3 boxes from COCO JSON
2025/12/08 14:21:29 - mmengine - INFO -     Box 0: [20, 3, 236.088, 98.86], label: 0
2025/12/08 14:21:29 - mmengine - INFO -     Box 1: [98, 204, 144.454, 441.219], label: 2
2025/12/08 14:21:29 - mmengine - INFO -     Box 2: [8, 179, 49.959, 309.058], label: 1
2025/12/08 14:21:29 - mmengine - INFO - [BBOX DEBUG 1] File: S161_Image__2025-09-23__09-54-50_bright_2_crop_1_jpg.rf.aebdad729797e8a49336f2557b5bfac3.jpg, Image shape: 720x256
2025/12/08 14:21:29 - mmengine - INFO -    Loaded 1 boxes from COCO JSON
2025/12/08 14:21:29 - mmengine - INFO -     Box 0: [49, 274, 254.571, 449.058], label: 0
2025/12/08 14:21:29 - mmengine - INFO - [BBOX DEBUG 2] File: S241_Image__2025-11-11__11-31-46_bright_3_crop_2_jpg.rf.3145dee5ec40cd945b07ddd058afe8fe.jpg, Image shape: 720x256
2025/12/08 14:21:29 - mmengine - INFO -    Loaded 2 boxes from COCO JSON
2025/12/08 14:21:29 - mmengine - INFO -     Box 0: [44, 122, 255.36, 464.72], label: 3
2025/12/08 14:21:29 - mmengine - INFO -     Box 1: [49, 506, 244.4, 655.27], label: 3
2025/12/08 14:21:29 - mmengine - INFO - [BBOX DEBUG 2] File: S161_Image__2025-09-23__09-54-50_bright_2_crop_2_jpg.rf.4ef014623afe6c6cf0f65a26023da8de.jpg, Image shape: 720x256
2025/12/08 14:21:29 - mmengine - INFO -    Loaded 1 boxes from COCO JSON
2025/12/08 14:21:29 - mmengine - INFO -     Box 0: [40, 214, 254.911, 408.576], label: 0
2025/12/08 14:21:29 - mmengine - INFO - [BBOX DEBUG 3] File: S241_Image__2025-11-11__11-31-46_bright_3_crop_3_jpg.rf.7a9cb2613803ec903554d0e73ea94a44.jpg, Image shape: 720x256
2025/12/08 14:21:29 - mmengine - INFO -    Loaded 2 boxes from COCO JSON
2025/12/08 14:21:29 - mmengine - INFO -     Box 0: [30, 296, 183.86, 675.46], label: 3
2025/12/08 14:21:29 - mmengine - INFO -     Box 1: [165, 119, 244.32, 316.63], label: 3
2025/12/08 14:21:29 - mmengine - INFO - [BBOX DEBUG 3] File: S161_Image__2025-09-23__09-54-50_bright_2_crop_3_jpg.rf.ea8e176ed5f7076dc273bd151b2a3cec.jpg, Image shape: 720x256
2025/12/08 14:21:29 - mmengine - INFO -    Loaded 1 boxes from COCO JSON
2025/12/08 14:21:29 - mmengine - INFO -     Box 0: [7, 202, 241.144, 386.103], label: 0
2025/12/08 14:21:29 - mmengine - INFO - [BBOX DEBUG 1] File: S143_Image__2025-09-24__11-30-30_bright_2_crop_3_jpg.rf.900697b752344e7c774e2aab8217ab3d.jpg, Image shape: 720x256
2025/12/08 14:21:29 - mmengine - INFO -    Loaded 1 boxes from COCO JSON
2025/12/08 14:21:29 - mmengine - INFO -     Box 0: [90, 20, 177.345, 124.897], label: 4
2025/12/08 14:21:29 - mmengine - INFO - [BBOX DEBUG 2] File: S143_Image__2025-09-24__11-30-30_bright_2_crop_4_jpg.rf.ec7869d72b663cb6a5cce559e471b504.jpg, Image shape: 720x256
2025/12/08 14:21:29 - mmengine - INFO -    Loaded 1 boxes from COCO JSON
2025/12/08 14:21:29 - mmengine - INFO -     Box 0: [43, 25, 167.35, 118.34], label: 4
2025/12/08 14:21:29 - mmengine - INFO - [BBOX DEBUG 3] File: S143_Image__2025-09-24__11-30-30_bright_2_crop_5_jpg.rf.fe322bb0a9be865e6be1d71f47d50278.jpg, Image shape: 720x256
2025/12/08 14:21:29 - mmengine - INFO -    Loaded 1 boxes from COCO JSON
2025/12/08 14:21:29 - mmengine - INFO -     Box 0: [123, 359, 173.828, 525.412], label: 2
Name of parameter - Initialization information

student.backbone.backbone.conv1.weight - torch.Size([64, 3, 7, 7]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.0.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.0.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.0.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.0.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.0.bn3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.0.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.0.downsample.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.0.downsample.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.1.bn3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.1.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.2.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.2.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.2.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.2.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.2.bn3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.2.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.0.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.0.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.0.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.0.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.0.bn3.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.0.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.0.downsample.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.0.downsample.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.1.bn3.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.1.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.2.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.2.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.2.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.2.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.2.bn3.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.2.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.3.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.3.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.3.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.3.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.3.bn3.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.3.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.0.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.0.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.0.downsample.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.0.downsample.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.1.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.1.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.2.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.2.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.2.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.2.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.2.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.2.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.3.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.3.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.3.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.3.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.3.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.3.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.4.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.4.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.4.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.4.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.4.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.4.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.5.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.5.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.5.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.5.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.5.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.5.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.0.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.0.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.0.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.0.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.0.bn3.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.0.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.0.downsample.1.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.0.downsample.1.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.1.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.1.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.1.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.1.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.1.bn3.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.1.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.2.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.2.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.2.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.2.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.2.bn3.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.2.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.residual_alpha - torch.Size([]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.layers.0.self_attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.layers.0.self_attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.layers.0.self_attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.layers.0.self_attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.layers.0.linear1.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.layers.0.linear1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.layers.0.linear2.weight - torch.Size([256, 512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.layers.0.linear2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.layers.0.norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.layers.0.norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.layers.0.norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.layers.0.norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.pooling_queries.1024 - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.pooling_queries.2048 - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.pooling_queries.256 - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.pooling_queries.512 - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.upsample_queries.128x128 - torch.Size([16384, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.upsample_queries.32x32 - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.upsample_queries.64x64 - torch.Size([4096, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.neck.lateral_convs.0.conv.weight - torch.Size([256, 256, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

student.neck.lateral_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.neck.lateral_convs.1.conv.weight - torch.Size([256, 512, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

student.neck.lateral_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.neck.lateral_convs.2.conv.weight - torch.Size([256, 1024, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

student.neck.lateral_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.neck.lateral_convs.3.conv.weight - torch.Size([256, 2048, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

student.neck.lateral_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

student.neck.fpn_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

student.neck.fpn_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

student.neck.fpn_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.neck.fpn_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

student.neck.fpn_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.rpn_head.rpn_conv.weight - torch.Size([256, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=0 

student.rpn_head.rpn_conv.bias - torch.Size([256]): 
NormalInit: mean=0, std=0.01, bias=0 

student.rpn_head.rpn_cls.weight - torch.Size([15, 256, 1, 1]): 
NormalInit: mean=0, std=0.01, bias=0 

student.rpn_head.rpn_cls.bias - torch.Size([15]): 
NormalInit: mean=0, std=0.01, bias=0 

student.rpn_head.rpn_reg.weight - torch.Size([60, 256, 1, 1]): 
NormalInit: mean=0, std=0.01, bias=0 

student.rpn_head.rpn_reg.bias - torch.Size([60]): 
NormalInit: mean=0, std=0.01, bias=0 

student.roi_head.bbox_head.fc_cls.weight - torch.Size([6, 1024]): 
NormalInit: mean=0, std=0.01, bias=0 

student.roi_head.bbox_head.fc_cls.bias - torch.Size([6]): 
NormalInit: mean=0, std=0.01, bias=0 

student.roi_head.bbox_head.fc_reg.weight - torch.Size([20, 1024]): 
NormalInit: mean=0, std=0.001, bias=0 

student.roi_head.bbox_head.fc_reg.bias - torch.Size([20]): 
NormalInit: mean=0, std=0.001, bias=0 

student.roi_head.bbox_head.shared_fcs.0.weight - torch.Size([1024, 12544]): 
XavierInit: gain=1, distribution=uniform, bias=0 

student.roi_head.bbox_head.shared_fcs.0.bias - torch.Size([1024]): 
XavierInit: gain=1, distribution=uniform, bias=0 

student.roi_head.bbox_head.shared_fcs.1.weight - torch.Size([1024, 1024]): 
XavierInit: gain=1, distribution=uniform, bias=0 

student.roi_head.bbox_head.shared_fcs.1.bias - torch.Size([1024]): 
XavierInit: gain=1, distribution=uniform, bias=0 

teacher.backbone.backbone.conv1.weight - torch.Size([64, 3, 7, 7]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.0.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.0.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.0.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.0.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.0.bn3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.0.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.0.downsample.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.0.downsample.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.1.bn3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.1.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.2.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.2.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.2.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.2.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.2.bn3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.2.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.0.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.0.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.0.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.0.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.0.bn3.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.0.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.0.downsample.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.0.downsample.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.1.bn3.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.1.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.2.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.2.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.2.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.2.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.2.bn3.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.2.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.3.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.3.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.3.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.3.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.3.bn3.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.3.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.0.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.0.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.0.downsample.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.0.downsample.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.1.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.1.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.2.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.2.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.2.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.2.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.2.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.2.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.3.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.3.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.3.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.3.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.3.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.3.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.4.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.4.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.4.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.4.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.4.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.4.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.5.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.5.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.5.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.5.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.5.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.5.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.0.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.0.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.0.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.0.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.0.bn3.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.0.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.0.downsample.1.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.0.downsample.1.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.1.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.1.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.1.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.1.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.1.bn3.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.1.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.2.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.2.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.2.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.2.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.2.bn3.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.2.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.residual_alpha - torch.Size([]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.layers.0.self_attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.layers.0.self_attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.layers.0.self_attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.layers.0.self_attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.layers.0.linear1.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.layers.0.linear1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.layers.0.linear2.weight - torch.Size([256, 512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.layers.0.linear2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.layers.0.norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.layers.0.norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.layers.0.norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.layers.0.norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.pooling_queries.1024 - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.pooling_queries.2048 - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.pooling_queries.256 - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.pooling_queries.512 - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.upsample_queries.128x128 - torch.Size([16384, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.upsample_queries.32x32 - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.upsample_queries.64x64 - torch.Size([4096, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.neck.lateral_convs.0.conv.weight - torch.Size([256, 256, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

teacher.neck.lateral_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.neck.lateral_convs.1.conv.weight - torch.Size([256, 512, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

teacher.neck.lateral_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.neck.lateral_convs.2.conv.weight - torch.Size([256, 1024, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

teacher.neck.lateral_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.neck.lateral_convs.3.conv.weight - torch.Size([256, 2048, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

teacher.neck.lateral_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

teacher.neck.fpn_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

teacher.neck.fpn_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

teacher.neck.fpn_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.neck.fpn_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

teacher.neck.fpn_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.rpn_head.rpn_conv.weight - torch.Size([256, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=0 

teacher.rpn_head.rpn_conv.bias - torch.Size([256]): 
NormalInit: mean=0, std=0.01, bias=0 

teacher.rpn_head.rpn_cls.weight - torch.Size([15, 256, 1, 1]): 
NormalInit: mean=0, std=0.01, bias=0 

teacher.rpn_head.rpn_cls.bias - torch.Size([15]): 
NormalInit: mean=0, std=0.01, bias=0 

teacher.rpn_head.rpn_reg.weight - torch.Size([60, 256, 1, 1]): 
NormalInit: mean=0, std=0.01, bias=0 

teacher.rpn_head.rpn_reg.bias - torch.Size([60]): 
NormalInit: mean=0, std=0.01, bias=0 

teacher.roi_head.bbox_head.fc_cls.weight - torch.Size([6, 1024]): 
NormalInit: mean=0, std=0.01, bias=0 

teacher.roi_head.bbox_head.fc_cls.bias - torch.Size([6]): 
NormalInit: mean=0, std=0.01, bias=0 

teacher.roi_head.bbox_head.fc_reg.weight - torch.Size([20, 1024]): 
NormalInit: mean=0, std=0.001, bias=0 

teacher.roi_head.bbox_head.fc_reg.bias - torch.Size([20]): 
NormalInit: mean=0, std=0.001, bias=0 

teacher.roi_head.bbox_head.shared_fcs.0.weight - torch.Size([1024, 12544]): 
XavierInit: gain=1, distribution=uniform, bias=0 

teacher.roi_head.bbox_head.shared_fcs.0.bias - torch.Size([1024]): 
XavierInit: gain=1, distribution=uniform, bias=0 

teacher.roi_head.bbox_head.shared_fcs.1.weight - torch.Size([1024, 1024]): 
XavierInit: gain=1, distribution=uniform, bias=0 

teacher.roi_head.bbox_head.shared_fcs.1.bias - torch.Size([1024]): 
XavierInit: gain=1, distribution=uniform, bias=0 
2025/12/08 14:21:33 - mmengine - WARNING - "FileClient" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io
2025/12/08 14:21:33 - mmengine - WARNING - "HardDiskBackend" is the alias of "LocalBackend" and the former will be deprecated in future.
2025/12/08 14:21:33 - mmengine - INFO - Checkpoints will be saved to /home/coder/data/trong/KLTN/Soft_Teacher/work_dirs/soft_teacher_custom_multi_view.
2025/12/08 14:21:33 - mmengine - INFO - [Supervised Step 0] Groups: 2 , G0:5box | G1:15box
2025/12/08 14:21:39 - mmengine - INFO - [Supervised Step 1] Groups: 2 , G0:8box | G1:17box
2025/12/08 14:21:42 - mmengine - INFO - [Supervised Step 2] Groups: 2 , G0:7box | G1:8box
2025/12/08 14:21:46 - mmengine - INFO - [Supervised Step 3] Groups: 2 , G0:15box | G1:6box
2025/12/08 14:21:49 - mmengine - INFO - [Supervised Step 4] Groups: 2 , G0:8box | G1:3box
2025/12/08 14:21:52 - mmengine - INFO - [Supervised Step 5] Groups: 2 , G0:15box | G1:15box
2025/12/08 14:21:55 - mmengine - INFO - [Supervised Step 6] Groups: 1 , G0:10box
2025/12/08 14:21:57 - mmengine - INFO - [Teacher Predictions] Total boxes: 160, Score range: [0.889, 1.000], Mean: 0.985, Median: 0.996
2025/12/08 14:21:57 - mmengine - INFO - [After Filtering] Threshold: 0.4, Kept: 80/160 (50.0%), Avg: 10.0 boxes/img
2025/12/08 14:21:58 - mmengine - INFO - [Cross-View Uncertainty] Groups: 1, G0:80box(unc=0.051)
2025/12/08 14:22:01 - mmengine - INFO - [Supervised Step 7] Groups: 2 , G0:11box | G1:8box
2025/12/08 14:22:04 - mmengine - INFO - [Supervised Step 8] Groups: 2 , G0:3box | G1:10box
2025/12/08 14:22:07 - mmengine - INFO - [Supervised Step 9] Groups: 2 , G0:14box | G1:14box
2025/12/08 14:24:17 - mmengine - INFO - Epoch(train) [1][ 50/125]  lr: 7.4257e-06  eta: 3 days, 19:06:42  time: 3.2817  data_time: 0.0416  memory: 25318  loss: 2.1832  sup_loss_rpn_cls: 0.2053  sup_loss_rpn_bbox: 0.7042  sup_loss_cls: 0.6618  sup_acc: 68.9873  sup_num_pred_fg: 59.0000  sup_num_pred_bg: 99.0000  sup_num_gt_fg: 52.0000  sup_num_gt_bg: 106.0000  sup_acc_fg: 25.0000  sup_acc_bg: 90.5660  sup_loss_bbox: 0.5857  unsup_loss_rpn_cls: 0.1214  unsup_loss_rpn_bbox: 0.8544  unsup_loss_cls: 0.3305  unsup_acc: 79.8077  unsup_num_pred_fg: 274.0000  unsup_num_pred_bg: 662.0000  unsup_num_gt_fg: 312.0000  unsup_num_gt_bg: 624.0000  unsup_acc_fg: 63.4615  unsup_acc_bg: 87.9808  unsup_loss_bbox: 0.0000
2025/12/08 14:24:17 - mmengine - INFO - [Supervised Step 50] Groups: 2 , G0:2box | G1:7box
2025/12/08 14:26:56 - mmengine - INFO - Epoch(train) [1][100/125]  lr: 9.9010e-06  eta: 3 days, 17:33:56  time: 3.1735  data_time: 0.0340  memory: 25318  loss: 2.0792  sup_loss_rpn_cls: 0.1679  sup_loss_rpn_bbox: 0.5172  sup_loss_cls: 0.3838  sup_acc: 68.0751  sup_num_pred_fg: 26.0000  sup_num_pred_bg: 187.0000  sup_num_gt_fg: 67.0000  sup_num_gt_bg: 146.0000  sup_acc_fg: 17.9104  sup_acc_bg: 91.0959  sup_loss_bbox: 0.9793  unsup_loss_rpn_cls: 0.1214  unsup_loss_rpn_bbox: 0.7849  unsup_loss_cls: 0.5193  unsup_acc: 59.9138  unsup_num_pred_fg: 71.0000  unsup_num_pred_bg: 625.0000  unsup_num_gt_fg: 232.0000  unsup_num_gt_bg: 464.0000  unsup_acc_fg: 3.8793  unsup_acc_bg: 87.9310  unsup_loss_bbox: 0.0000
2025/12/08 14:26:56 - mmengine - INFO - [Supervised Step 100] Groups: 2 , G0:25box | G1:8box
2025/12/08 14:28:15 - mmengine - INFO - Exp name: soft_teacher_custom_multi_view_20251208_142116
2025/12/08 14:29:36 - mmengine - INFO - Epoch(train) [1][150/125]  lr: 1.2376e-05  eta: 3 days, 17:18:06  time: 3.2039  data_time: 0.0335  memory: 25318  loss: 2.0763  sup_loss_rpn_cls: 0.1585  sup_loss_rpn_bbox: 0.4974  sup_loss_cls: 0.3307  sup_acc: 70.7317  sup_num_pred_fg: 25.0000  sup_num_pred_bg: 98.0000  sup_num_gt_fg: 37.0000  sup_num_gt_bg: 86.0000  sup_acc_fg: 32.4324  sup_acc_bg: 87.2093  sup_loss_bbox: 1.0616  unsup_loss_rpn_cls: 0.1247  unsup_loss_rpn_bbox: 0.7271  unsup_loss_cls: 0.5670  unsup_acc: 59.4128  unsup_num_pred_fg: 56.0000  unsup_num_pred_bg: 523.0000  unsup_num_gt_fg: 193.0000  unsup_num_gt_bg: 386.0000  unsup_acc_fg: 2.5907  unsup_acc_bg: 87.8238  unsup_loss_bbox: 0.0000
2025/12/08 14:29:36 - mmengine - INFO - [Supervised Step 150] Groups: 2 , G0:19box | G1:3box
2025/12/08 14:32:09 - mmengine - INFO - Epoch(train) [1][200/125]  lr: 1.4851e-05  eta: 3 days, 16:08:35  time: 3.0590  data_time: 0.0517  memory: 25318  loss: 2.0080  sup_loss_rpn_cls: 0.1491  sup_loss_rpn_bbox: 0.4648  sup_loss_cls: 0.3274  sup_acc: 62.5397  sup_num_pred_fg: 22.0000  sup_num_pred_bg: 293.0000  sup_num_gt_fg: 105.0000  sup_num_gt_bg: 210.0000  sup_acc_fg: 0.0000  sup_acc_bg: 93.8095  sup_loss_bbox: 1.0667  unsup_loss_rpn_cls: 0.1247  unsup_loss_rpn_bbox: 0.7271  unsup_loss_cls: 0.5670  unsup_acc: 59.4128  unsup_num_pred_fg: 56.0000  unsup_num_pred_bg: 523.0000  unsup_num_gt_fg: 193.0000  unsup_num_gt_bg: 386.0000  unsup_acc_fg: 2.5907  unsup_acc_bg: 87.8238  unsup_loss_bbox: 0.0000
2025/12/08 14:32:09 - mmengine - INFO - [Supervised Step 200] Groups: 2 , G0:3box | G1:9box
2025/12/08 14:34:49 - mmengine - INFO - Epoch(train) [1][250/125]  lr: 1.7327e-05  eta: 3 days, 16:11:56  time: 3.1976  data_time: 0.0351  memory: 25318  loss: 2.1057  sup_loss_rpn_cls: 0.1393  sup_loss_rpn_bbox: 0.4583  sup_loss_cls: 0.3261  sup_acc: 54.8204  sup_num_pred_fg: 74.0000  sup_num_pred_bg: 455.0000  sup_num_gt_fg: 175.0000  sup_num_gt_bg: 354.0000  sup_acc_fg: 0.5714  sup_acc_bg: 81.6384  sup_loss_bbox: 1.1285  unsup_loss_rpn_cls: 0.1285  unsup_loss_rpn_bbox: 0.6801  unsup_loss_cls: 0.5768  unsup_acc: 65.5647  unsup_num_pred_fg: 28.0000  unsup_num_pred_bg: 698.0000  unsup_num_gt_fg: 242.0000  unsup_num_gt_bg: 484.0000  unsup_acc_fg: 4.1322  unsup_acc_bg: 96.2810  unsup_loss_bbox: 0.0000
2025/12/08 14:34:49 - mmengine - INFO - [Supervised Step 250] Groups: 2 , G0:9box | G1:11box
2025/12/08 14:37:23 - mmengine - INFO - Epoch(train) [1][300/125]  lr: 1.9802e-05  eta: 3 days, 15:44:32  time: 3.0937  data_time: 0.0366  memory: 25318  loss: 2.1296  sup_loss_rpn_cls: 0.1392  sup_loss_rpn_bbox: 0.4489  sup_loss_cls: 0.3300  sup_acc: 64.5329  sup_num_pred_fg: 37.0000  sup_num_pred_bg: 541.0000  sup_num_gt_fg: 192.0000  sup_num_gt_bg: 386.0000  sup_acc_fg: 1.5625  sup_acc_bg: 95.8549  sup_loss_bbox: 1.2115  unsup_loss_rpn_cls: 0.1285  unsup_loss_rpn_bbox: 0.6801  unsup_loss_cls: 0.5768  unsup_acc: 65.5647  unsup_num_pred_fg: 28.0000  unsup_num_pred_bg: 698.0000  unsup_num_gt_fg: 242.0000  unsup_num_gt_bg: 484.0000  unsup_acc_fg: 4.1322  unsup_acc_bg: 96.2810  unsup_loss_bbox: 0.0000
2025/12/08 14:37:23 - mmengine - INFO - [Supervised Step 300] Groups: 2 , G0:5box | G1:7box
2025/12/08 14:39:59 - mmengine - INFO - Epoch(train) [1][350/125]  lr: 2.2277e-05  eta: 3 days, 15:31:35  time: 3.1248  data_time: 0.0308  memory: 25318  loss: 2.0961  sup_loss_rpn_cls: 0.1344  sup_loss_rpn_bbox: 0.4458  sup_loss_cls: 0.3199  sup_acc: 64.7059  sup_num_pred_fg: 29.0000  sup_num_pred_bg: 107.0000  sup_num_gt_fg: 42.0000  sup_num_gt_bg: 94.0000  sup_acc_fg: 16.6667  sup_acc_bg: 86.1702  sup_loss_bbox: 1.1959  unsup_loss_rpn_cls: 0.1285  unsup_loss_rpn_bbox: 0.6801  unsup_loss_cls: 0.5768  unsup_acc: 65.5647  unsup_num_pred_fg: 28.0000  unsup_num_pred_bg: 698.0000  unsup_num_gt_fg: 242.0000  unsup_num_gt_bg: 484.0000  unsup_acc_fg: 4.1322  unsup_acc_bg: 96.2810  unsup_loss_bbox: 0.0000
2025/12/08 14:40:00 - mmengine - INFO - [Supervised Step 350] Groups: 2 , G0:15box | G1:11box
2025/12/08 14:42:37 - mmengine - INFO - Epoch(train) [1][400/125]  lr: 2.4752e-05  eta: 3 days, 15:26:31  time: 3.1503  data_time: 0.0417  memory: 25318  loss: 2.1468  sup_loss_rpn_cls: 0.1324  sup_loss_rpn_bbox: 0.4299  sup_loss_cls: 0.3324  sup_acc: 65.2015  sup_num_pred_fg: 21.0000  sup_num_pred_bg: 525.0000  sup_num_gt_fg: 178.0000  sup_num_gt_bg: 368.0000  sup_acc_fg: 0.0000  sup_acc_bg: 96.7391  sup_loss_bbox: 1.2297  unsup_loss_rpn_cls: 0.1352  unsup_loss_rpn_bbox: 0.6465  unsup_loss_cls: 0.5584  unsup_acc: 68.0952  unsup_num_pred_fg: 12.0000  unsup_num_pred_bg: 408.0000  unsup_num_gt_fg: 140.0000  unsup_num_gt_bg: 280.0000  unsup_acc_fg: 6.4286  unsup_acc_bg: 98.9286  unsup_loss_bbox: 0.0000
2025/12/08 14:42:37 - mmengine - INFO - [Supervised Step 400] Groups: 2 , G0:11box | G1:6box
2025/12/08 14:45:17 - mmengine - INFO - Epoch(train) [1][450/125]  lr: 2.7228e-05  eta: 3 days, 15:32:32  time: 3.2075  data_time: 0.0516  memory: 25318  loss: 2.1337  sup_loss_rpn_cls: 0.1310  sup_loss_rpn_bbox: 0.4309  sup_loss_cls: 0.3106  sup_acc: 58.9831  sup_num_pred_fg: 45.0000  sup_num_pred_bg: 250.0000  sup_num_gt_fg: 95.0000  sup_num_gt_bg: 200.0000  sup_acc_fg: 2.1053  sup_acc_bg: 86.0000  sup_loss_bbox: 1.2385  unsup_loss_rpn_cls: 0.1397  unsup_loss_rpn_bbox: 0.6248  unsup_loss_cls: 0.5475  unsup_acc: 66.2526  unsup_num_pred_fg: 20.0000  unsup_num_pred_bg: 463.0000  unsup_num_gt_fg: 161.0000  unsup_num_gt_bg: 322.0000  unsup_acc_fg: 3.7267  unsup_acc_bg: 97.5155  unsup_loss_bbox: 0.0000
2025/12/08 14:45:17 - mmengine - INFO - [Supervised Step 450] Groups: 2 , G0:9box | G1:10box
2025/12/08 14:47:58 - mmengine - INFO - Epoch(train) [1][500/125]  lr: 2.9703e-05  eta: 3 days, 15:36:05  time: 3.2031  data_time: 0.0415  memory: 25318  loss: 2.1179  sup_loss_rpn_cls: 0.1281  sup_loss_rpn_bbox: 0.4277  sup_loss_cls: 0.3191  sup_acc: 71.1790  sup_num_pred_fg: 13.0000  sup_num_pred_bg: 216.0000  sup_num_gt_fg: 71.0000  sup_num_gt_bg: 158.0000  sup_acc_fg: 11.2676  sup_acc_bg: 98.1013  sup_loss_bbox: 1.2236  unsup_loss_rpn_cls: 0.1438  unsup_loss_rpn_bbox: 0.5986  unsup_loss_cls: 0.5271  unsup_acc: 68.7179  unsup_num_pred_fg: 20.0000  unsup_num_pred_bg: 370.0000  unsup_num_gt_fg: 130.0000  unsup_num_gt_bg: 260.0000  unsup_acc_fg: 9.2308  unsup_acc_bg: 98.4615  unsup_loss_bbox: 0.0000
2025/12/08 14:47:58 - mmengine - INFO - [Supervised Step 500] Groups: 2 , G0:8box | G1:9box
2025/12/08 14:50:34 - mmengine - INFO - Epoch(train) [1][550/125]  lr: 3.2178e-05  eta: 3 days, 15:28:54  time: 3.1393  data_time: 0.0435  memory: 25318  loss: 2.1064  sup_loss_rpn_cls: 0.1275  sup_loss_rpn_bbox: 0.4182  sup_loss_cls: 0.3126  sup_acc: 72.1805  sup_num_pred_fg: 9.0000  sup_num_pred_bg: 124.0000  sup_num_gt_fg: 41.0000  sup_num_gt_bg: 92.0000  sup_acc_fg: 14.6341  sup_acc_bg: 97.8261  sup_loss_bbox: 1.2258  unsup_loss_rpn_cls: 0.1453  unsup_loss_rpn_bbox: 0.5825  unsup_loss_cls: 0.5243  unsup_acc: 67.0330  unsup_num_pred_fg: 8.0000  unsup_num_pred_bg: 538.0000  unsup_num_gt_fg: 182.0000  unsup_num_gt_bg: 364.0000  unsup_acc_fg: 1.6484  unsup_acc_bg: 99.7253  unsup_loss_bbox: 0.0000
2025/12/08 14:50:34 - mmengine - INFO - [Supervised Step 550] Groups: 2 , G0:5box | G1:7box
2025/12/08 14:53:13 - mmengine - INFO - Epoch(train) [1][600/125]  lr: 3.4653e-05  eta: 3 days, 15:27:15  time: 3.1740  data_time: 0.0335  memory: 25318  loss: 2.0905  sup_loss_rpn_cls: 0.1288  sup_loss_rpn_bbox: 0.4210  sup_loss_cls: 0.3074  sup_acc: 65.1865  sup_num_pred_fg: 9.0000  sup_num_pred_bg: 554.0000  sup_num_gt_fg: 187.0000  sup_num_gt_bg: 376.0000  sup_acc_fg: 0.0000  sup_acc_bg: 97.6064  sup_loss_bbox: 1.2149  unsup_loss_rpn_cls: 0.1476  unsup_loss_rpn_bbox: 0.5628  unsup_loss_cls: 0.5089  unsup_acc: 66.6667  unsup_num_pred_fg: 30.0000  unsup_num_pred_bg: 510.0000  unsup_num_gt_fg: 180.0000  unsup_num_gt_bg: 360.0000  unsup_acc_fg: 6.1111  unsup_acc_bg: 96.9445  unsup_loss_bbox: 0.0000
2025/12/08 14:53:13 - mmengine - INFO - [Supervised Step 600] Groups: 2 , G0:7box | G1:14box
2025/12/08 14:55:51 - mmengine - INFO - Epoch(train) [1][650/125]  lr: 3.7129e-05  eta: 3 days, 15:23:10  time: 3.1560  data_time: 0.0467  memory: 25318  loss: 2.1074  sup_loss_rpn_cls: 0.1266  sup_loss_rpn_bbox: 0.4239  sup_loss_cls: 0.3131  sup_acc: 66.0508  sup_num_pred_fg: 103.0000  sup_num_pred_bg: 1196.0000  sup_num_gt_fg: 433.0000  sup_num_gt_bg: 866.0000  sup_acc_fg: 10.6236  sup_acc_bg: 93.7644  sup_loss_bbox: 1.2243  unsup_loss_rpn_cls: 0.1485  unsup_loss_rpn_bbox: 0.5484  unsup_loss_cls: 0.4998  unsup_acc: 66.6667  unsup_num_pred_fg: 14.0000  unsup_num_pred_bg: 406.0000  unsup_num_gt_fg: 140.0000  unsup_num_gt_bg: 280.0000  unsup_acc_fg: 4.2857  unsup_acc_bg: 97.8571  unsup_loss_bbox: 0.0000
2025/12/08 14:55:51 - mmengine - INFO - [Supervised Step 650] Groups: 2 , G0:9box | G1:11box
2025/12/08 14:58:29 - mmengine - INFO - Epoch(train) [1][700/125]  lr: 3.9604e-05  eta: 3 days, 15:18:50  time: 3.1521  data_time: 0.0383  memory: 25318  loss: 2.0871  sup_loss_rpn_cls: 0.1269  sup_loss_rpn_bbox: 0.4143  sup_loss_cls: 0.3009  sup_acc: 79.7386  sup_num_pred_fg: 58.0000  sup_num_pred_bg: 248.0000  sup_num_gt_fg: 100.0000  sup_num_gt_bg: 206.0000  sup_acc_fg: 46.0000  sup_acc_bg: 96.1165  sup_loss_bbox: 1.2272  unsup_loss_rpn_cls: 0.1500  unsup_loss_rpn_bbox: 0.5327  unsup_loss_cls: 0.4883  unsup_acc: 68.9266  unsup_num_pred_fg: 18.0000  unsup_num_pred_bg: 513.0000  unsup_num_gt_fg: 177.0000  unsup_num_gt_bg: 354.0000  unsup_acc_fg: 8.4746  unsup_acc_bg: 99.1525  unsup_loss_bbox: 0.0000
2025/12/08 14:58:29 - mmengine - INFO - [Supervised Step 700] Groups: 2 , G0:8box | G1:10box
2025/12/08 15:00:59 - mmengine - INFO - Epoch(train) [1][750/125]  lr: 4.2079e-05  eta: 3 days, 14:59:12  time: 3.0114  data_time: 0.0453  memory: 25318  loss: 2.0472  sup_loss_rpn_cls: 0.1245  sup_loss_rpn_bbox: 0.4038  sup_loss_cls: 0.2908  sup_acc: 64.7194  sup_num_pred_fg: 28.0000  sup_num_pred_bg: 845.0000  sup_num_gt_fg: 289.0000  sup_num_gt_bg: 584.0000  sup_acc_fg: 1.3841  sup_acc_bg: 96.0616  sup_loss_bbox: 1.2281  unsup_loss_rpn_cls: 0.1500  unsup_loss_rpn_bbox: 0.5327  unsup_loss_cls: 0.4883  unsup_acc: 68.9266  unsup_num_pred_fg: 18.0000  unsup_num_pred_bg: 513.0000  unsup_num_gt_fg: 177.0000  unsup_num_gt_bg: 354.0000  unsup_acc_fg: 8.4746  unsup_acc_bg: 99.1525  unsup_loss_bbox: 0.0000
2025/12/08 15:00:59 - mmengine - INFO - [Supervised Step 750] Groups: 2 , G0:11box | G1:3box
2025/12/08 15:03:15 - mmengine - INFO - Epoch(train) [1][800/125]  lr: 4.4554e-05  eta: 3 days, 14:10:20  time: 2.7076  data_time: 0.0361  memory: 25318  loss: 2.0843  sup_loss_rpn_cls: 0.1231  sup_loss_rpn_bbox: 0.4056  sup_loss_cls: 0.3081  sup_acc: 73.3615  sup_num_pred_fg: 49.0000  sup_num_pred_bg: 424.0000  sup_num_gt_fg: 155.0000  sup_num_gt_bg: 318.0000  sup_acc_fg: 24.5161  sup_acc_bg: 97.1698  sup_loss_bbox: 1.2150  unsup_loss_rpn_cls: 0.1502  unsup_loss_rpn_bbox: 0.5072  unsup_loss_cls: 0.4622  unsup_acc: 69.3333  unsup_num_pred_fg: 25.0000  unsup_num_pred_bg: 575.0000  unsup_num_gt_fg: 200.0000  unsup_num_gt_bg: 400.0000  unsup_acc_fg: 9.5000  unsup_acc_bg: 99.2500  unsup_loss_bbox: 0.0000
2025/12/08 15:03:15 - mmengine - INFO - [Supervised Step 800] Groups: 2 , G0:2box | G1:14box
2025/12/08 15:05:48 - mmengine - INFO - Epoch(train) [1][850/125]  lr: 4.7030e-05  eta: 3 days, 14:01:06  time: 3.0592  data_time: 0.0471  memory: 25318  loss: 2.0525  sup_loss_rpn_cls: 0.1206  sup_loss_rpn_bbox: 0.4083  sup_loss_cls: 0.2856  sup_acc: 75.7991  sup_num_pred_fg: 23.0000  sup_num_pred_bg: 196.0000  sup_num_gt_fg: 69.0000  sup_num_gt_bg: 150.0000  sup_acc_fg: 27.5362  sup_acc_bg: 98.0000  sup_loss_bbox: 1.2380  unsup_loss_rpn_cls: 0.1502  unsup_loss_rpn_bbox: 0.5072  unsup_loss_cls: 0.4622  unsup_acc: 69.3333  unsup_num_pred_fg: 25.0000  unsup_num_pred_bg: 575.0000  unsup_num_gt_fg: 200.0000  unsup_num_gt_bg: 400.0000  unsup_acc_fg: 9.5000  unsup_acc_bg: 99.2500  unsup_loss_bbox: 0.0000
2025/12/08 15:05:48 - mmengine - INFO - [Supervised Step 850] Groups: 2 , G0:13box | G1:3box
2025/12/08 15:08:23 - mmengine - INFO - Epoch(train) [1][900/125]  lr: 4.9505e-05  eta: 3 days, 13:58:04  time: 3.1184  data_time: 0.0440  memory: 25318  loss: 2.0496  sup_loss_rpn_cls: 0.1269  sup_loss_rpn_bbox: 0.4011  sup_loss_cls: 0.3009  sup_acc: 65.0394  sup_num_pred_fg: 51.0000  sup_num_pred_bg: 584.0000  sup_num_gt_fg: 207.0000  sup_num_gt_bg: 428.0000  sup_acc_fg: 3.8647  sup_acc_bg: 94.6262  sup_loss_bbox: 1.2071  unsup_loss_rpn_cls: 0.1507  unsup_loss_rpn_bbox: 0.4940  unsup_loss_cls: 0.4456  unsup_acc: 70.5718  unsup_num_pred_fg: 44.0000  unsup_num_pred_bg: 673.0000  unsup_num_gt_fg: 239.0000  unsup_num_gt_bg: 478.0000  unsup_acc_fg: 14.2259  unsup_acc_bg: 98.7448  unsup_loss_bbox: 0.0000
2025/12/08 15:08:23 - mmengine - INFO - [Supervised Step 900] Groups: 2 , G0:2box | G1:8box
2025/12/08 15:10:59 - mmengine - INFO - Epoch(train) [1][950/125]  lr: 5.1980e-05  eta: 3 days, 13:54:20  time: 3.1100  data_time: 0.0368  memory: 25318  loss: 2.0522  sup_loss_rpn_cls: 0.1222  sup_loss_rpn_bbox: 0.3948  sup_loss_cls: 0.2921  sup_acc: 65.2456  sup_num_pred_fg: 83.0000  sup_num_pred_bg: 996.0000  sup_num_gt_fg: 359.0000  sup_num_gt_bg: 720.0000  sup_acc_fg: 8.0780  sup_acc_bg: 93.7500  sup_loss_bbox: 1.2294  unsup_loss_rpn_cls: 0.1504  unsup_loss_rpn_bbox: 0.4826  unsup_loss_cls: 0.4318  unsup_acc: 70.5739  unsup_num_pred_fg: 54.0000  unsup_num_pred_bg: 765.0000  unsup_num_gt_fg: 273.0000  unsup_num_gt_bg: 546.0000  unsup_acc_fg: 14.6520  unsup_acc_bg: 98.5348  unsup_loss_bbox: 0.0000
2025/12/08 15:10:59 - mmengine - INFO - [Supervised Step 950] Groups: 2 , G0:6box | G1:8box
