2025/12/09 18:36:41 - mmengine - INFO - 
------------------------------------------------------------
System environment:
    sys.platform: linux
    Python: 3.9.19 (main, May  6 2024, 19:43:03) [GCC 11.2.0]
    CUDA available: True
    MUSA available: False
    numpy_random_seed: 989575180
    GPU 0: NVIDIA RTX A6000
    CUDA_HOME: /usr/local/cuda
    NVCC: Cuda compilation tools, release 12.6, V12.6.85
    GCC: gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
    PyTorch: 2.0.0+cu117
    PyTorch compiling details: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

    TorchVision: 0.15.1+cu117
    OpenCV: 4.11.0
    MMEngine: 0.11.0rc0

Runtime environment:
    cudnn_benchmark: False
    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}
    dist_cfg: {'backend': 'nccl'}
    seed: 989575180
    Distributed launcher: none
    Distributed training: False
    GPU number: 1
------------------------------------------------------------

2025/12/09 18:36:44 - mmengine - INFO - Config:
backend_args = None
batch_size = 2
branch_field = [
    'sup',
    'unsup_teacher',
    'unsup_student',
]
color_space = [
    [
        dict(prob=0.5, type='AutoContrast'),
    ],
    [
        dict(prob=0.4, type='Equalize'),
    ],
    [
        dict(level=7, max_mag=1.25, min_mag=0.8, prob=0.6, type='Brightness'),
    ],
    [
        dict(level=7, max_mag=1.3, min_mag=0.8, prob=0.6, type='Contrast'),
    ],
    [
        dict(level=6, max_mag=1.5, min_mag=0.8, prob=0.4, type='Sharpness'),
    ],
    [
        dict(level=5, max_mag=7.0, min_mag=5.0, prob=0.2, type='Posterize'),
    ],
]
custom_hooks = [
    dict(momentum=0.999, type='MeanTeacherHook'),
]
custom_imports = dict(
    allow_failed_imports=False,
    imports=[
        'mmdet.models.utils.multi_view',
        'mmdet.models.utils.multi_view_transformer',
        'mmdet.datasets.wrappers.multi_view_from_folder',
    ])
data_root = '/home/coder/data/trong/KLTN/Soft_Teacher/data_drill/'
dataset_type = 'CocoDataset'
default_hooks = dict(
    checkpoint=dict(
        interval=1,
        max_keep_ckpts=1,
        rule='greater',
        save_best='teacher/coco/bbox_mAP_50',
        type='CheckpointHook'),
    logger=dict(interval=50, type='LoggerHook'),
    param_scheduler=dict(type='ParamSchedulerHook'),
    sampler_seed=dict(type='DistSamplerSeedHook'),
    timer=dict(type='IterTimerHook'),
    visualization=dict(type='DetVisualizationHook'))
default_scope = 'mmdet'
detector = dict(
    backbone=dict(
        backbone=dict(
            depth=50,
            frozen_stages=1,
            init_cfg=dict(
                checkpoint='open-mmlab://detectron2/resnet50_caffe',
                type='Pretrained'),
            norm_cfg=dict(requires_grad=False, type='BN'),
            norm_eval=True,
            num_stages=4,
            out_indices=(
                0,
                1,
                2,
                3,
            ),
            style='caffe',
            type='ResNet'),
        concat_reduce=False,
        fusion='mvvit',
        mvvit=dict(
            dropout=0.2,
            embed_dim=256,
            mlp_ratio=2.0,
            num_heads=4,
            num_layers=1,
            spatial_attention='moderate',
            type='MVViT',
            use_gradient_checkpointing=True,
            use_layer_norm=True),
        type='MultiViewBackbone',
        views_per_sample=8),
    data_preprocessor=dict(
        bgr_to_rgb=False,
        mean=[
            103.53,
            116.28,
            123.675,
        ],
        pad_size_divisor=8,
        std=[
            1.0,
            1.0,
            1.0,
        ],
        type='DetDataPreprocessor'),
    neck=dict(
        in_channels=[
            256,
            512,
            1024,
            2048,
        ],
        num_outs=5,
        out_channels=256,
        type='FPN'),
    roi_head=dict(
        bbox_head=dict(
            bbox_coder=dict(
                target_means=[
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                ],
                target_stds=[
                    0.1,
                    0.1,
                    0.2,
                    0.2,
                ],
                type='DeltaXYWHBBoxCoder'),
            fc_out_channels=1024,
            in_channels=256,
            loss_bbox=dict(loss_weight=1.0, type='L1Loss'),
            loss_cls=dict(
                alpha=0.65,
                gamma=2.2,
                loss_weight=1.5,
                type='FocalLoss',
                use_sigmoid=True),
            num_classes=5,
            reg_class_agnostic=False,
            roi_feat_size=7,
            type='Shared2FCBBoxHead'),
        bbox_roi_extractor=dict(
            featmap_strides=[
                4,
                8,
                16,
                32,
            ],
            out_channels=256,
            roi_layer=dict(output_size=7, sampling_ratio=0, type='RoIAlign'),
            type='SingleRoIExtractor'),
        type='StandardRoIHead'),
    rpn_head=dict(
        anchor_generator=dict(
            ratios=[
                0.3,
                0.45,
                0.6,
                1.0,
                2.0,
            ],
            scales=[
                8,
                16,
                32,
            ],
            strides=[
                4,
                8,
                16,
                32,
                64,
            ],
            type='AnchorGenerator'),
        bbox_coder=dict(
            target_means=[
                0.0,
                0.0,
                0.0,
                0.0,
            ],
            target_stds=[
                1.0,
                1.0,
                1.0,
                1.0,
            ],
            type='DeltaXYWHBBoxCoder'),
        feat_channels=256,
        in_channels=256,
        loss_bbox=dict(loss_weight=1.0, type='L1Loss'),
        loss_cls=dict(
            alpha=0.65,
            gamma=2.0,
            loss_weight=2.0,
            type='FocalLoss',
            use_sigmoid=True),
        type='RPNHead'),
    test_cfg=dict(
        rcnn=dict(
            max_per_img=20,
            nms=dict(iou_threshold=0.5, type='nms'),
            score_thr=0.5),
        rpn=dict(
            max_per_img=1000,
            min_bbox_size=8,
            nms=dict(iou_threshold=0.7, type='nms'),
            nms_pre=2000)),
    train_cfg=dict(
        rcnn=dict(
            assigner=dict(
                ignore_iof_thr=-1,
                match_low_quality=False,
                min_pos_iou=0.5,
                neg_iou_thr=0.5,
                pos_iou_thr=0.5,
                type='MaxIoUAssigner'),
            debug=False,
            pos_weight=-1,
            sampler=dict(
                add_gt_as_proposals=True,
                neg_pos_ub=1.5,
                num=256,
                pos_fraction=0.7,
                type='RandomSampler')),
        rpn=dict(
            allowed_border=-1,
            assigner=dict(
                ignore_iof_thr=-1,
                match_low_quality=True,
                min_pos_iou=0.3,
                neg_iou_thr=0.3,
                pos_iou_thr=0.5,
                type='MaxIoUAssigner'),
            debug=False,
            pos_weight=-1,
            sampler=dict(
                add_gt_as_proposals=False,
                neg_pos_ub=1.5,
                num=256,
                pos_fraction=0.7,
                type='RandomSampler')),
        rpn_proposal=dict(
            max_per_img=1000,
            min_bbox_size=0,
            nms=dict(iou_threshold=0.7, type='nms'),
            nms_pre=2000)),
    type='FasterRCNN')
env_cfg = dict(
    cudnn_benchmark=False,
    dist_cfg=dict(backend='nccl'),
    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))
geometric = [
    [
        dict(level=7, max_mag=0.05, min_mag=0.0, prob=0.3, type='TranslateX'),
    ],
    [
        dict(level=7, max_mag=0.05, min_mag=0.0, prob=0.3, type='TranslateY'),
    ],
]
img_scale = (
    256,
    720,
)
labeled_dataset = dict(
    ann_file=
    '/home/coder/data/trong/KLTN/Soft_Teacher/data_drill/semi_anno_multiview/_annotations.coco.labeled.grouped@40.bright.json',
    backend_args=None,
    data_prefix=dict(img='train/'),
    data_root='/home/coder/data/trong/KLTN/Soft_Teacher/data_drill/',
    filter_cfg=dict(filter_empty_gt=True, min_size=4),
    metainfo=dict(
        classes=(
            'Broken',
            'Chipped',
            'Scratched',
            'Severe_Rust',
            'Tip_Wear',
        ),
        palette=[
            (
                134,
                34,
                255,
            ),
            (
                0,
                255,
                206,
            ),
            (
                255,
                128,
                0,
            ),
            (
                254,
                0,
                86,
            ),
            (
                199,
                252,
                0,
            ),
        ]),
    pipeline=[
        dict(
            backend_args=None,
            imdecode_backend='cv2',
            to_float32=False,
            type='LoadImageFromFile'),
        dict(type='LoadAnnotations', with_bbox=True),
        dict(keep_ratio=True, scale=(
            256,
            720,
        ), type='Resize'),
        dict(prob=0.5, type='RandomFlip'),
        dict(
            aug_num=1,
            aug_space=[
                [
                    dict(prob=0.5, type='AutoContrast'),
                ],
                [
                    dict(prob=0.4, type='Equalize'),
                ],
                [
                    dict(
                        level=7,
                        max_mag=1.25,
                        min_mag=0.8,
                        prob=0.6,
                        type='Brightness'),
                ],
                [
                    dict(
                        level=7,
                        max_mag=1.3,
                        min_mag=0.8,
                        prob=0.6,
                        type='Contrast'),
                ],
                [
                    dict(
                        level=6,
                        max_mag=1.5,
                        min_mag=0.8,
                        prob=0.4,
                        type='Sharpness'),
                ],
                [
                    dict(
                        level=5,
                        max_mag=7.0,
                        min_mag=5.0,
                        prob=0.2,
                        type='Posterize'),
                ],
            ],
            type='RandAugment'),
        dict(min_gt_bbox_wh=(
            4,
            4,
        ), type='FilterAnnotations'),
        dict(
            branch_field=[
                'sup',
                'unsup_teacher',
                'unsup_student',
            ],
            sup=dict(type='PackDetInputs'),
            type='MultiBranch'),
    ],
    type='MultiViewFromFolder',
    views_per_sample=8)
launcher = 'none'
load_from = None
log_level = 'INFO'
log_processor = dict(by_epoch=True, type='LogProcessor', window_size=50)
metainfo = dict(
    classes=(
        'Broken',
        'Chipped',
        'Scratched',
        'Severe_Rust',
        'Tip_Wear',
    ),
    palette=[
        (
            134,
            34,
            255,
        ),
        (
            0,
            255,
            206,
        ),
        (
            255,
            128,
            0,
        ),
        (
            254,
            0,
            86,
        ),
        (
            199,
            252,
            0,
        ),
    ])
model = dict(
    aggregate_views='mean',
    data_preprocessor=dict(
        data_preprocessor=dict(
            bgr_to_rgb=False,
            mean=[
                103.53,
                116.28,
                123.675,
            ],
            pad_size_divisor=8,
            std=[
                1.0,
                1.0,
                1.0,
            ],
            type='DetDataPreprocessor'),
        type='MultiBranchDataPreprocessor'),
    detector=dict(
        backbone=dict(
            backbone=dict(
                depth=50,
                frozen_stages=1,
                init_cfg=dict(
                    checkpoint='open-mmlab://detectron2/resnet50_caffe',
                    type='Pretrained'),
                norm_cfg=dict(requires_grad=False, type='BN'),
                norm_eval=True,
                num_stages=4,
                out_indices=(
                    0,
                    1,
                    2,
                    3,
                ),
                style='caffe',
                type='ResNet'),
            concat_reduce=False,
            fusion='mvvit',
            mvvit=dict(
                dropout=0.2,
                embed_dim=256,
                mlp_ratio=2.0,
                num_heads=4,
                num_layers=1,
                spatial_attention='moderate',
                type='MVViT',
                use_gradient_checkpointing=True,
                use_layer_norm=True),
            type='MultiViewBackbone',
            views_per_sample=8),
        data_preprocessor=dict(
            bgr_to_rgb=False,
            mean=[
                103.53,
                116.28,
                123.675,
            ],
            pad_size_divisor=8,
            std=[
                1.0,
                1.0,
                1.0,
            ],
            type='DetDataPreprocessor'),
        neck=dict(
            in_channels=[
                256,
                512,
                1024,
                2048,
            ],
            num_outs=5,
            out_channels=256,
            type='FPN'),
        roi_head=dict(
            bbox_head=dict(
                bbox_coder=dict(
                    target_means=[
                        0.0,
                        0.0,
                        0.0,
                        0.0,
                    ],
                    target_stds=[
                        0.1,
                        0.1,
                        0.2,
                        0.2,
                    ],
                    type='DeltaXYWHBBoxCoder'),
                fc_out_channels=1024,
                in_channels=256,
                loss_bbox=dict(loss_weight=1.0, type='L1Loss'),
                loss_cls=dict(
                    alpha=0.65,
                    gamma=2.2,
                    loss_weight=1.5,
                    type='FocalLoss',
                    use_sigmoid=True),
                num_classes=5,
                reg_class_agnostic=False,
                roi_feat_size=7,
                type='Shared2FCBBoxHead'),
            bbox_roi_extractor=dict(
                featmap_strides=[
                    4,
                    8,
                    16,
                    32,
                ],
                out_channels=256,
                roi_layer=dict(
                    output_size=7, sampling_ratio=0, type='RoIAlign'),
                type='SingleRoIExtractor'),
            type='StandardRoIHead'),
        rpn_head=dict(
            anchor_generator=dict(
                ratios=[
                    0.3,
                    0.45,
                    0.6,
                    1.0,
                    2.0,
                ],
                scales=[
                    8,
                    16,
                    32,
                ],
                strides=[
                    4,
                    8,
                    16,
                    32,
                    64,
                ],
                type='AnchorGenerator'),
            bbox_coder=dict(
                target_means=[
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                ],
                target_stds=[
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                ],
                type='DeltaXYWHBBoxCoder'),
            feat_channels=256,
            in_channels=256,
            loss_bbox=dict(loss_weight=1.0, type='L1Loss'),
            loss_cls=dict(
                alpha=0.65,
                gamma=2.0,
                loss_weight=2.0,
                type='FocalLoss',
                use_sigmoid=True),
            type='RPNHead'),
        test_cfg=dict(
            rcnn=dict(
                max_per_img=20,
                nms=dict(iou_threshold=0.5, type='nms'),
                score_thr=0.5),
            rpn=dict(
                max_per_img=1000,
                min_bbox_size=8,
                nms=dict(iou_threshold=0.7, type='nms'),
                nms_pre=2000)),
        train_cfg=dict(
            rcnn=dict(
                assigner=dict(
                    ignore_iof_thr=-1,
                    match_low_quality=False,
                    min_pos_iou=0.5,
                    neg_iou_thr=0.5,
                    pos_iou_thr=0.5,
                    type='MaxIoUAssigner'),
                debug=False,
                pos_weight=-1,
                sampler=dict(
                    add_gt_as_proposals=True,
                    neg_pos_ub=1.5,
                    num=256,
                    pos_fraction=0.7,
                    type='RandomSampler')),
            rpn=dict(
                allowed_border=-1,
                assigner=dict(
                    ignore_iof_thr=-1,
                    match_low_quality=True,
                    min_pos_iou=0.3,
                    neg_iou_thr=0.3,
                    pos_iou_thr=0.5,
                    type='MaxIoUAssigner'),
                debug=False,
                pos_weight=-1,
                sampler=dict(
                    add_gt_as_proposals=False,
                    neg_pos_ub=1.5,
                    num=256,
                    pos_fraction=0.7,
                    type='RandomSampler')),
            rpn_proposal=dict(
                max_per_img=1000,
                min_bbox_size=0,
                nms=dict(iou_threshold=0.7, type='nms'),
                nms_pre=2000)),
        type='FasterRCNN'),
    semi_test_cfg=dict(predict_on='teacher'),
    semi_train_cfg=dict(
        cls_pseudo_thr=0.35,
        enable_consensus=True,
        freeze_teacher=True,
        jitter_scale=0.03,
        jitter_times=5,
        min_pseudo_bbox_wh=(
            8,
            8,
        ),
        pseudo_label_initial_score_thr=0.4,
        reg_pseudo_thr=0.085,
        rpn_pseudo_thr=0.35,
        sup_weight=1.0,
        unsup_weight=1.0),
    type='MultiViewSoftTeacher',
    views_per_sample=8)
num_workers = 4
optim_wrapper = dict(
    optimizer=dict(lr=0.0005, momentum=0.9, type='SGD', weight_decay=0.0001),
    type='OptimWrapper')
param_scheduler = [
    dict(
        begin=0, by_epoch=False, end=10000, start_factor=0.01,
        type='LinearLR'),
    dict(
        begin=10000, by_epoch=False, end=70000, factor=1.0, type='ConstantLR'),
    dict(
        begin=70000,
        by_epoch=False,
        end=100000,
        eta_min=5e-05,
        type='CosineAnnealingLR'),
]
resume = False
strong_pipeline = [
    dict(keep_ratio=True, scale=(
        256,
        720,
    ), type='Resize'),
    dict(prob=0.5, type='RandomFlip'),
    dict(
        transforms=[
            dict(
                aug_num=2,
                aug_space=[
                    [
                        dict(prob=0.5, type='AutoContrast'),
                    ],
                    [
                        dict(prob=0.4, type='Equalize'),
                    ],
                    [
                        dict(
                            level=7,
                            max_mag=1.25,
                            min_mag=0.8,
                            prob=0.6,
                            type='Brightness'),
                    ],
                    [
                        dict(
                            level=7,
                            max_mag=1.3,
                            min_mag=0.8,
                            prob=0.6,
                            type='Contrast'),
                    ],
                    [
                        dict(
                            level=6,
                            max_mag=1.5,
                            min_mag=0.8,
                            prob=0.4,
                            type='Sharpness'),
                    ],
                    [
                        dict(
                            level=5,
                            max_mag=7.0,
                            min_mag=5.0,
                            prob=0.2,
                            type='Posterize'),
                    ],
                ],
                type='RandAugment'),
            dict(
                aug_num=1,
                aug_space=[
                    [
                        dict(
                            level=7,
                            max_mag=0.05,
                            min_mag=0.0,
                            prob=0.3,
                            type='TranslateX'),
                    ],
                    [
                        dict(
                            level=7,
                            max_mag=0.05,
                            min_mag=0.0,
                            prob=0.3,
                            type='TranslateY'),
                    ],
                ],
                type='RandAugment'),
        ],
        type='RandomOrder'),
    dict(n_patches=(
        1,
        3,
    ), ratio=(
        0,
        0.15,
    ), type='RandomErasing'),
    dict(min_gt_bbox_wh=(
        4,
        4,
    ), type='FilterAnnotations'),
    dict(
        meta_keys=(
            'img_id',
            'img_path',
            'ori_shape',
            'img_shape',
            'scale_factor',
            'flip',
            'flip_direction',
            'homography_matrix',
        ),
        type='PackDetInputs'),
]
sup_pipeline = [
    dict(
        backend_args=None,
        imdecode_backend='cv2',
        to_float32=False,
        type='LoadImageFromFile'),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(keep_ratio=True, scale=(
        256,
        720,
    ), type='Resize'),
    dict(prob=0.5, type='RandomFlip'),
    dict(
        aug_num=1,
        aug_space=[
            [
                dict(prob=0.5, type='AutoContrast'),
            ],
            [
                dict(prob=0.4, type='Equalize'),
            ],
            [
                dict(
                    level=7,
                    max_mag=1.25,
                    min_mag=0.8,
                    prob=0.6,
                    type='Brightness'),
            ],
            [
                dict(
                    level=7,
                    max_mag=1.3,
                    min_mag=0.8,
                    prob=0.6,
                    type='Contrast'),
            ],
            [
                dict(
                    level=6,
                    max_mag=1.5,
                    min_mag=0.8,
                    prob=0.4,
                    type='Sharpness'),
            ],
            [
                dict(
                    level=5,
                    max_mag=7.0,
                    min_mag=5.0,
                    prob=0.2,
                    type='Posterize'),
            ],
        ],
        type='RandAugment'),
    dict(min_gt_bbox_wh=(
        4,
        4,
    ), type='FilterAnnotations'),
    dict(
        branch_field=[
            'sup',
            'unsup_teacher',
            'unsup_student',
        ],
        sup=dict(type='PackDetInputs'),
        type='MultiBranch'),
]
test_cfg = dict(type='TestLoop')
test_dataloader = dict(
    batch_size=1,
    collate_fn=dict(
        type=
        'mmdet.datasets.wrappers.multi_view_from_folder.multi_view_collate_val'
    ),
    dataset=dict(
        ann_file=
        '/home/coder/data/trong/KLTN/Soft_Teacher/data_drill/anno_valid/_annotations_filtered.bright.coco.json',
        backend_args=None,
        data_prefix=dict(img='valid/'),
        data_root='/home/coder/data/trong/KLTN/Soft_Teacher/data_drill/',
        metainfo=dict(
            classes=(
                'Broken',
                'Chipped',
                'Scratched',
                'Severe_Rust',
                'Tip_Wear',
            ),
            palette=[
                (
                    134,
                    34,
                    255,
                ),
                (
                    0,
                    255,
                    206,
                ),
                (
                    255,
                    128,
                    0,
                ),
                (
                    254,
                    0,
                    86,
                ),
                (
                    199,
                    252,
                    0,
                ),
            ]),
        pipeline=[
            dict(
                backend_args=None,
                imdecode_backend='cv2',
                to_float32=False,
                type='LoadImageFromFile'),
            dict(keep_ratio=True, scale=(
                256,
                720,
            ), type='Resize'),
            dict(
                meta_keys=(
                    'img_id',
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                ),
                type='PackDetInputs'),
        ],
        test_mode=True,
        type='MultiViewFromFolder',
        views_per_sample=8),
    drop_last=False,
    num_workers=2,
    persistent_workers=True,
    pin_memory=True,
    sampler=dict(shuffle=False, type='DefaultSampler'))
test_evaluator = dict(
    aggregation='none',
    ann_file=
    '/home/coder/data/trong/KLTN/Soft_Teacher/data_drill/anno_valid/_annotations_filtered.bright.coco.json',
    backend_args=None,
    classwise=True,
    enable_aggregation=False,
    extract_base_name=True,
    format_only=False,
    metric='bbox',
    nms_iou_thr=0.5,
    type='MultiViewCocoMetric',
    views_per_sample=8)
test_pipeline = [
    dict(
        backend_args=None,
        imdecode_backend='cv2',
        to_float32=False,
        type='LoadImageFromFile'),
    dict(keep_ratio=True, scale=(
        256,
        720,
    ), type='Resize'),
    dict(
        meta_keys=(
            'img_id',
            'img_path',
            'ori_shape',
            'img_shape',
            'scale_factor',
        ),
        type='PackDetInputs'),
]
train_cfg = dict(
    max_iters=100000, type='IterBasedTrainLoop', val_interval=10000)
train_dataloader = dict(
    batch_size=2,
    collate_fn=dict(
        type=
        'mmdet.datasets.wrappers.multi_view_from_folder.multi_view_collate_flatten'
    ),
    dataset=dict(
        datasets=[
            dict(
                ann_file=
                '/home/coder/data/trong/KLTN/Soft_Teacher/data_drill/semi_anno_multiview/_annotations.coco.labeled.grouped@40.bright.json',
                backend_args=None,
                data_prefix=dict(img='train/'),
                data_root=
                '/home/coder/data/trong/KLTN/Soft_Teacher/data_drill/',
                filter_cfg=dict(filter_empty_gt=True, min_size=4),
                metainfo=dict(
                    classes=(
                        'Broken',
                        'Chipped',
                        'Scratched',
                        'Severe_Rust',
                        'Tip_Wear',
                    ),
                    palette=[
                        (
                            134,
                            34,
                            255,
                        ),
                        (
                            0,
                            255,
                            206,
                        ),
                        (
                            255,
                            128,
                            0,
                        ),
                        (
                            254,
                            0,
                            86,
                        ),
                        (
                            199,
                            252,
                            0,
                        ),
                    ]),
                pipeline=[
                    dict(
                        backend_args=None,
                        imdecode_backend='cv2',
                        to_float32=False,
                        type='LoadImageFromFile'),
                    dict(type='LoadAnnotations', with_bbox=True),
                    dict(keep_ratio=True, scale=(
                        256,
                        720,
                    ), type='Resize'),
                    dict(prob=0.5, type='RandomFlip'),
                    dict(
                        aug_num=1,
                        aug_space=[
                            [
                                dict(prob=0.5, type='AutoContrast'),
                            ],
                            [
                                dict(prob=0.4, type='Equalize'),
                            ],
                            [
                                dict(
                                    level=7,
                                    max_mag=1.25,
                                    min_mag=0.8,
                                    prob=0.6,
                                    type='Brightness'),
                            ],
                            [
                                dict(
                                    level=7,
                                    max_mag=1.3,
                                    min_mag=0.8,
                                    prob=0.6,
                                    type='Contrast'),
                            ],
                            [
                                dict(
                                    level=6,
                                    max_mag=1.5,
                                    min_mag=0.8,
                                    prob=0.4,
                                    type='Sharpness'),
                            ],
                            [
                                dict(
                                    level=5,
                                    max_mag=7.0,
                                    min_mag=5.0,
                                    prob=0.2,
                                    type='Posterize'),
                            ],
                        ],
                        type='RandAugment'),
                    dict(min_gt_bbox_wh=(
                        4,
                        4,
                    ), type='FilterAnnotations'),
                    dict(
                        branch_field=[
                            'sup',
                            'unsup_teacher',
                            'unsup_student',
                        ],
                        sup=dict(type='PackDetInputs'),
                        type='MultiBranch'),
                ],
                type='MultiViewFromFolder',
                views_per_sample=8),
            dict(
                ann_file=
                '/home/coder/data/trong/KLTN/Soft_Teacher/data_drill/semi_anno_multiview/_annotations.coco.unlabeled.grouped@40.bright.json',
                backend_args=None,
                data_prefix=dict(img='train/'),
                data_root=
                '/home/coder/data/trong/KLTN/Soft_Teacher/data_drill/',
                filter_cfg=dict(filter_empty_gt=False, min_size=4),
                metainfo=dict(
                    classes=(
                        'Broken',
                        'Chipped',
                        'Scratched',
                        'Severe_Rust',
                        'Tip_Wear',
                    ),
                    palette=[
                        (
                            134,
                            34,
                            255,
                        ),
                        (
                            0,
                            255,
                            206,
                        ),
                        (
                            255,
                            128,
                            0,
                        ),
                        (
                            254,
                            0,
                            86,
                        ),
                        (
                            199,
                            252,
                            0,
                        ),
                    ]),
                pipeline=[
                    dict(
                        backend_args=None,
                        imdecode_backend='cv2',
                        to_float32=False,
                        type='LoadImageFromFile'),
                    dict(type='LoadEmptyAnnotations'),
                    dict(
                        branch_field=[
                            'sup',
                            'unsup_teacher',
                            'unsup_student',
                        ],
                        type='MultiBranch',
                        unsup_student=[
                            dict(
                                keep_ratio=True,
                                scale=(
                                    256,
                                    720,
                                ),
                                type='Resize'),
                            dict(prob=0.5, type='RandomFlip'),
                            dict(
                                transforms=[
                                    dict(
                                        aug_num=2,
                                        aug_space=[
                                            [
                                                dict(
                                                    prob=0.5,
                                                    type='AutoContrast'),
                                            ],
                                            [
                                                dict(
                                                    prob=0.4, type='Equalize'),
                                            ],
                                            [
                                                dict(
                                                    level=7,
                                                    max_mag=1.25,
                                                    min_mag=0.8,
                                                    prob=0.6,
                                                    type='Brightness'),
                                            ],
                                            [
                                                dict(
                                                    level=7,
                                                    max_mag=1.3,
                                                    min_mag=0.8,
                                                    prob=0.6,
                                                    type='Contrast'),
                                            ],
                                            [
                                                dict(
                                                    level=6,
                                                    max_mag=1.5,
                                                    min_mag=0.8,
                                                    prob=0.4,
                                                    type='Sharpness'),
                                            ],
                                            [
                                                dict(
                                                    level=5,
                                                    max_mag=7.0,
                                                    min_mag=5.0,
                                                    prob=0.2,
                                                    type='Posterize'),
                                            ],
                                        ],
                                        type='RandAugment'),
                                    dict(
                                        aug_num=1,
                                        aug_space=[
                                            [
                                                dict(
                                                    level=7,
                                                    max_mag=0.05,
                                                    min_mag=0.0,
                                                    prob=0.3,
                                                    type='TranslateX'),
                                            ],
                                            [
                                                dict(
                                                    level=7,
                                                    max_mag=0.05,
                                                    min_mag=0.0,
                                                    prob=0.3,
                                                    type='TranslateY'),
                                            ],
                                        ],
                                        type='RandAugment'),
                                ],
                                type='RandomOrder'),
                            dict(
                                n_patches=(
                                    1,
                                    3,
                                ),
                                ratio=(
                                    0,
                                    0.15,
                                ),
                                type='RandomErasing'),
                            dict(
                                min_gt_bbox_wh=(
                                    4,
                                    4,
                                ),
                                type='FilterAnnotations'),
                            dict(
                                meta_keys=(
                                    'img_id',
                                    'img_path',
                                    'ori_shape',
                                    'img_shape',
                                    'scale_factor',
                                    'flip',
                                    'flip_direction',
                                    'homography_matrix',
                                ),
                                type='PackDetInputs'),
                        ],
                        unsup_teacher=[
                            dict(
                                keep_ratio=True,
                                scale=(
                                    256,
                                    720,
                                ),
                                type='Resize'),
                            dict(prob=0.5, type='RandomFlip'),
                            dict(
                                meta_keys=(
                                    'img_id',
                                    'img_path',
                                    'ori_shape',
                                    'img_shape',
                                    'scale_factor',
                                    'flip',
                                    'flip_direction',
                                    'homography_matrix',
                                ),
                                type='PackDetInputs'),
                        ]),
                ],
                type='MultiViewFromFolder',
                views_per_sample=8),
        ],
        type='ConcatDataset'),
    num_workers=4,
    persistent_workers=True,
    pin_memory=True,
    prefetch_factor=2,
    sampler=dict(
        batch_size=2, source_ratio=[
            1,
            1,
        ], type='GroupMultiSourceSampler'))
unlabeled_dataset = dict(
    ann_file=
    '/home/coder/data/trong/KLTN/Soft_Teacher/data_drill/semi_anno_multiview/_annotations.coco.unlabeled.grouped@40.bright.json',
    backend_args=None,
    data_prefix=dict(img='train/'),
    data_root='/home/coder/data/trong/KLTN/Soft_Teacher/data_drill/',
    filter_cfg=dict(filter_empty_gt=False, min_size=4),
    metainfo=dict(
        classes=(
            'Broken',
            'Chipped',
            'Scratched',
            'Severe_Rust',
            'Tip_Wear',
        ),
        palette=[
            (
                134,
                34,
                255,
            ),
            (
                0,
                255,
                206,
            ),
            (
                255,
                128,
                0,
            ),
            (
                254,
                0,
                86,
            ),
            (
                199,
                252,
                0,
            ),
        ]),
    pipeline=[
        dict(
            backend_args=None,
            imdecode_backend='cv2',
            to_float32=False,
            type='LoadImageFromFile'),
        dict(type='LoadEmptyAnnotations'),
        dict(
            branch_field=[
                'sup',
                'unsup_teacher',
                'unsup_student',
            ],
            type='MultiBranch',
            unsup_student=[
                dict(keep_ratio=True, scale=(
                    256,
                    720,
                ), type='Resize'),
                dict(prob=0.5, type='RandomFlip'),
                dict(
                    transforms=[
                        dict(
                            aug_num=2,
                            aug_space=[
                                [
                                    dict(prob=0.5, type='AutoContrast'),
                                ],
                                [
                                    dict(prob=0.4, type='Equalize'),
                                ],
                                [
                                    dict(
                                        level=7,
                                        max_mag=1.25,
                                        min_mag=0.8,
                                        prob=0.6,
                                        type='Brightness'),
                                ],
                                [
                                    dict(
                                        level=7,
                                        max_mag=1.3,
                                        min_mag=0.8,
                                        prob=0.6,
                                        type='Contrast'),
                                ],
                                [
                                    dict(
                                        level=6,
                                        max_mag=1.5,
                                        min_mag=0.8,
                                        prob=0.4,
                                        type='Sharpness'),
                                ],
                                [
                                    dict(
                                        level=5,
                                        max_mag=7.0,
                                        min_mag=5.0,
                                        prob=0.2,
                                        type='Posterize'),
                                ],
                            ],
                            type='RandAugment'),
                        dict(
                            aug_num=1,
                            aug_space=[
                                [
                                    dict(
                                        level=7,
                                        max_mag=0.05,
                                        min_mag=0.0,
                                        prob=0.3,
                                        type='TranslateX'),
                                ],
                                [
                                    dict(
                                        level=7,
                                        max_mag=0.05,
                                        min_mag=0.0,
                                        prob=0.3,
                                        type='TranslateY'),
                                ],
                            ],
                            type='RandAugment'),
                    ],
                    type='RandomOrder'),
                dict(
                    n_patches=(
                        1,
                        3,
                    ),
                    ratio=(
                        0,
                        0.15,
                    ),
                    type='RandomErasing'),
                dict(min_gt_bbox_wh=(
                    4,
                    4,
                ), type='FilterAnnotations'),
                dict(
                    meta_keys=(
                        'img_id',
                        'img_path',
                        'ori_shape',
                        'img_shape',
                        'scale_factor',
                        'flip',
                        'flip_direction',
                        'homography_matrix',
                    ),
                    type='PackDetInputs'),
            ],
            unsup_teacher=[
                dict(keep_ratio=True, scale=(
                    256,
                    720,
                ), type='Resize'),
                dict(prob=0.5, type='RandomFlip'),
                dict(
                    meta_keys=(
                        'img_id',
                        'img_path',
                        'ori_shape',
                        'img_shape',
                        'scale_factor',
                        'flip',
                        'flip_direction',
                        'homography_matrix',
                    ),
                    type='PackDetInputs'),
            ]),
    ],
    type='MultiViewFromFolder',
    views_per_sample=8)
unsup_pipeline = [
    dict(
        backend_args=None,
        imdecode_backend='cv2',
        to_float32=False,
        type='LoadImageFromFile'),
    dict(type='LoadEmptyAnnotations'),
    dict(
        branch_field=[
            'sup',
            'unsup_teacher',
            'unsup_student',
        ],
        type='MultiBranch',
        unsup_student=[
            dict(keep_ratio=True, scale=(
                256,
                720,
            ), type='Resize'),
            dict(prob=0.5, type='RandomFlip'),
            dict(
                transforms=[
                    dict(
                        aug_num=2,
                        aug_space=[
                            [
                                dict(prob=0.5, type='AutoContrast'),
                            ],
                            [
                                dict(prob=0.4, type='Equalize'),
                            ],
                            [
                                dict(
                                    level=7,
                                    max_mag=1.25,
                                    min_mag=0.8,
                                    prob=0.6,
                                    type='Brightness'),
                            ],
                            [
                                dict(
                                    level=7,
                                    max_mag=1.3,
                                    min_mag=0.8,
                                    prob=0.6,
                                    type='Contrast'),
                            ],
                            [
                                dict(
                                    level=6,
                                    max_mag=1.5,
                                    min_mag=0.8,
                                    prob=0.4,
                                    type='Sharpness'),
                            ],
                            [
                                dict(
                                    level=5,
                                    max_mag=7.0,
                                    min_mag=5.0,
                                    prob=0.2,
                                    type='Posterize'),
                            ],
                        ],
                        type='RandAugment'),
                    dict(
                        aug_num=1,
                        aug_space=[
                            [
                                dict(
                                    level=7,
                                    max_mag=0.05,
                                    min_mag=0.0,
                                    prob=0.3,
                                    type='TranslateX'),
                            ],
                            [
                                dict(
                                    level=7,
                                    max_mag=0.05,
                                    min_mag=0.0,
                                    prob=0.3,
                                    type='TranslateY'),
                            ],
                        ],
                        type='RandAugment'),
                ],
                type='RandomOrder'),
            dict(n_patches=(
                1,
                3,
            ), ratio=(
                0,
                0.15,
            ), type='RandomErasing'),
            dict(min_gt_bbox_wh=(
                4,
                4,
            ), type='FilterAnnotations'),
            dict(
                meta_keys=(
                    'img_id',
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                    'flip',
                    'flip_direction',
                    'homography_matrix',
                ),
                type='PackDetInputs'),
        ],
        unsup_teacher=[
            dict(keep_ratio=True, scale=(
                256,
                720,
            ), type='Resize'),
            dict(prob=0.5, type='RandomFlip'),
            dict(
                meta_keys=(
                    'img_id',
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                    'flip',
                    'flip_direction',
                    'homography_matrix',
                ),
                type='PackDetInputs'),
        ]),
]
val_cfg = dict(type='TeacherStudentValLoop')
val_dataloader = dict(
    batch_size=1,
    collate_fn=dict(
        type=
        'mmdet.datasets.wrappers.multi_view_from_folder.multi_view_collate_val'
    ),
    dataset=dict(
        ann_file=
        '/home/coder/data/trong/KLTN/Soft_Teacher/data_drill/anno_valid/_annotations_filtered.bright.coco.json',
        backend_args=None,
        data_prefix=dict(img='valid/'),
        data_root='/home/coder/data/trong/KLTN/Soft_Teacher/data_drill/',
        metainfo=dict(
            classes=(
                'Broken',
                'Chipped',
                'Scratched',
                'Severe_Rust',
                'Tip_Wear',
            ),
            palette=[
                (
                    134,
                    34,
                    255,
                ),
                (
                    0,
                    255,
                    206,
                ),
                (
                    255,
                    128,
                    0,
                ),
                (
                    254,
                    0,
                    86,
                ),
                (
                    199,
                    252,
                    0,
                ),
            ]),
        pipeline=[
            dict(
                backend_args=None,
                imdecode_backend='cv2',
                to_float32=False,
                type='LoadImageFromFile'),
            dict(keep_ratio=True, scale=(
                256,
                720,
            ), type='Resize'),
            dict(
                meta_keys=(
                    'img_id',
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                ),
                type='PackDetInputs'),
        ],
        test_mode=True,
        type='MultiViewFromFolder',
        views_per_sample=8),
    drop_last=False,
    num_workers=2,
    persistent_workers=True,
    pin_memory=True,
    sampler=dict(shuffle=False, type='DefaultSampler'))
val_evaluator = dict(
    aggregation='none',
    ann_file=
    '/home/coder/data/trong/KLTN/Soft_Teacher/data_drill/anno_valid/_annotations_filtered.bright.coco.json',
    backend_args=None,
    classwise=True,
    enable_aggregation=False,
    extract_base_name=True,
    format_only=False,
    metric='bbox',
    nms_iou_thr=0.5,
    type='MultiViewCocoMetric',
    views_per_sample=8)
views_per_sample = 8
vis_backends = [
    dict(type='LocalVisBackend'),
]
visualizer = dict(
    name='visualizer',
    type='DetLocalVisualizer',
    vis_backends=[
        dict(type='LocalVisBackend'),
    ])
weak_pipeline = [
    dict(keep_ratio=True, scale=(
        256,
        720,
    ), type='Resize'),
    dict(prob=0.5, type='RandomFlip'),
    dict(
        meta_keys=(
            'img_id',
            'img_path',
            'ori_shape',
            'img_shape',
            'scale_factor',
            'flip',
            'flip_direction',
            'homography_matrix',
        ),
        type='PackDetInputs'),
]
work_dir = './work_dirs/soft_teacher_custom_multi_view'

2025/12/09 18:36:47 - mmengine - INFO - MultiViewBackbone: Initialized with MVViT fusion, views_per_sample=8
2025/12/09 18:36:48 - mmengine - INFO - MultiViewBackbone: Initialized with MVViT fusion, views_per_sample=8
2025/12/09 18:36:50 - mmengine - INFO - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.
2025/12/09 18:36:50 - mmengine - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) RuntimeInfoHook                    
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
before_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) MeanTeacherHook                    
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DistSamplerSeedHook                
 -------------------- 
before_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) MeanTeacherHook                    
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) IterTimerHook                      
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_val_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_val_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_val_iter:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DetVisualizationHook               
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_val_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_test_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_test_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_test_iter:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DetVisualizationHook               
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_run:
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
2025/12/09 18:36:53 - mmengine - INFO - [COCO ANNO] Loaded from: /home/coder/data/trong/KLTN/Soft_Teacher/data_drill/semi_anno_multiview/_annotations.coco.labeled.grouped@40.bright.json
2025/12/09 18:36:53 - mmengine - INFO -    1208 images, 1448 annotations
2025/12/09 18:36:53 - mmengine - INFO -    946 files have annotations
2025/12/09 18:36:53 - mmengine - INFO -    1208 files have base_img_id
2025/12/09 18:36:53 - mmengine - INFO -    Categories: [0, 1, 2, 3, 4, 5]
2025/12/09 18:36:53 - mmengine - INFO -    cat2label mapping: {1: 0, 2: 1, 3: 2, 4: 3, 5: 4}
2025/12/09 18:36:53 - mmengine - INFO -    File 'S136_Image__2025-09-23__16-42-55_bright_4_crop_8_jpg.rf.8d03a89b2a2643c02b4733eb858e68be.jpg': 1 boxes, base_img_id=S136_bright_4, crop_num=8
2025/12/09 18:36:53 - mmengine - INFO -    File 'S40_Image__2025-09-04__16-02-50_bright_2_crop_6_jpg.rf.0006368956fc4380b51ac33ff3fbee7c.jpg': 1 boxes, base_img_id=S40_bright_2, crop_num=6
2025/12/09 18:36:53 - mmengine - INFO -    File 'S124_Image__2025-09-23__10-26-03_bright_3_crop_5_jpg.rf.eff0c11d31452dcd01ccb1959ba342f5.jpg': 2 boxes, base_img_id=S124_bright_3, crop_num=5
2025/12/09 18:36:53 - mmengine - INFO - [COCO] Built 151 groups from base_img_id
2025/12/09 18:36:53 - mmengine - INFO - [COCO] Total files in COCO JSON: 1208
2025/12/09 18:36:53 - mmengine - INFO - [COCO] Unique base_img_ids: 151
2025/12/09 18:36:53 - mmengine - INFO -    Group 0: base_img_id=S101_bright_2, 8/8 views
2025/12/09 18:36:53 - mmengine - INFO -    Group 1: base_img_id=S111_bright_3, 8/8 views
2025/12/09 18:36:53 - mmengine - INFO -    Group 2: base_img_id=S111_bright_4, 8/8 views
2025/12/09 18:36:53 - mmengine - INFO - [COCO ANNO] Loaded from: /home/coder/data/trong/KLTN/Soft_Teacher/data_drill/semi_anno_multiview/_annotations.coco.unlabeled.grouped@40.bright.json
2025/12/09 18:36:53 - mmengine - INFO -    784 images, 956 annotations
2025/12/09 18:36:53 - mmengine - INFO -    649 files have annotations
2025/12/09 18:36:53 - mmengine - INFO -    784 files have base_img_id
2025/12/09 18:36:53 - mmengine - INFO -    Categories: [0, 1, 2, 3, 4, 5]
2025/12/09 18:36:53 - mmengine - INFO -    cat2label mapping: {1: 0, 2: 1, 3: 2, 4: 3, 5: 4}
2025/12/09 18:36:53 - mmengine - INFO -    File 'S4_than_Image__2025-09-03__14-00-16_bright_2_crop_3_jpg.rf.93c6a9e3d563bf395cd5c9ef7519b742.jpg': 3 boxes, base_img_id=S4_bright_2, crop_num=3
2025/12/09 18:36:53 - mmengine - INFO -    File 'S225_Image__2025-07-01__14-30-51_bright_2_crop_7_jpg.rf.6b508c0dd7ed86aff7e31273da30c5e4.jpg': 2 boxes, base_img_id=S225_bright_2, crop_num=7
2025/12/09 18:36:53 - mmengine - INFO -    File 'S78_Image__2025-09-08__16-35-36_bright_3_crop_8_jpg.rf.84d34f89fe2af65f5adacb8437ed2ee6.jpg': 3 boxes, base_img_id=S78_bright_3, crop_num=8
2025/12/09 18:36:53 - mmengine - INFO - [COCO] Built 98 groups from base_img_id
2025/12/09 18:36:53 - mmengine - INFO - [COCO] Total files in COCO JSON: 784
2025/12/09 18:36:53 - mmengine - INFO - [COCO] Unique base_img_ids: 98
2025/12/09 18:36:53 - mmengine - INFO -    Group 0: base_img_id=S111_bright_2, 8/8 views
2025/12/09 18:36:53 - mmengine - INFO -    Group 1: base_img_id=S114_bright_2, 8/8 views
2025/12/09 18:36:53 - mmengine - INFO -    Group 2: base_img_id=S117_bright_2, 8/8 views
2025/12/09 18:36:53 - mmengine - INFO - [COCO ANNO] Loaded from: /home/coder/data/trong/KLTN/Soft_Teacher/data_drill/anno_valid/_annotations_filtered.bright.coco.json
2025/12/09 18:36:53 - mmengine - INFO -    320 images, 442 annotations
2025/12/09 18:36:53 - mmengine - INFO -    284 files have annotations
2025/12/09 18:36:53 - mmengine - INFO -    320 files have base_img_id
2025/12/09 18:36:53 - mmengine - INFO -    Categories: [0, 1, 2, 3, 4, 5]
2025/12/09 18:36:53 - mmengine - INFO -    cat2label mapping: {1: 0, 2: 1, 3: 2, 4: 3, 5: 4}
2025/12/09 18:36:53 - mmengine - INFO -    File 'S245_Image__2025-11-11__12-09-08_bright_2_crop_5_jpg.rf.b0924b8796050c8c09b6c63180a8df2a.jpg': 1 boxes, base_img_id=S245_bright_2, crop_num=5
2025/12/09 18:36:53 - mmengine - INFO -    File 'S135_Image__2025-09-23__16-23-02_bright_4_crop_7_jpg.rf.1b8c818ab9826dbcd27a64c2e38414e1.jpg': 2 boxes, base_img_id=S135_bright_4, crop_num=7
2025/12/09 18:36:53 - mmengine - INFO -    File 'S242_Image__2025-11-11__11-56-30_bright_8_crop_1_jpg.rf.06e5607648d1e41e35c7bec4fc2eeb57.jpg': 1 boxes, base_img_id=S242_bright_8, crop_num=1
2025/12/09 18:36:53 - mmengine - INFO - [COCO] Built 40 groups from base_img_id
2025/12/09 18:36:53 - mmengine - INFO - [COCO] Total files in COCO JSON: 320
2025/12/09 18:36:53 - mmengine - INFO - [COCO] Unique base_img_ids: 40
2025/12/09 18:36:53 - mmengine - INFO -    Group 0: base_img_id=S110_bright_2, 8/8 views
2025/12/09 18:36:53 - mmengine - INFO -    Group 1: base_img_id=S110_bright_3, 8/8 views
2025/12/09 18:36:53 - mmengine - INFO -    Group 2: base_img_id=S110_bright_4, 8/8 views
2025/12/09 18:36:53 - mmengine - INFO - [BBOX DEBUG 1] File: S69_Image__2025-09-08__11-24-36_bright_3_crop_1_jpg.rf.d38ad4d690c0dbe1d7a0a1ac76d8044d.jpg, Image shape: 720x256
2025/12/09 18:36:53 - mmengine - INFO -    Loaded 1 boxes from COCO JSON
2025/12/09 18:36:53 - mmengine - INFO -     Box 0: [179, 490, 245.52100000000002, 653.754], label: 3
2025/12/09 18:36:53 - mmengine - INFO - MultiViewCocoMetric initialized: PER-CROP evaluation mode (no aggregation, 8 crops evaluated independently)
2025/12/09 18:36:53 - mmengine - INFO - [BBOX DEBUG 2] File: S69_Image__2025-09-08__11-24-36_bright_3_crop_2_jpg.rf.898173db23e047c0aed692b28d389f41.jpg, Image shape: 720x256
2025/12/09 18:36:53 - mmengine - INFO -    Loaded 2 boxes from COCO JSON
2025/12/09 18:36:53 - mmengine - INFO -     Box 0: [151, 131, 199.44299999999998, 301.966], label: 2
2025/12/09 18:36:53 - mmengine - INFO -     Box 1: [107, 445, 184.832, 623.309], label: 3
2025/12/09 18:36:53 - mmengine - INFO - [BBOX DEBUG 1] File: S198_Image__2025-05-30__10-51-42_bright_2_crop_2_jpg.rf.59859817947ca368f60b43279e73d25e.jpg, Image shape: 720x256
2025/12/09 18:36:53 - mmengine - INFO -    Loaded 1 boxes from COCO JSON
2025/12/09 18:36:53 - mmengine - INFO -     Box 0: [166, 401, 228.66, 476.949], label: 1
2025/12/09 18:36:53 - mmengine - INFO - [BBOX DEBUG 2] File: S198_Image__2025-05-30__10-51-42_bright_2_crop_3_jpg.rf.b007782e88ba2f464e10e26e27522d52.jpg, Image shape: 720x256
2025/12/09 18:36:53 - mmengine - INFO -    Loaded 3 boxes from COCO JSON
2025/12/09 18:36:53 - mmengine - INFO -     Box 0: [194, 406, 244.45600000000002, 474.471], label: 1
2025/12/09 18:36:53 - mmengine - INFO -     Box 1: [94, 411, 172.192, 465.745], label: 1
2025/12/09 18:36:53 - mmengine - INFO -     Box 2: [154, 464, 201.8, 585.729], label: 2
2025/12/09 18:36:53 - mmengine - INFO - [BBOX DEBUG 3] File: S69_Image__2025-09-08__11-24-36_bright_3_crop_3_jpg.rf.4890334e991921ea8857d39f7cf43385.jpg, Image shape: 720x256
2025/12/09 18:36:53 - mmengine - INFO -    Loaded 1 boxes from COCO JSON
2025/12/09 18:36:53 - mmengine - INFO -     Box 0: [66, 107, 143.406, 314.3], label: 2
2025/12/09 18:36:53 - mmengine - INFO - [BBOX DEBUG 1] File: S124_Image__2025-09-23__10-18-00_bright_2_crop_1_jpg.rf.22f84fcef43a3965e66daac22143c8b6.jpg, Image shape: 720x256
2025/12/09 18:36:53 - mmengine - INFO -    Loaded 1 boxes from COCO JSON
2025/12/09 18:36:53 - mmengine - INFO -     Box 0: [30, 228, 224.023, 302.243], label: 0
2025/12/09 18:36:53 - mmengine - INFO - [BBOX DEBUG 1] File: S54_Image__2025-09-05__13-45-20_bright_2_crop_1_jpg.rf.24ae02b06c093631937c21c864ca6652.jpg, Image shape: 720x256
2025/12/09 18:36:53 - mmengine - INFO -    Loaded 1 boxes from COCO JSON
2025/12/09 18:36:53 - mmengine - INFO -     Box 0: [57, 140, 225.12, 642.47], label: 3
2025/12/09 18:36:53 - mmengine - INFO - [BBOX DEBUG 2] File: S124_Image__2025-09-23__10-18-00_bright_2_crop_2_jpg.rf.400157c0e92243e4752bb548ace149ce.jpg, Image shape: 720x256
2025/12/09 18:36:53 - mmengine - INFO -    Loaded 1 boxes from COCO JSON
2025/12/09 18:36:53 - mmengine - INFO -     Box 0: [51, 190, 218.03, 260.21], label: 0
2025/12/09 18:36:53 - mmengine - INFO - [BBOX DEBUG 3] File: S124_Image__2025-09-23__10-18-00_bright_2_crop_3_jpg.rf.791ca94de522157b8b9caaef3c58caaf.jpg, Image shape: 720x256
2025/12/09 18:36:53 - mmengine - INFO -    Loaded 1 boxes from COCO JSON
2025/12/09 18:36:53 - mmengine - INFO -     Box 0: [24, 190, 244.826, 254.49], label: 0
2025/12/09 18:36:53 - mmengine - INFO - [BBOX DEBUG 1] File: S225_Image__2025-07-01__14-30-51_bright_2_crop_1_jpg.rf.a1ababafc899c49acc50e72f46512c84.jpg, Image shape: 720x256
2025/12/09 18:36:53 - mmengine - INFO -    Loaded 1 boxes from COCO JSON
2025/12/09 18:36:53 - mmengine - INFO -     Box 0: [39, 375, 121.536, 574.107], label: 1
2025/12/09 18:36:53 - mmengine - INFO - [BBOX DEBUG 3] File: S198_Image__2025-05-30__10-51-42_bright_2_crop_4_jpg.rf.c4f188f6dbeb8faa2a2ef1c6b0d47f2a.jpg, Image shape: 720x256
2025/12/09 18:36:53 - mmengine - INFO -    Loaded 3 boxes from COCO JSON
2025/12/09 18:36:53 - mmengine - INFO -     Box 0: [155, 417, 214.36, 499.64], label: 1
2025/12/09 18:36:53 - mmengine - INFO -     Box 1: [30, 434, 94.31, 493.65], label: 1
2025/12/09 18:36:53 - mmengine - INFO -     Box 2: [55, 496, 122.34, 621.28], label: 2
2025/12/09 18:36:53 - mmengine - INFO - [BBOX DEBUG 2] File: S225_Image__2025-07-01__14-30-51_bright_2_crop_2_jpg.rf.dabe237fc39c89fd2f74b70107dcb052.jpg, Image shape: 720x256
2025/12/09 18:36:53 - mmengine - INFO -    Loaded 3 boxes from COCO JSON
2025/12/09 18:36:53 - mmengine - INFO -     Box 0: [137, 464, 197.15, 575.58], label: 2
2025/12/09 18:36:53 - mmengine - INFO -     Box 1: [15, 352, 49.14, 509.25], label: 1
2025/12/09 18:36:53 - mmengine - INFO -     Box 2: [120, 614, 162.94, 706.05], label: 2
2025/12/09 18:36:53 - mmengine - INFO - [BBOX DEBUG 3] File: S225_Image__2025-07-01__14-30-51_bright_2_crop_3_jpg.rf.94edda4a20eb8a5730d3dc7ebf24faed.jpg, Image shape: 720x256
2025/12/09 18:36:53 - mmengine - INFO -    Loaded 2 boxes from COCO JSON
2025/12/09 18:36:53 - mmengine - INFO -     Box 0: [88, 453, 123.97, 549.38], label: 2
2025/12/09 18:36:53 - mmengine - INFO -     Box 1: [67, 610, 107.15, 680.26], label: 2
2025/12/09 18:36:53 - mmengine - INFO - [BBOX DEBUG 2] File: S54_Image__2025-09-05__13-45-20_bright_2_crop_2_jpg.rf.8a7876cbc4ff505d5f756d6a898b496a.jpg, Image shape: 720x256
2025/12/09 18:36:53 - mmengine - INFO -    Loaded 1 boxes from COCO JSON
2025/12/09 18:36:53 - mmengine - INFO -     Box 0: [49, 278, 237.06, 679.19], label: 3
2025/12/09 18:36:53 - mmengine - INFO - [BBOX DEBUG 1] File: S161_Image__2025-09-23__09-54-50_bright_2_crop_1_jpg.rf.aebdad729797e8a49336f2557b5bfac3.jpg, Image shape: 720x256
2025/12/09 18:36:53 - mmengine - INFO -    Loaded 1 boxes from COCO JSON
2025/12/09 18:36:53 - mmengine - INFO -     Box 0: [49, 274, 254.571, 449.058], label: 0
2025/12/09 18:36:53 - mmengine - INFO - [BBOX DEBUG 3] File: S54_Image__2025-09-05__13-45-20_bright_2_crop_3_jpg.rf.e6ab2df3816e5581745a3f784c5ae4b3.jpg, Image shape: 720x256
2025/12/09 18:36:53 - mmengine - INFO -    Loaded 1 boxes from COCO JSON
2025/12/09 18:36:53 - mmengine - INFO -     Box 0: [184, 69, 253.81, 320.82], label: 1
2025/12/09 18:36:53 - mmengine - INFO - [BBOX DEBUG 2] File: S161_Image__2025-09-23__09-54-50_bright_2_crop_2_jpg.rf.4ef014623afe6c6cf0f65a26023da8de.jpg, Image shape: 720x256
2025/12/09 18:36:53 - mmengine - INFO -    Loaded 1 boxes from COCO JSON
2025/12/09 18:36:53 - mmengine - INFO -     Box 0: [40, 214, 254.911, 408.576], label: 0
2025/12/09 18:36:53 - mmengine - INFO - [BBOX DEBUG 3] File: S161_Image__2025-09-23__09-54-50_bright_2_crop_3_jpg.rf.ea8e176ed5f7076dc273bd151b2a3cec.jpg, Image shape: 720x256
2025/12/09 18:36:53 - mmengine - INFO -    Loaded 1 boxes from COCO JSON
2025/12/09 18:36:53 - mmengine - INFO -     Box 0: [7, 202, 241.144, 386.103], label: 0
2025/12/09 18:36:53 - mmengine - INFO - [BBOX DEBUG 1] File: S242_Image__2025-11-11__11-52-52_bright_6_crop_1_jpg.rf.0c26109ccf0e2cc29fe2773c927e5c66.jpg, Image shape: 720x256
2025/12/09 18:36:53 - mmengine - INFO -    Loaded 1 boxes from COCO JSON
2025/12/09 18:36:53 - mmengine - INFO -     Box 0: [40, 13, 181, 353], label: 3
2025/12/09 18:36:53 - mmengine - INFO - [BBOX DEBUG 2] File: S242_Image__2025-11-11__11-52-52_bright_6_crop_2_jpg.rf.e9218a7d74d4717a4369e85ca1fe4369.jpg, Image shape: 720x256
2025/12/09 18:36:53 - mmengine - INFO -    Loaded 3 boxes from COCO JSON
2025/12/09 18:36:53 - mmengine - INFO -     Box 0: [40, 9, 108, 454], label: 3
2025/12/09 18:36:53 - mmengine - INFO -     Box 1: [179, 526, 218, 606], label: 1
2025/12/09 18:36:53 - mmengine - INFO -     Box 2: [89, 600, 208, 688], label: 3
2025/12/09 18:36:53 - mmengine - INFO - [BBOX DEBUG 1] File: S167_Image__2025-09-22__11-17-44_bright_3_crop_1_jpg.rf.16cf2cfbcf8a381c19acda298c10f36b.jpg, Image shape: 720x256
2025/12/09 18:36:53 - mmengine - INFO -    Loaded 1 boxes from COCO JSON
2025/12/09 18:36:53 - mmengine - INFO -     Box 0: [87, 237, 209.702, 486.473], label: 2
2025/12/09 18:36:53 - mmengine - INFO - [BBOX DEBUG 3] File: S242_Image__2025-11-11__11-52-52_bright_6_crop_3_jpg.rf.f9b208c4fe47ff924f5ef089c4ec8462.jpg, Image shape: 720x256
2025/12/09 18:36:53 - mmengine - INFO -    Loaded 5 boxes from COCO JSON
2025/12/09 18:36:53 - mmengine - INFO -     Box 0: [19, 11, 95, 311], label: 3
2025/12/09 18:36:53 - mmengine - INFO -     Box 1: [151, 0, 244, 149], label: 3
2025/12/09 18:36:53 - mmengine - INFO -     Box 2: [196, 234, 248, 304], label: 3
2025/12/09 18:36:53 - mmengine - INFO - [BBOX DEBUG 2] File: S167_Image__2025-09-22__11-17-44_bright_3_crop_2_jpg.rf.a62800d9167d460e4c8438ade5caa629.jpg, Image shape: 720x256
2025/12/09 18:36:53 - mmengine - INFO -    Loaded 1 boxes from COCO JSON
2025/12/09 18:36:53 - mmengine - INFO -     Box 0: [39, 186, 155.447, 438.472], label: 2
2025/12/09 18:36:53 - mmengine - INFO - [BBOX DEBUG 3] File: S167_Image__2025-09-22__11-17-44_bright_3_crop_3_jpg.rf.357269a22e445a22752f9988c2203913.jpg, Image shape: 720x256
2025/12/09 18:36:53 - mmengine - INFO -    Loaded 1 boxes from COCO JSON
2025/12/09 18:36:53 - mmengine - INFO -     Box 0: [195, 100, 236.68200000000002, 315.696], label: 2
Name of parameter - Initialization information

student.backbone.backbone.conv1.weight - torch.Size([64, 3, 7, 7]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.0.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.0.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.0.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.0.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.0.bn3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.0.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.0.downsample.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.0.downsample.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.1.bn3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.1.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.2.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.2.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.2.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.2.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.2.bn3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.2.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.0.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.0.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.0.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.0.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.0.bn3.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.0.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.0.downsample.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.0.downsample.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.1.bn3.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.1.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.2.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.2.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.2.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.2.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.2.bn3.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.2.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.3.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.3.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.3.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.3.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.3.bn3.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.3.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.0.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.0.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.0.downsample.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.0.downsample.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.1.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.1.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.2.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.2.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.2.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.2.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.2.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.2.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.3.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.3.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.3.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.3.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.3.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.3.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.4.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.4.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.4.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.4.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.4.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.4.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.5.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.5.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.5.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.5.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.5.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.5.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.0.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.0.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.0.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.0.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.0.bn3.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.0.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.0.downsample.1.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.0.downsample.1.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.1.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.1.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.1.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.1.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.1.bn3.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.1.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.2.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.2.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.2.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.2.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.2.bn3.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.2.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.residual_alpha - torch.Size([]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.layers.0.self_attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.layers.0.self_attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.layers.0.self_attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.layers.0.self_attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.layers.0.linear1.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.layers.0.linear1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.layers.0.linear2.weight - torch.Size([256, 512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.layers.0.linear2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.layers.0.norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.layers.0.norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.layers.0.norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.layers.0.norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.pooling_queries.1024 - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.pooling_queries.2048 - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.pooling_queries.256 - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.pooling_queries.512 - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.upsample_queries.128x128 - torch.Size([16384, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.upsample_queries.32x32 - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.upsample_queries.64x64 - torch.Size([4096, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.neck.lateral_convs.0.conv.weight - torch.Size([256, 256, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

student.neck.lateral_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.neck.lateral_convs.1.conv.weight - torch.Size([256, 512, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

student.neck.lateral_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.neck.lateral_convs.2.conv.weight - torch.Size([256, 1024, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

student.neck.lateral_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.neck.lateral_convs.3.conv.weight - torch.Size([256, 2048, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

student.neck.lateral_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

student.neck.fpn_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

student.neck.fpn_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

student.neck.fpn_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.neck.fpn_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

student.neck.fpn_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.rpn_head.rpn_conv.weight - torch.Size([256, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=0 

student.rpn_head.rpn_conv.bias - torch.Size([256]): 
NormalInit: mean=0, std=0.01, bias=0 

student.rpn_head.rpn_cls.weight - torch.Size([15, 256, 1, 1]): 
NormalInit: mean=0, std=0.01, bias=0 

student.rpn_head.rpn_cls.bias - torch.Size([15]): 
NormalInit: mean=0, std=0.01, bias=0 

student.rpn_head.rpn_reg.weight - torch.Size([60, 256, 1, 1]): 
NormalInit: mean=0, std=0.01, bias=0 

student.rpn_head.rpn_reg.bias - torch.Size([60]): 
NormalInit: mean=0, std=0.01, bias=0 

student.roi_head.bbox_head.fc_cls.weight - torch.Size([6, 1024]): 
NormalInit: mean=0, std=0.01, bias=0 

student.roi_head.bbox_head.fc_cls.bias - torch.Size([6]): 
NormalInit: mean=0, std=0.01, bias=0 

student.roi_head.bbox_head.fc_reg.weight - torch.Size([20, 1024]): 
NormalInit: mean=0, std=0.001, bias=0 

student.roi_head.bbox_head.fc_reg.bias - torch.Size([20]): 
NormalInit: mean=0, std=0.001, bias=0 

student.roi_head.bbox_head.shared_fcs.0.weight - torch.Size([1024, 12544]): 
XavierInit: gain=1, distribution=uniform, bias=0 

student.roi_head.bbox_head.shared_fcs.0.bias - torch.Size([1024]): 
XavierInit: gain=1, distribution=uniform, bias=0 

student.roi_head.bbox_head.shared_fcs.1.weight - torch.Size([1024, 1024]): 
XavierInit: gain=1, distribution=uniform, bias=0 

student.roi_head.bbox_head.shared_fcs.1.bias - torch.Size([1024]): 
XavierInit: gain=1, distribution=uniform, bias=0 

teacher.backbone.backbone.conv1.weight - torch.Size([64, 3, 7, 7]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.0.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.0.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.0.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.0.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.0.bn3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.0.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.0.downsample.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.0.downsample.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.1.bn3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.1.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.2.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.2.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.2.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.2.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.2.bn3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.2.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.0.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.0.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.0.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.0.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.0.bn3.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.0.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.0.downsample.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.0.downsample.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.1.bn3.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.1.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.2.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.2.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.2.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.2.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.2.bn3.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.2.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.3.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.3.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.3.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.3.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.3.bn3.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.3.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.0.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.0.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.0.downsample.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.0.downsample.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.1.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.1.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.2.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.2.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.2.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.2.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.2.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.2.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.3.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.3.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.3.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.3.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.3.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.3.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.4.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.4.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.4.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.4.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.4.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.4.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.5.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.5.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.5.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.5.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.5.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.5.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.0.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.0.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.0.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.0.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.0.bn3.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.0.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.0.downsample.1.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.0.downsample.1.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.1.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.1.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.1.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.1.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.1.bn3.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.1.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.2.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.2.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.2.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.2.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.2.bn3.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.2.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.residual_alpha - torch.Size([]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.layers.0.self_attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.layers.0.self_attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.layers.0.self_attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.layers.0.self_attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.layers.0.linear1.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.layers.0.linear1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.layers.0.linear2.weight - torch.Size([256, 512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.layers.0.linear2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.layers.0.norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.layers.0.norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.layers.0.norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.layers.0.norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.pooling_queries.1024 - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.pooling_queries.2048 - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.pooling_queries.256 - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.pooling_queries.512 - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.upsample_queries.128x128 - torch.Size([16384, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.upsample_queries.32x32 - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.upsample_queries.64x64 - torch.Size([4096, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.neck.lateral_convs.0.conv.weight - torch.Size([256, 256, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

teacher.neck.lateral_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.neck.lateral_convs.1.conv.weight - torch.Size([256, 512, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

teacher.neck.lateral_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.neck.lateral_convs.2.conv.weight - torch.Size([256, 1024, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

teacher.neck.lateral_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.neck.lateral_convs.3.conv.weight - torch.Size([256, 2048, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

teacher.neck.lateral_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

teacher.neck.fpn_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

teacher.neck.fpn_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

teacher.neck.fpn_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.neck.fpn_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

teacher.neck.fpn_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.rpn_head.rpn_conv.weight - torch.Size([256, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=0 

teacher.rpn_head.rpn_conv.bias - torch.Size([256]): 
NormalInit: mean=0, std=0.01, bias=0 

teacher.rpn_head.rpn_cls.weight - torch.Size([15, 256, 1, 1]): 
NormalInit: mean=0, std=0.01, bias=0 

teacher.rpn_head.rpn_cls.bias - torch.Size([15]): 
NormalInit: mean=0, std=0.01, bias=0 

teacher.rpn_head.rpn_reg.weight - torch.Size([60, 256, 1, 1]): 
NormalInit: mean=0, std=0.01, bias=0 

teacher.rpn_head.rpn_reg.bias - torch.Size([60]): 
NormalInit: mean=0, std=0.01, bias=0 

teacher.roi_head.bbox_head.fc_cls.weight - torch.Size([6, 1024]): 
NormalInit: mean=0, std=0.01, bias=0 

teacher.roi_head.bbox_head.fc_cls.bias - torch.Size([6]): 
NormalInit: mean=0, std=0.01, bias=0 

teacher.roi_head.bbox_head.fc_reg.weight - torch.Size([20, 1024]): 
NormalInit: mean=0, std=0.001, bias=0 

teacher.roi_head.bbox_head.fc_reg.bias - torch.Size([20]): 
NormalInit: mean=0, std=0.001, bias=0 

teacher.roi_head.bbox_head.shared_fcs.0.weight - torch.Size([1024, 12544]): 
XavierInit: gain=1, distribution=uniform, bias=0 

teacher.roi_head.bbox_head.shared_fcs.0.bias - torch.Size([1024]): 
XavierInit: gain=1, distribution=uniform, bias=0 

teacher.roi_head.bbox_head.shared_fcs.1.weight - torch.Size([1024, 1024]): 
XavierInit: gain=1, distribution=uniform, bias=0 

teacher.roi_head.bbox_head.shared_fcs.1.bias - torch.Size([1024]): 
XavierInit: gain=1, distribution=uniform, bias=0 
2025/12/09 18:36:57 - mmengine - WARNING - "FileClient" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io
2025/12/09 18:36:57 - mmengine - WARNING - "HardDiskBackend" is the alias of "LocalBackend" and the former will be deprecated in future.
2025/12/09 18:36:57 - mmengine - INFO - Checkpoints will be saved to /home/coder/data/trong/KLTN/Soft_Teacher/work_dirs/soft_teacher_custom_multi_view.
2025/12/09 18:36:57 - mmengine - INFO - [Supervised Step 0] Groups: 2 , G0:8box | G1:13box
2025/12/09 18:37:02 - mmengine - INFO - [Supervised Step 1] Groups: 2 , G0:16box | G1:20box
2025/12/09 18:37:05 - mmengine - INFO - [Supervised Step 2] Groups: 2 , G0:11box | G1:7box
2025/12/09 18:37:08 - mmengine - INFO - [Supervised Step 3] Groups: 2 , G0:8box | G1:8box
2025/12/09 18:37:11 - mmengine - INFO - [Supervised Step 4] Groups: 2 , G0:7box | G1:11box
2025/12/09 18:37:14 - mmengine - INFO - [Supervised Step 5] Groups: 2 , G0:14box | G1:11box
2025/12/09 18:37:18 - mmengine - INFO - [Supervised Step 6] Groups: 2 , G0:10box | G1:7box
2025/12/09 18:37:21 - mmengine - INFO - [Supervised Step 7] Groups: 2 , G0:15box | G1:8box
2025/12/09 18:37:24 - mmengine - INFO - [Supervised Step 8] Groups: 2 , G0:7box | G1:8box
2025/12/09 18:37:27 - mmengine - INFO - [Supervised Step 9] Groups: 2 , G0:11box | G1:16box
2025/12/09 18:39:36 - mmengine - INFO - Epoch(train) [1][ 50/125]  lr: 7.4257e-06  eta: 3 days, 16:43:41  time: 3.1958  data_time: 0.0142  memory: 25310  loss: 3.5453  sup_loss_rpn_cls: 0.2001  sup_loss_rpn_bbox: 0.8832  sup_loss_cls: 1.1736  sup_acc: 49.0826  sup_num_pred_fg: 52.0000  sup_num_pred_bg: 166.0000  sup_num_gt_fg: 89.0000  sup_num_gt_bg: 129.0000  sup_acc_fg: 8.9888  sup_acc_bg: 76.7442  sup_loss_bbox: 1.2883
2025/12/09 18:39:36 - mmengine - INFO - [Supervised Step 50] Groups: 2 , G0:3box | G1:9box
2025/12/09 18:40:21 - mmengine - INFO - [Teacher Predictions] Total boxes: 160, Score range: [0.767, 0.977], Mean: 0.882, Median: 0.883
2025/12/09 18:40:21 - mmengine - INFO - [After Filtering] Threshold: 0.4, Kept: 50/160 (31.2%), Avg: 6.2 boxes/img
2025/12/09 18:40:21 - mmengine - INFO - [Class Distribution] C0:2922 | Broken:11 | Chipped:12724 | Scratched:33
2025/12/09 18:40:21 - mmengine - INFO - [Before uncertainty] Box counts per view: [7, 5, 7, 7, 6, 6, 6, 6]
2025/12/09 18:40:21 - mmengine - INFO - [After reg_uncs] Box counts: [7, 5, 7, 7, 6, 6, 6, 6], reg_uncs sizes: [7, 5, 7, 7, 6, 6, 6, 6]
2025/12/09 18:40:21 - mmengine - INFO - [After reg_uncs] reg_uncs shapes: ['(7,)', '(5,)', '(7,)', '(7,)', '(6,)', '(6,)', '(6,)', '(6,)']
2025/12/09 18:40:22 - mmengine - INFO - [After cv_uncs] cv_uncs sizes: [7, 5, 7, 7, 6, 6, 6, 6]
2025/12/09 18:40:22 - mmengine - INFO - [After cv_uncs] cv_uncs shapes: ['(7,)', '(5,)', '(7,)', '(7,)', '(6,)', '(6,)', '(6,)', '(6,)']
2025/12/09 18:40:22 - mmengine - INFO - [Cross-View Uncertainty] Groups: 1, G0:50box(unc=0.076)
2025/12/09 18:42:22 - mmengine - INFO - Epoch(train) [1][100/125]  lr: 9.9010e-06  eta: 3 days, 18:19:56  time: 3.3146  data_time: 0.0317  memory: 25317  loss: 2.7832  sup_loss_rpn_cls: 0.1636  sup_loss_rpn_bbox: 0.6004  sup_loss_cls: 0.5198  sup_acc: 65.9722  sup_num_pred_fg: 56.0000  sup_num_pred_bg: 232.0000  sup_num_gt_fg: 117.0000  sup_num_gt_bg: 171.0000  sup_acc_fg: 29.0598  sup_acc_bg: 91.2281  sup_loss_bbox: 1.3308  unsup_loss_rpn_cls: 0.3124  unsup_loss_rpn_bbox: 0.9594  unsup_loss_cls: 1.3380  unsup_acc: 47.8328  unsup_num_pred_fg: 211.0000  unsup_num_pred_bg: 435.0000  unsup_num_gt_fg: 259.0000  unsup_num_gt_bg: 387.0000  unsup_acc_fg: 24.3243  unsup_acc_bg: 63.5659  unsup_loss_bbox: 1.6056
2025/12/09 18:42:22 - mmengine - INFO - [Supervised Step 100] Groups: 2 , G0:3box | G1:17box
2025/12/09 18:43:41 - mmengine - INFO - Exp name: soft_teacher_custom_multi_view_20251209_183641
2025/12/09 18:45:01 - mmengine - INFO - Epoch(train) [1][150/125]  lr: 1.2376e-05  eta: 3 days, 17:36:25  time: 3.1817  data_time: 0.0401  memory: 25317  loss: 2.5109  sup_loss_rpn_cls: 0.1495  sup_loss_rpn_bbox: 0.5542  sup_loss_cls: 0.4010  sup_acc: 57.1429  sup_num_pred_fg: 10.0000  sup_num_pred_bg: 193.0000  sup_num_gt_fg: 81.0000  sup_num_gt_bg: 122.0000  sup_acc_fg: 0.0000  sup_acc_bg: 95.0820  sup_loss_bbox: 1.4062  unsup_loss_rpn_cls: 0.3124  unsup_loss_rpn_bbox: 0.9594  unsup_loss_cls: 1.3380  unsup_acc: 47.8328  unsup_num_pred_fg: 211.0000  unsup_num_pred_bg: 435.0000  unsup_num_gt_fg: 259.0000  unsup_num_gt_bg: 387.0000  unsup_acc_fg: 24.3243  unsup_acc_bg: 63.5659  unsup_loss_bbox: 1.6056
2025/12/09 18:45:01 - mmengine - INFO - [Supervised Step 150] Groups: 2 , G0:6box | G1:13box
