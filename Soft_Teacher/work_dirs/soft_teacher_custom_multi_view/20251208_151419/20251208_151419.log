2025/12/08 15:14:19 - mmengine - INFO - 
------------------------------------------------------------
System environment:
    sys.platform: linux
    Python: 3.9.19 (main, May  6 2024, 19:43:03) [GCC 11.2.0]
    CUDA available: True
    MUSA available: False
    numpy_random_seed: 1782903252
    GPU 0: NVIDIA RTX A6000
    CUDA_HOME: /usr/local/cuda
    NVCC: Cuda compilation tools, release 12.6, V12.6.85
    GCC: gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
    PyTorch: 2.0.0+cu117
    PyTorch compiling details: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

    TorchVision: 0.15.1+cu117
    OpenCV: 4.11.0
    MMEngine: 0.11.0rc0

Runtime environment:
    cudnn_benchmark: False
    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}
    dist_cfg: {'backend': 'nccl'}
    seed: 1782903252
    Distributed launcher: none
    Distributed training: False
    GPU number: 1
------------------------------------------------------------

2025/12/08 15:14:22 - mmengine - INFO - Config:
backend_args = None
batch_size = 2
branch_field = [
    'sup',
    'unsup_teacher',
    'unsup_student',
]
color_space = [
    [
        dict(prob=0.5, type='AutoContrast'),
    ],
    [
        dict(prob=0.4, type='Equalize'),
    ],
    [
        dict(level=7, max_mag=1.25, min_mag=0.8, prob=0.6, type='Brightness'),
    ],
    [
        dict(level=7, max_mag=1.3, min_mag=0.8, prob=0.6, type='Contrast'),
    ],
    [
        dict(level=6, max_mag=1.5, min_mag=0.8, prob=0.4, type='Sharpness'),
    ],
    [
        dict(level=5, max_mag=7.0, min_mag=5.0, prob=0.2, type='Posterize'),
    ],
]
custom_hooks = [
    dict(momentum=0.999, type='MeanTeacherHook'),
]
custom_imports = dict(
    allow_failed_imports=False,
    imports=[
        'mmdet.models.utils.multi_view',
        'mmdet.models.utils.multi_view_transformer',
        'mmdet.datasets.wrappers.multi_view_from_folder',
    ])
data_root = '/home/coder/data/trong/KLTN/Soft_Teacher/data_drill/'
dataset_type = 'CocoDataset'
default_hooks = dict(
    checkpoint=dict(
        interval=1,
        max_keep_ckpts=1,
        rule='greater',
        save_best='teacher/coco/bbox_mAP_50',
        type='CheckpointHook'),
    logger=dict(interval=50, type='LoggerHook'),
    param_scheduler=dict(type='ParamSchedulerHook'),
    sampler_seed=dict(type='DistSamplerSeedHook'),
    timer=dict(type='IterTimerHook'),
    visualization=dict(type='DetVisualizationHook'))
default_scope = 'mmdet'
detector = dict(
    backbone=dict(
        backbone=dict(
            depth=50,
            frozen_stages=1,
            init_cfg=dict(
                checkpoint='open-mmlab://detectron2/resnet50_caffe',
                type='Pretrained'),
            norm_cfg=dict(requires_grad=False, type='BN'),
            norm_eval=True,
            num_stages=4,
            out_indices=(
                0,
                1,
                2,
                3,
            ),
            style='caffe',
            type='ResNet'),
        concat_reduce=False,
        fusion='mvvit',
        mvvit=dict(
            dropout=0.2,
            embed_dim=256,
            mlp_ratio=2.0,
            num_heads=4,
            num_layers=1,
            spatial_attention='moderate',
            type='MVViT',
            use_gradient_checkpointing=True,
            use_layer_norm=True),
        type='MultiViewBackbone',
        views_per_sample=8),
    data_preprocessor=dict(
        bgr_to_rgb=False,
        mean=[
            103.53,
            116.28,
            123.675,
        ],
        pad_size_divisor=8,
        std=[
            1.0,
            1.0,
            1.0,
        ],
        type='DetDataPreprocessor'),
    neck=dict(
        in_channels=[
            256,
            512,
            1024,
            2048,
        ],
        num_outs=5,
        out_channels=256,
        type='FPN'),
    roi_head=dict(
        bbox_head=dict(
            bbox_coder=dict(
                target_means=[
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                ],
                target_stds=[
                    0.1,
                    0.1,
                    0.2,
                    0.2,
                ],
                type='DeltaXYWHBBoxCoder'),
            fc_out_channels=1024,
            in_channels=256,
            loss_bbox=dict(loss_weight=1.0, type='L1Loss'),
            loss_cls=dict(
                alpha=0.5,
                gamma=2.2,
                loss_weight=1.5,
                type='FocalLoss',
                use_sigmoid=True),
            num_classes=5,
            reg_class_agnostic=False,
            roi_feat_size=7,
            type='Shared2FCBBoxHead'),
        bbox_roi_extractor=dict(
            featmap_strides=[
                4,
                8,
                16,
                32,
            ],
            out_channels=256,
            roi_layer=dict(output_size=7, sampling_ratio=0, type='RoIAlign'),
            type='SingleRoIExtractor'),
        type='StandardRoIHead'),
    rpn_head=dict(
        anchor_generator=dict(
            ratios=[
                0.3,
                0.45,
                0.6,
                1.0,
                2.0,
            ],
            scales=[
                8,
                16,
                32,
            ],
            strides=[
                4,
                8,
                16,
                32,
                64,
            ],
            type='AnchorGenerator'),
        bbox_coder=dict(
            target_means=[
                0.0,
                0.0,
                0.0,
                0.0,
            ],
            target_stds=[
                1.0,
                1.0,
                1.0,
                1.0,
            ],
            type='DeltaXYWHBBoxCoder'),
        feat_channels=256,
        in_channels=256,
        loss_bbox=dict(loss_weight=1.0, type='L1Loss'),
        loss_cls=dict(
            alpha=0.5,
            gamma=2.0,
            loss_weight=2.0,
            type='FocalLoss',
            use_sigmoid=True),
        type='RPNHead'),
    test_cfg=dict(
        rcnn=dict(
            max_per_img=20,
            nms=dict(iou_threshold=0.5, type='nms'),
            score_thr=0.5),
        rpn=dict(
            max_per_img=1000,
            min_bbox_size=8,
            nms=dict(iou_threshold=0.7, type='nms'),
            nms_pre=2000)),
    train_cfg=dict(
        rcnn=dict(
            assigner=dict(
                ignore_iof_thr=-1,
                match_low_quality=False,
                min_pos_iou=0.5,
                neg_iou_thr=0.5,
                pos_iou_thr=0.5,
                type='MaxIoUAssigner'),
            debug=False,
            pos_weight=-1,
            sampler=dict(
                add_gt_as_proposals=True,
                neg_pos_ub=2,
                num=256,
                pos_fraction=0.6,
                type='RandomSampler')),
        rpn=dict(
            allowed_border=-1,
            assigner=dict(
                ignore_iof_thr=-1,
                match_low_quality=True,
                min_pos_iou=0.3,
                neg_iou_thr=0.3,
                pos_iou_thr=0.5,
                type='MaxIoUAssigner'),
            debug=False,
            pos_weight=-1,
            sampler=dict(
                add_gt_as_proposals=False,
                neg_pos_ub=2,
                num=256,
                pos_fraction=0.6,
                type='RandomSampler')),
        rpn_proposal=dict(
            max_per_img=1000,
            min_bbox_size=0,
            nms=dict(iou_threshold=0.7, type='nms'),
            nms_pre=2000)),
    type='FasterRCNN')
env_cfg = dict(
    cudnn_benchmark=False,
    dist_cfg=dict(backend='nccl'),
    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))
geometric = [
    [
        dict(level=7, max_mag=0.05, min_mag=0.0, prob=0.3, type='TranslateX'),
    ],
    [
        dict(level=7, max_mag=0.05, min_mag=0.0, prob=0.3, type='TranslateY'),
    ],
]
img_scale = (
    256,
    720,
)
labeled_dataset = dict(
    ann_file=
    '/home/coder/data/trong/KLTN/Soft_Teacher/data_drill/semi_anno_multiview/_annotations.coco.labeled.grouped@40.bright.json',
    backend_args=None,
    data_prefix=dict(img='train/'),
    data_root='/home/coder/data/trong/KLTN/Soft_Teacher/data_drill/',
    filter_cfg=dict(filter_empty_gt=True, min_size=4),
    metainfo=dict(
        classes=(
            'Broken',
            'Chipped',
            'Scratched',
            'Severe_Rust',
            'Tip_Wear',
        ),
        palette=[
            (
                134,
                34,
                255,
            ),
            (
                0,
                255,
                206,
            ),
            (
                255,
                128,
                0,
            ),
            (
                254,
                0,
                86,
            ),
            (
                199,
                252,
                0,
            ),
        ]),
    pipeline=[
        dict(
            backend_args=None,
            imdecode_backend='cv2',
            to_float32=False,
            type='LoadImageFromFile'),
        dict(type='LoadAnnotations', with_bbox=True),
        dict(keep_ratio=True, scale=(
            256,
            720,
        ), type='Resize'),
        dict(prob=0.5, type='RandomFlip'),
        dict(
            aug_num=1,
            aug_space=[
                [
                    dict(prob=0.5, type='AutoContrast'),
                ],
                [
                    dict(prob=0.4, type='Equalize'),
                ],
                [
                    dict(
                        level=7,
                        max_mag=1.25,
                        min_mag=0.8,
                        prob=0.6,
                        type='Brightness'),
                ],
                [
                    dict(
                        level=7,
                        max_mag=1.3,
                        min_mag=0.8,
                        prob=0.6,
                        type='Contrast'),
                ],
                [
                    dict(
                        level=6,
                        max_mag=1.5,
                        min_mag=0.8,
                        prob=0.4,
                        type='Sharpness'),
                ],
                [
                    dict(
                        level=5,
                        max_mag=7.0,
                        min_mag=5.0,
                        prob=0.2,
                        type='Posterize'),
                ],
            ],
            type='RandAugment'),
        dict(min_gt_bbox_wh=(
            4,
            4,
        ), type='FilterAnnotations'),
        dict(
            branch_field=[
                'sup',
                'unsup_teacher',
                'unsup_student',
            ],
            sup=dict(type='PackDetInputs'),
            type='MultiBranch'),
    ],
    type='MultiViewFromFolder',
    views_per_sample=8)
launcher = 'none'
load_from = None
log_level = 'INFO'
log_processor = dict(by_epoch=True, type='LogProcessor', window_size=50)
metainfo = dict(
    classes=(
        'Broken',
        'Chipped',
        'Scratched',
        'Severe_Rust',
        'Tip_Wear',
    ),
    palette=[
        (
            134,
            34,
            255,
        ),
        (
            0,
            255,
            206,
        ),
        (
            255,
            128,
            0,
        ),
        (
            254,
            0,
            86,
        ),
        (
            199,
            252,
            0,
        ),
    ])
model = dict(
    aggregate_views='mean',
    data_preprocessor=dict(
        data_preprocessor=dict(
            bgr_to_rgb=False,
            mean=[
                103.53,
                116.28,
                123.675,
            ],
            pad_size_divisor=8,
            std=[
                1.0,
                1.0,
                1.0,
            ],
            type='DetDataPreprocessor'),
        type='MultiBranchDataPreprocessor'),
    detector=dict(
        backbone=dict(
            backbone=dict(
                depth=50,
                frozen_stages=1,
                init_cfg=dict(
                    checkpoint='open-mmlab://detectron2/resnet50_caffe',
                    type='Pretrained'),
                norm_cfg=dict(requires_grad=False, type='BN'),
                norm_eval=True,
                num_stages=4,
                out_indices=(
                    0,
                    1,
                    2,
                    3,
                ),
                style='caffe',
                type='ResNet'),
            concat_reduce=False,
            fusion='mvvit',
            mvvit=dict(
                dropout=0.2,
                embed_dim=256,
                mlp_ratio=2.0,
                num_heads=4,
                num_layers=1,
                spatial_attention='moderate',
                type='MVViT',
                use_gradient_checkpointing=True,
                use_layer_norm=True),
            type='MultiViewBackbone',
            views_per_sample=8),
        data_preprocessor=dict(
            bgr_to_rgb=False,
            mean=[
                103.53,
                116.28,
                123.675,
            ],
            pad_size_divisor=8,
            std=[
                1.0,
                1.0,
                1.0,
            ],
            type='DetDataPreprocessor'),
        neck=dict(
            in_channels=[
                256,
                512,
                1024,
                2048,
            ],
            num_outs=5,
            out_channels=256,
            type='FPN'),
        roi_head=dict(
            bbox_head=dict(
                bbox_coder=dict(
                    target_means=[
                        0.0,
                        0.0,
                        0.0,
                        0.0,
                    ],
                    target_stds=[
                        0.1,
                        0.1,
                        0.2,
                        0.2,
                    ],
                    type='DeltaXYWHBBoxCoder'),
                fc_out_channels=1024,
                in_channels=256,
                loss_bbox=dict(loss_weight=1.0, type='L1Loss'),
                loss_cls=dict(
                    alpha=0.5,
                    gamma=2.2,
                    loss_weight=1.5,
                    type='FocalLoss',
                    use_sigmoid=True),
                num_classes=5,
                reg_class_agnostic=False,
                roi_feat_size=7,
                type='Shared2FCBBoxHead'),
            bbox_roi_extractor=dict(
                featmap_strides=[
                    4,
                    8,
                    16,
                    32,
                ],
                out_channels=256,
                roi_layer=dict(
                    output_size=7, sampling_ratio=0, type='RoIAlign'),
                type='SingleRoIExtractor'),
            type='StandardRoIHead'),
        rpn_head=dict(
            anchor_generator=dict(
                ratios=[
                    0.3,
                    0.45,
                    0.6,
                    1.0,
                    2.0,
                ],
                scales=[
                    8,
                    16,
                    32,
                ],
                strides=[
                    4,
                    8,
                    16,
                    32,
                    64,
                ],
                type='AnchorGenerator'),
            bbox_coder=dict(
                target_means=[
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                ],
                target_stds=[
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                ],
                type='DeltaXYWHBBoxCoder'),
            feat_channels=256,
            in_channels=256,
            loss_bbox=dict(loss_weight=1.0, type='L1Loss'),
            loss_cls=dict(
                alpha=0.5,
                gamma=2.0,
                loss_weight=2.0,
                type='FocalLoss',
                use_sigmoid=True),
            type='RPNHead'),
        test_cfg=dict(
            rcnn=dict(
                max_per_img=20,
                nms=dict(iou_threshold=0.5, type='nms'),
                score_thr=0.5),
            rpn=dict(
                max_per_img=1000,
                min_bbox_size=8,
                nms=dict(iou_threshold=0.7, type='nms'),
                nms_pre=2000)),
        train_cfg=dict(
            rcnn=dict(
                assigner=dict(
                    ignore_iof_thr=-1,
                    match_low_quality=False,
                    min_pos_iou=0.5,
                    neg_iou_thr=0.5,
                    pos_iou_thr=0.5,
                    type='MaxIoUAssigner'),
                debug=False,
                pos_weight=-1,
                sampler=dict(
                    add_gt_as_proposals=True,
                    neg_pos_ub=2,
                    num=256,
                    pos_fraction=0.6,
                    type='RandomSampler')),
            rpn=dict(
                allowed_border=-1,
                assigner=dict(
                    ignore_iof_thr=-1,
                    match_low_quality=True,
                    min_pos_iou=0.3,
                    neg_iou_thr=0.3,
                    pos_iou_thr=0.5,
                    type='MaxIoUAssigner'),
                debug=False,
                pos_weight=-1,
                sampler=dict(
                    add_gt_as_proposals=False,
                    neg_pos_ub=2,
                    num=256,
                    pos_fraction=0.6,
                    type='RandomSampler')),
            rpn_proposal=dict(
                max_per_img=1000,
                min_bbox_size=0,
                nms=dict(iou_threshold=0.7, type='nms'),
                nms_pre=2000)),
        type='FasterRCNN'),
    semi_test_cfg=dict(predict_on='teacher'),
    semi_train_cfg=dict(
        cls_pseudo_thr=0.35,
        enable_consensus=True,
        freeze_teacher=True,
        jitter_scale=0.03,
        jitter_times=5,
        min_pseudo_bbox_wh=(
            8,
            8,
        ),
        pseudo_label_initial_score_thr=0.4,
        reg_pseudo_thr=0.1,
        rpn_pseudo_thr=0.35,
        sup_weight=1.0,
        unsup_weight=0.5),
    type='MultiViewSoftTeacher',
    views_per_sample=8)
num_workers = 4
optim_wrapper = dict(
    optimizer=dict(lr=0.0005, momentum=0.9, type='SGD', weight_decay=0.0001),
    type='OptimWrapper')
param_scheduler = [
    dict(
        begin=0, by_epoch=False, end=10000, start_factor=0.01,
        type='LinearLR'),
    dict(
        begin=10000, by_epoch=False, end=70000, factor=1.0, type='ConstantLR'),
    dict(
        begin=70000,
        by_epoch=False,
        end=100000,
        eta_min=5e-05,
        type='CosineAnnealingLR'),
]
resume = False
strong_pipeline = [
    dict(keep_ratio=True, scale=(
        256,
        720,
    ), type='Resize'),
    dict(prob=0.5, type='RandomFlip'),
    dict(
        transforms=[
            dict(
                aug_num=2,
                aug_space=[
                    [
                        dict(prob=0.5, type='AutoContrast'),
                    ],
                    [
                        dict(prob=0.4, type='Equalize'),
                    ],
                    [
                        dict(
                            level=7,
                            max_mag=1.25,
                            min_mag=0.8,
                            prob=0.6,
                            type='Brightness'),
                    ],
                    [
                        dict(
                            level=7,
                            max_mag=1.3,
                            min_mag=0.8,
                            prob=0.6,
                            type='Contrast'),
                    ],
                    [
                        dict(
                            level=6,
                            max_mag=1.5,
                            min_mag=0.8,
                            prob=0.4,
                            type='Sharpness'),
                    ],
                    [
                        dict(
                            level=5,
                            max_mag=7.0,
                            min_mag=5.0,
                            prob=0.2,
                            type='Posterize'),
                    ],
                ],
                type='RandAugment'),
            dict(
                aug_num=1,
                aug_space=[
                    [
                        dict(
                            level=7,
                            max_mag=0.05,
                            min_mag=0.0,
                            prob=0.3,
                            type='TranslateX'),
                    ],
                    [
                        dict(
                            level=7,
                            max_mag=0.05,
                            min_mag=0.0,
                            prob=0.3,
                            type='TranslateY'),
                    ],
                ],
                type='RandAugment'),
        ],
        type='RandomOrder'),
    dict(n_patches=(
        1,
        3,
    ), ratio=(
        0,
        0.15,
    ), type='RandomErasing'),
    dict(min_gt_bbox_wh=(
        4,
        4,
    ), type='FilterAnnotations'),
    dict(
        meta_keys=(
            'img_id',
            'img_path',
            'ori_shape',
            'img_shape',
            'scale_factor',
            'flip',
            'flip_direction',
            'homography_matrix',
        ),
        type='PackDetInputs'),
]
sup_pipeline = [
    dict(
        backend_args=None,
        imdecode_backend='cv2',
        to_float32=False,
        type='LoadImageFromFile'),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(keep_ratio=True, scale=(
        256,
        720,
    ), type='Resize'),
    dict(prob=0.5, type='RandomFlip'),
    dict(
        aug_num=1,
        aug_space=[
            [
                dict(prob=0.5, type='AutoContrast'),
            ],
            [
                dict(prob=0.4, type='Equalize'),
            ],
            [
                dict(
                    level=7,
                    max_mag=1.25,
                    min_mag=0.8,
                    prob=0.6,
                    type='Brightness'),
            ],
            [
                dict(
                    level=7,
                    max_mag=1.3,
                    min_mag=0.8,
                    prob=0.6,
                    type='Contrast'),
            ],
            [
                dict(
                    level=6,
                    max_mag=1.5,
                    min_mag=0.8,
                    prob=0.4,
                    type='Sharpness'),
            ],
            [
                dict(
                    level=5,
                    max_mag=7.0,
                    min_mag=5.0,
                    prob=0.2,
                    type='Posterize'),
            ],
        ],
        type='RandAugment'),
    dict(min_gt_bbox_wh=(
        4,
        4,
    ), type='FilterAnnotations'),
    dict(
        branch_field=[
            'sup',
            'unsup_teacher',
            'unsup_student',
        ],
        sup=dict(type='PackDetInputs'),
        type='MultiBranch'),
]
test_cfg = dict(type='TestLoop')
test_dataloader = dict(
    batch_size=1,
    collate_fn=dict(
        type=
        'mmdet.datasets.wrappers.multi_view_from_folder.multi_view_collate_val'
    ),
    dataset=dict(
        ann_file=
        '/home/coder/data/trong/KLTN/Soft_Teacher/data_drill/anno_valid/_annotations_filtered.bright.coco.json',
        backend_args=None,
        data_prefix=dict(img='valid/'),
        data_root='/home/coder/data/trong/KLTN/Soft_Teacher/data_drill/',
        metainfo=dict(
            classes=(
                'Broken',
                'Chipped',
                'Scratched',
                'Severe_Rust',
                'Tip_Wear',
            ),
            palette=[
                (
                    134,
                    34,
                    255,
                ),
                (
                    0,
                    255,
                    206,
                ),
                (
                    255,
                    128,
                    0,
                ),
                (
                    254,
                    0,
                    86,
                ),
                (
                    199,
                    252,
                    0,
                ),
            ]),
        pipeline=[
            dict(
                backend_args=None,
                imdecode_backend='cv2',
                to_float32=False,
                type='LoadImageFromFile'),
            dict(keep_ratio=True, scale=(
                256,
                720,
            ), type='Resize'),
            dict(
                meta_keys=(
                    'img_id',
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                ),
                type='PackDetInputs'),
        ],
        test_mode=True,
        type='MultiViewFromFolder',
        views_per_sample=8),
    drop_last=False,
    num_workers=2,
    persistent_workers=True,
    pin_memory=True,
    sampler=dict(shuffle=False, type='DefaultSampler'))
test_evaluator = dict(
    aggregation='none',
    ann_file=
    '/home/coder/data/trong/KLTN/Soft_Teacher/data_drill/anno_valid/_annotations_filtered.bright.coco.json',
    backend_args=None,
    classwise=True,
    enable_aggregation=False,
    extract_base_name=True,
    format_only=False,
    metric='bbox',
    nms_iou_thr=0.4,
    type='MultiViewCocoMetric',
    views_per_sample=8)
test_pipeline = [
    dict(
        backend_args=None,
        imdecode_backend='cv2',
        to_float32=False,
        type='LoadImageFromFile'),
    dict(keep_ratio=True, scale=(
        256,
        720,
    ), type='Resize'),
    dict(
        meta_keys=(
            'img_id',
            'img_path',
            'ori_shape',
            'img_shape',
            'scale_factor',
        ),
        type='PackDetInputs'),
]
train_cfg = dict(
    max_iters=100000, type='IterBasedTrainLoop', val_interval=10000)
train_dataloader = dict(
    batch_size=2,
    collate_fn=dict(
        type=
        'mmdet.datasets.wrappers.multi_view_from_folder.multi_view_collate_flatten'
    ),
    dataset=dict(
        datasets=[
            dict(
                ann_file=
                '/home/coder/data/trong/KLTN/Soft_Teacher/data_drill/semi_anno_multiview/_annotations.coco.labeled.grouped@40.bright.json',
                backend_args=None,
                data_prefix=dict(img='train/'),
                data_root=
                '/home/coder/data/trong/KLTN/Soft_Teacher/data_drill/',
                filter_cfg=dict(filter_empty_gt=True, min_size=4),
                metainfo=dict(
                    classes=(
                        'Broken',
                        'Chipped',
                        'Scratched',
                        'Severe_Rust',
                        'Tip_Wear',
                    ),
                    palette=[
                        (
                            134,
                            34,
                            255,
                        ),
                        (
                            0,
                            255,
                            206,
                        ),
                        (
                            255,
                            128,
                            0,
                        ),
                        (
                            254,
                            0,
                            86,
                        ),
                        (
                            199,
                            252,
                            0,
                        ),
                    ]),
                pipeline=[
                    dict(
                        backend_args=None,
                        imdecode_backend='cv2',
                        to_float32=False,
                        type='LoadImageFromFile'),
                    dict(type='LoadAnnotations', with_bbox=True),
                    dict(keep_ratio=True, scale=(
                        256,
                        720,
                    ), type='Resize'),
                    dict(prob=0.5, type='RandomFlip'),
                    dict(
                        aug_num=1,
                        aug_space=[
                            [
                                dict(prob=0.5, type='AutoContrast'),
                            ],
                            [
                                dict(prob=0.4, type='Equalize'),
                            ],
                            [
                                dict(
                                    level=7,
                                    max_mag=1.25,
                                    min_mag=0.8,
                                    prob=0.6,
                                    type='Brightness'),
                            ],
                            [
                                dict(
                                    level=7,
                                    max_mag=1.3,
                                    min_mag=0.8,
                                    prob=0.6,
                                    type='Contrast'),
                            ],
                            [
                                dict(
                                    level=6,
                                    max_mag=1.5,
                                    min_mag=0.8,
                                    prob=0.4,
                                    type='Sharpness'),
                            ],
                            [
                                dict(
                                    level=5,
                                    max_mag=7.0,
                                    min_mag=5.0,
                                    prob=0.2,
                                    type='Posterize'),
                            ],
                        ],
                        type='RandAugment'),
                    dict(min_gt_bbox_wh=(
                        4,
                        4,
                    ), type='FilterAnnotations'),
                    dict(
                        branch_field=[
                            'sup',
                            'unsup_teacher',
                            'unsup_student',
                        ],
                        sup=dict(type='PackDetInputs'),
                        type='MultiBranch'),
                ],
                type='MultiViewFromFolder',
                views_per_sample=8),
            dict(
                ann_file=
                '/home/coder/data/trong/KLTN/Soft_Teacher/data_drill/semi_anno_multiview/_annotations.coco.unlabeled.grouped@40.bright.json',
                backend_args=None,
                data_prefix=dict(img='train/'),
                data_root=
                '/home/coder/data/trong/KLTN/Soft_Teacher/data_drill/',
                filter_cfg=dict(filter_empty_gt=False, min_size=4),
                metainfo=dict(
                    classes=(
                        'Broken',
                        'Chipped',
                        'Scratched',
                        'Severe_Rust',
                        'Tip_Wear',
                    ),
                    palette=[
                        (
                            134,
                            34,
                            255,
                        ),
                        (
                            0,
                            255,
                            206,
                        ),
                        (
                            255,
                            128,
                            0,
                        ),
                        (
                            254,
                            0,
                            86,
                        ),
                        (
                            199,
                            252,
                            0,
                        ),
                    ]),
                pipeline=[
                    dict(
                        backend_args=None,
                        imdecode_backend='cv2',
                        to_float32=False,
                        type='LoadImageFromFile'),
                    dict(type='LoadEmptyAnnotations'),
                    dict(
                        branch_field=[
                            'sup',
                            'unsup_teacher',
                            'unsup_student',
                        ],
                        type='MultiBranch',
                        unsup_student=[
                            dict(
                                keep_ratio=True,
                                scale=(
                                    256,
                                    720,
                                ),
                                type='Resize'),
                            dict(prob=0.5, type='RandomFlip'),
                            dict(
                                transforms=[
                                    dict(
                                        aug_num=2,
                                        aug_space=[
                                            [
                                                dict(
                                                    prob=0.5,
                                                    type='AutoContrast'),
                                            ],
                                            [
                                                dict(
                                                    prob=0.4, type='Equalize'),
                                            ],
                                            [
                                                dict(
                                                    level=7,
                                                    max_mag=1.25,
                                                    min_mag=0.8,
                                                    prob=0.6,
                                                    type='Brightness'),
                                            ],
                                            [
                                                dict(
                                                    level=7,
                                                    max_mag=1.3,
                                                    min_mag=0.8,
                                                    prob=0.6,
                                                    type='Contrast'),
                                            ],
                                            [
                                                dict(
                                                    level=6,
                                                    max_mag=1.5,
                                                    min_mag=0.8,
                                                    prob=0.4,
                                                    type='Sharpness'),
                                            ],
                                            [
                                                dict(
                                                    level=5,
                                                    max_mag=7.0,
                                                    min_mag=5.0,
                                                    prob=0.2,
                                                    type='Posterize'),
                                            ],
                                        ],
                                        type='RandAugment'),
                                    dict(
                                        aug_num=1,
                                        aug_space=[
                                            [
                                                dict(
                                                    level=7,
                                                    max_mag=0.05,
                                                    min_mag=0.0,
                                                    prob=0.3,
                                                    type='TranslateX'),
                                            ],
                                            [
                                                dict(
                                                    level=7,
                                                    max_mag=0.05,
                                                    min_mag=0.0,
                                                    prob=0.3,
                                                    type='TranslateY'),
                                            ],
                                        ],
                                        type='RandAugment'),
                                ],
                                type='RandomOrder'),
                            dict(
                                n_patches=(
                                    1,
                                    3,
                                ),
                                ratio=(
                                    0,
                                    0.15,
                                ),
                                type='RandomErasing'),
                            dict(
                                min_gt_bbox_wh=(
                                    4,
                                    4,
                                ),
                                type='FilterAnnotations'),
                            dict(
                                meta_keys=(
                                    'img_id',
                                    'img_path',
                                    'ori_shape',
                                    'img_shape',
                                    'scale_factor',
                                    'flip',
                                    'flip_direction',
                                    'homography_matrix',
                                ),
                                type='PackDetInputs'),
                        ],
                        unsup_teacher=[
                            dict(
                                keep_ratio=True,
                                scale=(
                                    256,
                                    720,
                                ),
                                type='Resize'),
                            dict(prob=0.5, type='RandomFlip'),
                            dict(
                                meta_keys=(
                                    'img_id',
                                    'img_path',
                                    'ori_shape',
                                    'img_shape',
                                    'scale_factor',
                                    'flip',
                                    'flip_direction',
                                    'homography_matrix',
                                ),
                                type='PackDetInputs'),
                        ]),
                ],
                type='MultiViewFromFolder',
                views_per_sample=8),
        ],
        type='ConcatDataset'),
    num_workers=4,
    persistent_workers=True,
    pin_memory=True,
    prefetch_factor=2,
    sampler=dict(
        batch_size=2, source_ratio=[
            1,
            1,
        ], type='GroupMultiSourceSampler'))
unlabeled_dataset = dict(
    ann_file=
    '/home/coder/data/trong/KLTN/Soft_Teacher/data_drill/semi_anno_multiview/_annotations.coco.unlabeled.grouped@40.bright.json',
    backend_args=None,
    data_prefix=dict(img='train/'),
    data_root='/home/coder/data/trong/KLTN/Soft_Teacher/data_drill/',
    filter_cfg=dict(filter_empty_gt=False, min_size=4),
    metainfo=dict(
        classes=(
            'Broken',
            'Chipped',
            'Scratched',
            'Severe_Rust',
            'Tip_Wear',
        ),
        palette=[
            (
                134,
                34,
                255,
            ),
            (
                0,
                255,
                206,
            ),
            (
                255,
                128,
                0,
            ),
            (
                254,
                0,
                86,
            ),
            (
                199,
                252,
                0,
            ),
        ]),
    pipeline=[
        dict(
            backend_args=None,
            imdecode_backend='cv2',
            to_float32=False,
            type='LoadImageFromFile'),
        dict(type='LoadEmptyAnnotations'),
        dict(
            branch_field=[
                'sup',
                'unsup_teacher',
                'unsup_student',
            ],
            type='MultiBranch',
            unsup_student=[
                dict(keep_ratio=True, scale=(
                    256,
                    720,
                ), type='Resize'),
                dict(prob=0.5, type='RandomFlip'),
                dict(
                    transforms=[
                        dict(
                            aug_num=2,
                            aug_space=[
                                [
                                    dict(prob=0.5, type='AutoContrast'),
                                ],
                                [
                                    dict(prob=0.4, type='Equalize'),
                                ],
                                [
                                    dict(
                                        level=7,
                                        max_mag=1.25,
                                        min_mag=0.8,
                                        prob=0.6,
                                        type='Brightness'),
                                ],
                                [
                                    dict(
                                        level=7,
                                        max_mag=1.3,
                                        min_mag=0.8,
                                        prob=0.6,
                                        type='Contrast'),
                                ],
                                [
                                    dict(
                                        level=6,
                                        max_mag=1.5,
                                        min_mag=0.8,
                                        prob=0.4,
                                        type='Sharpness'),
                                ],
                                [
                                    dict(
                                        level=5,
                                        max_mag=7.0,
                                        min_mag=5.0,
                                        prob=0.2,
                                        type='Posterize'),
                                ],
                            ],
                            type='RandAugment'),
                        dict(
                            aug_num=1,
                            aug_space=[
                                [
                                    dict(
                                        level=7,
                                        max_mag=0.05,
                                        min_mag=0.0,
                                        prob=0.3,
                                        type='TranslateX'),
                                ],
                                [
                                    dict(
                                        level=7,
                                        max_mag=0.05,
                                        min_mag=0.0,
                                        prob=0.3,
                                        type='TranslateY'),
                                ],
                            ],
                            type='RandAugment'),
                    ],
                    type='RandomOrder'),
                dict(
                    n_patches=(
                        1,
                        3,
                    ),
                    ratio=(
                        0,
                        0.15,
                    ),
                    type='RandomErasing'),
                dict(min_gt_bbox_wh=(
                    4,
                    4,
                ), type='FilterAnnotations'),
                dict(
                    meta_keys=(
                        'img_id',
                        'img_path',
                        'ori_shape',
                        'img_shape',
                        'scale_factor',
                        'flip',
                        'flip_direction',
                        'homography_matrix',
                    ),
                    type='PackDetInputs'),
            ],
            unsup_teacher=[
                dict(keep_ratio=True, scale=(
                    256,
                    720,
                ), type='Resize'),
                dict(prob=0.5, type='RandomFlip'),
                dict(
                    meta_keys=(
                        'img_id',
                        'img_path',
                        'ori_shape',
                        'img_shape',
                        'scale_factor',
                        'flip',
                        'flip_direction',
                        'homography_matrix',
                    ),
                    type='PackDetInputs'),
            ]),
    ],
    type='MultiViewFromFolder',
    views_per_sample=8)
unsup_pipeline = [
    dict(
        backend_args=None,
        imdecode_backend='cv2',
        to_float32=False,
        type='LoadImageFromFile'),
    dict(type='LoadEmptyAnnotations'),
    dict(
        branch_field=[
            'sup',
            'unsup_teacher',
            'unsup_student',
        ],
        type='MultiBranch',
        unsup_student=[
            dict(keep_ratio=True, scale=(
                256,
                720,
            ), type='Resize'),
            dict(prob=0.5, type='RandomFlip'),
            dict(
                transforms=[
                    dict(
                        aug_num=2,
                        aug_space=[
                            [
                                dict(prob=0.5, type='AutoContrast'),
                            ],
                            [
                                dict(prob=0.4, type='Equalize'),
                            ],
                            [
                                dict(
                                    level=7,
                                    max_mag=1.25,
                                    min_mag=0.8,
                                    prob=0.6,
                                    type='Brightness'),
                            ],
                            [
                                dict(
                                    level=7,
                                    max_mag=1.3,
                                    min_mag=0.8,
                                    prob=0.6,
                                    type='Contrast'),
                            ],
                            [
                                dict(
                                    level=6,
                                    max_mag=1.5,
                                    min_mag=0.8,
                                    prob=0.4,
                                    type='Sharpness'),
                            ],
                            [
                                dict(
                                    level=5,
                                    max_mag=7.0,
                                    min_mag=5.0,
                                    prob=0.2,
                                    type='Posterize'),
                            ],
                        ],
                        type='RandAugment'),
                    dict(
                        aug_num=1,
                        aug_space=[
                            [
                                dict(
                                    level=7,
                                    max_mag=0.05,
                                    min_mag=0.0,
                                    prob=0.3,
                                    type='TranslateX'),
                            ],
                            [
                                dict(
                                    level=7,
                                    max_mag=0.05,
                                    min_mag=0.0,
                                    prob=0.3,
                                    type='TranslateY'),
                            ],
                        ],
                        type='RandAugment'),
                ],
                type='RandomOrder'),
            dict(n_patches=(
                1,
                3,
            ), ratio=(
                0,
                0.15,
            ), type='RandomErasing'),
            dict(min_gt_bbox_wh=(
                4,
                4,
            ), type='FilterAnnotations'),
            dict(
                meta_keys=(
                    'img_id',
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                    'flip',
                    'flip_direction',
                    'homography_matrix',
                ),
                type='PackDetInputs'),
        ],
        unsup_teacher=[
            dict(keep_ratio=True, scale=(
                256,
                720,
            ), type='Resize'),
            dict(prob=0.5, type='RandomFlip'),
            dict(
                meta_keys=(
                    'img_id',
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                    'flip',
                    'flip_direction',
                    'homography_matrix',
                ),
                type='PackDetInputs'),
        ]),
]
val_cfg = dict(type='TeacherStudentValLoop')
val_dataloader = dict(
    batch_size=1,
    collate_fn=dict(
        type=
        'mmdet.datasets.wrappers.multi_view_from_folder.multi_view_collate_val'
    ),
    dataset=dict(
        ann_file=
        '/home/coder/data/trong/KLTN/Soft_Teacher/data_drill/anno_valid/_annotations_filtered.bright.coco.json',
        backend_args=None,
        data_prefix=dict(img='valid/'),
        data_root='/home/coder/data/trong/KLTN/Soft_Teacher/data_drill/',
        metainfo=dict(
            classes=(
                'Broken',
                'Chipped',
                'Scratched',
                'Severe_Rust',
                'Tip_Wear',
            ),
            palette=[
                (
                    134,
                    34,
                    255,
                ),
                (
                    0,
                    255,
                    206,
                ),
                (
                    255,
                    128,
                    0,
                ),
                (
                    254,
                    0,
                    86,
                ),
                (
                    199,
                    252,
                    0,
                ),
            ]),
        pipeline=[
            dict(
                backend_args=None,
                imdecode_backend='cv2',
                to_float32=False,
                type='LoadImageFromFile'),
            dict(keep_ratio=True, scale=(
                256,
                720,
            ), type='Resize'),
            dict(
                meta_keys=(
                    'img_id',
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                ),
                type='PackDetInputs'),
        ],
        test_mode=True,
        type='MultiViewFromFolder',
        views_per_sample=8),
    drop_last=False,
    num_workers=2,
    persistent_workers=True,
    pin_memory=True,
    sampler=dict(shuffle=False, type='DefaultSampler'))
val_evaluator = dict(
    aggregation='none',
    ann_file=
    '/home/coder/data/trong/KLTN/Soft_Teacher/data_drill/anno_valid/_annotations_filtered.bright.coco.json',
    backend_args=None,
    classwise=True,
    enable_aggregation=False,
    extract_base_name=True,
    format_only=False,
    metric='bbox',
    nms_iou_thr=0.4,
    type='MultiViewCocoMetric',
    views_per_sample=8)
views_per_sample = 8
vis_backends = [
    dict(type='LocalVisBackend'),
]
visualizer = dict(
    name='visualizer',
    type='DetLocalVisualizer',
    vis_backends=[
        dict(type='LocalVisBackend'),
    ])
weak_pipeline = [
    dict(keep_ratio=True, scale=(
        256,
        720,
    ), type='Resize'),
    dict(prob=0.5, type='RandomFlip'),
    dict(
        meta_keys=(
            'img_id',
            'img_path',
            'ori_shape',
            'img_shape',
            'scale_factor',
            'flip',
            'flip_direction',
            'homography_matrix',
        ),
        type='PackDetInputs'),
]
work_dir = './work_dirs/soft_teacher_custom_multi_view'

2025/12/08 15:14:26 - mmengine - INFO - MultiViewBackbone: Initialized with MVViT fusion, views_per_sample=8
2025/12/08 15:14:26 - mmengine - INFO - MultiViewBackbone: Initialized with MVViT fusion, views_per_sample=8
2025/12/08 15:14:28 - mmengine - INFO - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.
2025/12/08 15:14:28 - mmengine - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) RuntimeInfoHook                    
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
before_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) MeanTeacherHook                    
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DistSamplerSeedHook                
 -------------------- 
before_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) MeanTeacherHook                    
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) IterTimerHook                      
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_val_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_val_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_val_iter:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DetVisualizationHook               
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_val_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_test_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_test_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_test_iter:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DetVisualizationHook               
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_run:
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
2025/12/08 15:14:31 - mmengine - INFO - [COCO ANNO] Loaded from: /home/coder/data/trong/KLTN/Soft_Teacher/data_drill/semi_anno_multiview/_annotations.coco.labeled.grouped@40.bright.json
2025/12/08 15:14:31 - mmengine - INFO -    1208 images, 1448 annotations
2025/12/08 15:14:31 - mmengine - INFO -    946 files have annotations
2025/12/08 15:14:31 - mmengine - INFO -    1208 files have base_img_id
2025/12/08 15:14:31 - mmengine - INFO -    Categories: [0, 1, 2, 3, 4, 5]
2025/12/08 15:14:31 - mmengine - INFO -    cat2label mapping: {1: 0, 2: 1, 3: 2, 4: 3, 5: 4}
2025/12/08 15:14:31 - mmengine - INFO -    File 'S136_Image__2025-09-23__16-42-55_bright_4_crop_8_jpg.rf.8d03a89b2a2643c02b4733eb858e68be.jpg': 1 boxes, base_img_id=S136_bright_4, crop_num=8
2025/12/08 15:14:31 - mmengine - INFO -    File 'S40_Image__2025-09-04__16-02-50_bright_2_crop_6_jpg.rf.0006368956fc4380b51ac33ff3fbee7c.jpg': 1 boxes, base_img_id=S40_bright_2, crop_num=6
2025/12/08 15:14:31 - mmengine - INFO -    File 'S124_Image__2025-09-23__10-26-03_bright_3_crop_5_jpg.rf.eff0c11d31452dcd01ccb1959ba342f5.jpg': 2 boxes, base_img_id=S124_bright_3, crop_num=5
2025/12/08 15:14:31 - mmengine - INFO - [COCO] Built 151 groups from base_img_id
2025/12/08 15:14:31 - mmengine - INFO - [COCO] Total files in COCO JSON: 1208
2025/12/08 15:14:31 - mmengine - INFO - [COCO] Unique base_img_ids: 151
2025/12/08 15:14:31 - mmengine - INFO -    Group 0: base_img_id=S101_bright_2, 8/8 views
2025/12/08 15:14:31 - mmengine - INFO -    Group 1: base_img_id=S111_bright_3, 8/8 views
2025/12/08 15:14:31 - mmengine - INFO -    Group 2: base_img_id=S111_bright_4, 8/8 views
2025/12/08 15:14:31 - mmengine - INFO - [COCO ANNO] Loaded from: /home/coder/data/trong/KLTN/Soft_Teacher/data_drill/semi_anno_multiview/_annotations.coco.unlabeled.grouped@40.bright.json
2025/12/08 15:14:31 - mmengine - INFO -    784 images, 956 annotations
2025/12/08 15:14:31 - mmengine - INFO -    649 files have annotations
2025/12/08 15:14:31 - mmengine - INFO -    784 files have base_img_id
2025/12/08 15:14:31 - mmengine - INFO -    Categories: [0, 1, 2, 3, 4, 5]
2025/12/08 15:14:31 - mmengine - INFO -    cat2label mapping: {1: 0, 2: 1, 3: 2, 4: 3, 5: 4}
2025/12/08 15:14:31 - mmengine - INFO -    File 'S4_than_Image__2025-09-03__14-00-16_bright_2_crop_3_jpg.rf.93c6a9e3d563bf395cd5c9ef7519b742.jpg': 3 boxes, base_img_id=S4_bright_2, crop_num=3
2025/12/08 15:14:31 - mmengine - INFO -    File 'S225_Image__2025-07-01__14-30-51_bright_2_crop_7_jpg.rf.6b508c0dd7ed86aff7e31273da30c5e4.jpg': 2 boxes, base_img_id=S225_bright_2, crop_num=7
2025/12/08 15:14:31 - mmengine - INFO -    File 'S78_Image__2025-09-08__16-35-36_bright_3_crop_8_jpg.rf.84d34f89fe2af65f5adacb8437ed2ee6.jpg': 3 boxes, base_img_id=S78_bright_3, crop_num=8
2025/12/08 15:14:31 - mmengine - INFO - [COCO] Built 98 groups from base_img_id
2025/12/08 15:14:31 - mmengine - INFO - [COCO] Total files in COCO JSON: 784
2025/12/08 15:14:31 - mmengine - INFO - [COCO] Unique base_img_ids: 98
2025/12/08 15:14:31 - mmengine - INFO -    Group 0: base_img_id=S111_bright_2, 8/8 views
2025/12/08 15:14:31 - mmengine - INFO -    Group 1: base_img_id=S114_bright_2, 8/8 views
2025/12/08 15:14:31 - mmengine - INFO -    Group 2: base_img_id=S117_bright_2, 8/8 views
2025/12/08 15:14:32 - mmengine - INFO - [COCO ANNO] Loaded from: /home/coder/data/trong/KLTN/Soft_Teacher/data_drill/anno_valid/_annotations_filtered.bright.coco.json
2025/12/08 15:14:32 - mmengine - INFO -    320 images, 442 annotations
2025/12/08 15:14:32 - mmengine - INFO -    284 files have annotations
2025/12/08 15:14:32 - mmengine - INFO -    320 files have base_img_id
2025/12/08 15:14:32 - mmengine - INFO -    Categories: [0, 1, 2, 3, 4, 5]
2025/12/08 15:14:32 - mmengine - INFO -    cat2label mapping: {1: 0, 2: 1, 3: 2, 4: 3, 5: 4}
2025/12/08 15:14:32 - mmengine - INFO -    File 'S245_Image__2025-11-11__12-09-08_bright_2_crop_5_jpg.rf.b0924b8796050c8c09b6c63180a8df2a.jpg': 1 boxes, base_img_id=S245_bright_2, crop_num=5
2025/12/08 15:14:32 - mmengine - INFO -    File 'S135_Image__2025-09-23__16-23-02_bright_4_crop_7_jpg.rf.1b8c818ab9826dbcd27a64c2e38414e1.jpg': 2 boxes, base_img_id=S135_bright_4, crop_num=7
2025/12/08 15:14:32 - mmengine - INFO -    File 'S242_Image__2025-11-11__11-56-30_bright_8_crop_1_jpg.rf.06e5607648d1e41e35c7bec4fc2eeb57.jpg': 1 boxes, base_img_id=S242_bright_8, crop_num=1
2025/12/08 15:14:32 - mmengine - INFO - [COCO] Built 40 groups from base_img_id
2025/12/08 15:14:32 - mmengine - INFO - [COCO] Total files in COCO JSON: 320
2025/12/08 15:14:32 - mmengine - INFO - [COCO] Unique base_img_ids: 40
2025/12/08 15:14:32 - mmengine - INFO -    Group 0: base_img_id=S110_bright_2, 8/8 views
2025/12/08 15:14:32 - mmengine - INFO -    Group 1: base_img_id=S110_bright_3, 8/8 views
2025/12/08 15:14:32 - mmengine - INFO -    Group 2: base_img_id=S110_bright_4, 8/8 views
2025/12/08 15:14:32 - mmengine - INFO - [BBOX DEBUG 1] File: S41_Image__2025-09-04__16-14-12_bright_2_crop_1_jpg.rf.704f95b6f2acf08915ff45852ad88d83.jpg, Image shape: 720x256
2025/12/08 15:14:32 - mmengine - INFO -    Loaded 2 boxes from COCO JSON
2025/12/08 15:14:32 - mmengine - INFO -     Box 0: [93, 5, 138.38, 68.27000000000001], label: 1
2025/12/08 15:14:32 - mmengine - INFO -     Box 1: [68, 146, 144.7, 447.68], label: 3
2025/12/08 15:14:32 - mmengine - INFO - [BBOX DEBUG 1] File: S229_Image__2025-11-05__13-20-36_bright_5_crop_1_jpg.rf.3c81f5377065682ac900591d340c46cb.jpg, Image shape: 720x256
2025/12/08 15:14:32 - mmengine - INFO -    Loaded 1 boxes from COCO JSON
2025/12/08 15:14:32 - mmengine - INFO -     Box 0: [187, 201, 246, 394], label: 1
2025/12/08 15:14:32 - mmengine - INFO - MultiViewCocoMetric initialized: PER-CROP evaluation mode (no aggregation, 8 crops evaluated independently)
2025/12/08 15:14:32 - mmengine - INFO - [BBOX DEBUG 2] File: S229_Image__2025-11-05__13-20-36_bright_5_crop_2_jpg.rf.4136d9859fa01319003cda6949d37d36.jpg, Image shape: 720x256
2025/12/08 15:14:32 - mmengine - INFO -    Loaded 1 boxes from COCO JSON
2025/12/08 15:14:32 - mmengine - INFO -     Box 0: [109, 190, 201, 391], label: 1
2025/12/08 15:14:32 - mmengine - INFO - [BBOX DEBUG 2] File: S41_Image__2025-09-04__16-14-12_bright_2_crop_2_jpg.rf.cbfbd2a394274f8669aec9bd3c3e65c5.jpg, Image shape: 720x256
2025/12/08 15:14:32 - mmengine - INFO -    Loaded 1 boxes from COCO JSON
2025/12/08 15:14:32 - mmengine - INFO -     Box 0: [43, 102, 147.43, 294.72], label: 3
2025/12/08 15:14:32 - mmengine - INFO - [BBOX DEBUG 3] File: S229_Image__2025-11-05__13-20-36_bright_5_crop_3_jpg.rf.63eaed220d283e07c2fd99bd5ae9b143.jpg, Image shape: 720x256
2025/12/08 15:14:32 - mmengine - INFO -    Loaded 2 boxes from COCO JSON
2025/12/08 15:14:32 - mmengine - INFO -     Box 0: [50, 196, 135, 385], label: 1
2025/12/08 15:14:32 - mmengine - INFO -     Box 1: [149, 391, 196.37, 468.64], label: 3
2025/12/08 15:14:32 - mmengine - INFO - [BBOX DEBUG 1] File: S163_Image__2025-09-23__13-14-55_bright_3_crop_4_jpg.rf.663043df714795fe70bfedba2a6ef0bd.jpg, Image shape: 720x256
2025/12/08 15:14:32 - mmengine - INFO -    Loaded 1 boxes from COCO JSON
2025/12/08 15:14:32 - mmengine - INFO -     Box 0: [137, 0, 218.256, 324.961], label: 2
2025/12/08 15:14:32 - mmengine - INFO - [BBOX DEBUG 2] File: S163_Image__2025-09-23__13-14-55_bright_3_crop_5_jpg.rf.f600d82ccdf12ff830d8f0a8aa2d35ab.jpg, Image shape: 720x256
2025/12/08 15:14:32 - mmengine - INFO -    Loaded 1 boxes from COCO JSON
2025/12/08 15:14:32 - mmengine - INFO -     Box 0: [53, 0, 140.451, 321.916], label: 2
2025/12/08 15:14:32 - mmengine - INFO - [BBOX DEBUG 1] File: S213_Image__2025-06-05__10-33-38_bright_2_crop_3_jpg.rf.81dd9291d4f05e5863b14171eb083aa2.jpg, Image shape: 720x256
2025/12/08 15:14:32 - mmengine - INFO -    Loaded 2 boxes from COCO JSON
2025/12/08 15:14:32 - mmengine - INFO -     Box 0: [186, 423, 219.32999999999998, 494.11], label: 1
2025/12/08 15:14:32 - mmengine - INFO -     Box 1: [70, 24, 131.65, 135.01999999999998], label: 4
2025/12/08 15:14:32 - mmengine - INFO - [BBOX DEBUG 3] File: S163_Image__2025-09-23__13-14-55_bright_3_crop_6_jpg.rf.785cb998b7d3dabe80d877fe36af141c.jpg, Image shape: 720x256
2025/12/08 15:14:32 - mmengine - INFO -    Loaded 1 boxes from COCO JSON
2025/12/08 15:14:32 - mmengine - INFO -     Box 0: [0, 0, 60.527, 332.768], label: 2
2025/12/08 15:14:32 - mmengine - INFO - [BBOX DEBUG 2] File: S213_Image__2025-06-05__10-33-38_bright_2_crop_4_jpg.rf.a90c71bb472e8cbc463f06e9364e9124.jpg, Image shape: 720x256
2025/12/08 15:14:32 - mmengine - INFO -    Loaded 2 boxes from COCO JSON
2025/12/08 15:14:32 - mmengine - INFO -     Box 0: [103, 453, 144.11, 517.44], label: 1
2025/12/08 15:14:32 - mmengine - INFO -     Box 1: [49, 46, 148.64, 150.35], label: 4
2025/12/08 15:14:32 - mmengine - INFO - [BBOX DEBUG 3] File: S41_Image__2025-09-04__16-14-12_bright_2_crop_4_jpg.rf.927c94f79084214e45e0e471529647c4.jpg, Image shape: 720x256
2025/12/08 15:14:32 - mmengine - INFO -    Loaded 2 boxes from COCO JSON
2025/12/08 15:14:32 - mmengine - INFO -     Box 0: [120, 351, 181.77, 621.38], label: 2
2025/12/08 15:14:32 - mmengine - INFO -     Box 1: [67, 0, 247.21, 99.91], label: 4
2025/12/08 15:14:32 - mmengine - INFO - [BBOX DEBUG 3] File: S213_Image__2025-06-05__10-33-38_bright_2_crop_5_jpg.rf.d6bbe6473e8239f44910dfe0041e8699.jpg, Image shape: 720x256
2025/12/08 15:14:32 - mmengine - INFO -    Loaded 1 boxes from COCO JSON
2025/12/08 15:14:32 - mmengine - INFO -     Box 0: [20, 454, 70, 519.56], label: 1
2025/12/08 15:14:32 - mmengine - INFO - [BBOX DEBUG 1] File: S5_than_Image__2025-09-03__14-12-46_bright_2_crop_1_jpg.rf.f317142168608141505aa5d286b1bd0b.jpg, Image shape: 720x256
2025/12/08 15:14:32 - mmengine - INFO -    Loaded 1 boxes from COCO JSON
2025/12/08 15:14:32 - mmengine - INFO -     Box 0: [155, 360, 210.52, 556.19], label: 2
2025/12/08 15:14:32 - mmengine - INFO - [BBOX DEBUG 1] File: S141_Image__2025-09-24__10-58-54_bright_4_crop_1_jpg.rf.7ace99fd539f58014bf7a54b0e06fa05.jpg, Image shape: 720x256
2025/12/08 15:14:32 - mmengine - INFO -    Loaded 1 boxes from COCO JSON
2025/12/08 15:14:32 - mmengine - INFO -     Box 0: [166, 157, 214.62, 275.31600000000003], label: 2
2025/12/08 15:14:32 - mmengine - INFO - [BBOX DEBUG 2] File: S5_than_Image__2025-09-03__14-12-46_bright_2_crop_2_jpg.rf.816a6b4098896c30669385cccda9f82b.jpg, Image shape: 720x256
2025/12/08 15:14:32 - mmengine - INFO -    Loaded 1 boxes from COCO JSON
2025/12/08 15:14:32 - mmengine - INFO - [BBOX DEBUG 2] File: S141_Image__2025-09-24__10-58-54_bright_4_crop_2_jpg.rf.ea898feee42354fa7e1e57178a0da825.jpg, Image shape: 720x256
2025/12/08 15:14:32 - mmengine - INFO -     Box 0: [73, 277, 134.539, 526.137], label: 2
2025/12/08 15:14:32 - mmengine - INFO -    Loaded 1 boxes from COCO JSON
2025/12/08 15:14:32 - mmengine - INFO -     Box 0: [93, 101, 154.789, 226.019], label: 2
2025/12/08 15:14:32 - mmengine - INFO - [BBOX DEBUG 3] File: S5_than_Image__2025-09-03__14-12-46_bright_2_crop_3_jpg.rf.a022100ca19424b01d810fd44cde9bf8.jpg, Image shape: 720x256
2025/12/08 15:14:32 - mmengine - INFO -    Loaded 1 boxes from COCO JSON
2025/12/08 15:14:32 - mmengine - INFO - [BBOX DEBUG 1] File: S70_Image__2025-09-08__13-28-35_bright_2_crop_2_jpg.rf.b7863291299802b80502d3909f4a5dbe.jpg, Image shape: 720x256
2025/12/08 15:14:32 - mmengine - INFO -     Box 0: [45, 403, 70.501, 533.6990000000001], label: 2
2025/12/08 15:14:32 - mmengine - INFO -    Loaded 2 boxes from COCO JSON
2025/12/08 15:14:32 - mmengine - INFO -     Box 0: [148, 206, 225.29000000000002, 442.19], label: 2
2025/12/08 15:14:32 - mmengine - INFO -     Box 1: [138, 118, 198.38, 191.31], label: 4
2025/12/08 15:14:32 - mmengine - INFO - [BBOX DEBUG 3] File: S141_Image__2025-09-24__10-58-54_bright_4_crop_3_jpg.rf.7a4368aad147641862b16f8a2054f61f.jpg, Image shape: 720x256
2025/12/08 15:14:32 - mmengine - INFO -    Loaded 1 boxes from COCO JSON
2025/12/08 15:14:32 - mmengine - INFO -     Box 0: [35, 105, 85.39500000000001, 218.373], label: 2
2025/12/08 15:14:32 - mmengine - INFO - [BBOX DEBUG 2] File: S70_Image__2025-09-08__13-28-35_bright_2_crop_3_jpg.rf.341ab90d0c47c6edda9d196897325d73.jpg, Image shape: 720x256
2025/12/08 15:14:32 - mmengine - INFO -    Loaded 2 boxes from COCO JSON
2025/12/08 15:14:32 - mmengine - INFO -     Box 0: [43, 162, 140.41, 414.24], label: 2
2025/12/08 15:14:32 - mmengine - INFO -     Box 1: [74, 85, 163.66, 157.85], label: 4
2025/12/08 15:14:32 - mmengine - INFO - [BBOX DEBUG 3] File: S70_Image__2025-09-08__13-28-35_bright_2_crop_4_jpg.rf.8bb68394baaa44e2b185e2eb42211b9b.jpg, Image shape: 720x256
2025/12/08 15:14:32 - mmengine - INFO -    Loaded 3 boxes from COCO JSON
2025/12/08 15:14:32 - mmengine - INFO -     Box 0: [94, 397, 229.91, 587.67], label: 3
2025/12/08 15:14:32 - mmengine - INFO -     Box 1: [2, 202, 64.03, 435.16999999999996], label: 2
2025/12/08 15:14:32 - mmengine - INFO -     Box 2: [48, 91, 142.17000000000002, 194.12], label: 4
2025/12/08 15:14:32 - mmengine - INFO - [BBOX DEBUG 1] File: S149_Image__2025-09-24__13-47-22_bright_2_crop_3_jpg.rf.d7e723c4b096606b2d350821af3d87b2.jpg, Image shape: 720x256
2025/12/08 15:14:32 - mmengine - INFO -    Loaded 1 boxes from COCO JSON
2025/12/08 15:14:32 - mmengine - INFO -     Box 0: [49, 113, 153.207, 201.66899999999998], label: 4
2025/12/08 15:14:32 - mmengine - INFO - [BBOX DEBUG 2] File: S149_Image__2025-09-24__13-47-22_bright_2_crop_4_jpg.rf.27e4c250a5a4e01d4781944da42fab06.jpg, Image shape: 720x256
2025/12/08 15:14:32 - mmengine - INFO -    Loaded 2 boxes from COCO JSON
2025/12/08 15:14:32 - mmengine - INFO -     Box 0: [166, 515, 230.29000000000002, 664.76], label: 2
2025/12/08 15:14:32 - mmengine - INFO -     Box 1: [56, 120, 168.70999999999998, 214.72], label: 4
2025/12/08 15:14:32 - mmengine - INFO - [BBOX DEBUG 3] File: S149_Image__2025-09-24__13-47-22_bright_2_crop_5_jpg.rf.38201707003bb2d86a3892c8b9e2a543.jpg, Image shape: 720x256
2025/12/08 15:14:32 - mmengine - INFO -    Loaded 1 boxes from COCO JSON
2025/12/08 15:14:32 - mmengine - INFO -     Box 0: [107, 448, 198.34699999999998, 646.989], label: 2
Name of parameter - Initialization information

student.backbone.backbone.conv1.weight - torch.Size([64, 3, 7, 7]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.0.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.0.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.0.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.0.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.0.bn3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.0.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.0.downsample.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.0.downsample.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.1.bn3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.1.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.2.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.2.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.2.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.2.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.2.bn3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer1.2.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.0.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.0.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.0.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.0.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.0.bn3.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.0.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.0.downsample.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.0.downsample.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.1.bn3.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.1.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.2.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.2.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.2.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.2.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.2.bn3.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.2.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.3.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.3.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.3.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.3.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.3.bn3.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer2.3.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.0.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.0.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.0.downsample.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.0.downsample.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.1.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.1.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.2.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.2.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.2.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.2.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.2.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.2.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.3.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.3.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.3.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.3.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.3.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.3.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.4.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.4.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.4.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.4.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.4.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.4.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.5.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.5.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.5.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.5.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.5.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer3.5.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.0.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.0.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.0.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.0.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.0.bn3.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.0.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.0.downsample.1.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.0.downsample.1.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.1.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.1.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.1.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.1.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.1.bn3.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.1.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.2.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.2.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.2.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.2.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.2.bn3.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.backbone.layer4.2.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.residual_alpha - torch.Size([]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.layers.0.self_attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.layers.0.self_attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.layers.0.self_attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.layers.0.self_attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.layers.0.linear1.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.layers.0.linear1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.layers.0.linear2.weight - torch.Size([256, 512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.layers.0.linear2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.layers.0.norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.layers.0.norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.layers.0.norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.layers.0.norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.pooling_queries.1024 - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.pooling_queries.2048 - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.pooling_queries.256 - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.pooling_queries.512 - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.upsample_queries.128x128 - torch.Size([16384, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.upsample_queries.32x32 - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.backbone.mvvit.upsample_queries.64x64 - torch.Size([4096, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.neck.lateral_convs.0.conv.weight - torch.Size([256, 256, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

student.neck.lateral_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.neck.lateral_convs.1.conv.weight - torch.Size([256, 512, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

student.neck.lateral_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.neck.lateral_convs.2.conv.weight - torch.Size([256, 1024, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

student.neck.lateral_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.neck.lateral_convs.3.conv.weight - torch.Size([256, 2048, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

student.neck.lateral_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

student.neck.fpn_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

student.neck.fpn_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

student.neck.fpn_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.neck.fpn_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

student.neck.fpn_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

student.rpn_head.rpn_conv.weight - torch.Size([256, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=0 

student.rpn_head.rpn_conv.bias - torch.Size([256]): 
NormalInit: mean=0, std=0.01, bias=0 

student.rpn_head.rpn_cls.weight - torch.Size([15, 256, 1, 1]): 
NormalInit: mean=0, std=0.01, bias=0 

student.rpn_head.rpn_cls.bias - torch.Size([15]): 
NormalInit: mean=0, std=0.01, bias=0 

student.rpn_head.rpn_reg.weight - torch.Size([60, 256, 1, 1]): 
NormalInit: mean=0, std=0.01, bias=0 

student.rpn_head.rpn_reg.bias - torch.Size([60]): 
NormalInit: mean=0, std=0.01, bias=0 

student.roi_head.bbox_head.fc_cls.weight - torch.Size([6, 1024]): 
NormalInit: mean=0, std=0.01, bias=0 

student.roi_head.bbox_head.fc_cls.bias - torch.Size([6]): 
NormalInit: mean=0, std=0.01, bias=0 

student.roi_head.bbox_head.fc_reg.weight - torch.Size([20, 1024]): 
NormalInit: mean=0, std=0.001, bias=0 

student.roi_head.bbox_head.fc_reg.bias - torch.Size([20]): 
NormalInit: mean=0, std=0.001, bias=0 

student.roi_head.bbox_head.shared_fcs.0.weight - torch.Size([1024, 12544]): 
XavierInit: gain=1, distribution=uniform, bias=0 

student.roi_head.bbox_head.shared_fcs.0.bias - torch.Size([1024]): 
XavierInit: gain=1, distribution=uniform, bias=0 

student.roi_head.bbox_head.shared_fcs.1.weight - torch.Size([1024, 1024]): 
XavierInit: gain=1, distribution=uniform, bias=0 

student.roi_head.bbox_head.shared_fcs.1.bias - torch.Size([1024]): 
XavierInit: gain=1, distribution=uniform, bias=0 

teacher.backbone.backbone.conv1.weight - torch.Size([64, 3, 7, 7]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.0.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.0.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.0.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.0.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.0.bn3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.0.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.0.downsample.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.0.downsample.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.1.bn3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.1.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.2.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.2.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.2.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.2.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.2.bn3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer1.2.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.0.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.0.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.0.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.0.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.0.bn3.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.0.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.0.downsample.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.0.downsample.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.1.bn3.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.1.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.2.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.2.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.2.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.2.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.2.bn3.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.2.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.3.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.3.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.3.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.3.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.3.bn3.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer2.3.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.0.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.0.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.0.downsample.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.0.downsample.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.1.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.1.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.2.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.2.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.2.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.2.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.2.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.2.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.3.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.3.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.3.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.3.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.3.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.3.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.4.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.4.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.4.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.4.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.4.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.4.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.5.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.5.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.5.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.5.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.5.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer3.5.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.0.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.0.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.0.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.0.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.0.bn3.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.0.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.0.downsample.1.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.0.downsample.1.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.1.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.1.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.1.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.1.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.1.bn3.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.1.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.2.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.2.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.2.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.2.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.2.bn3.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.backbone.layer4.2.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.residual_alpha - torch.Size([]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.layers.0.self_attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.layers.0.self_attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.layers.0.self_attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.layers.0.self_attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.layers.0.linear1.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.layers.0.linear1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.layers.0.linear2.weight - torch.Size([256, 512]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.layers.0.linear2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.layers.0.norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.layers.0.norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.layers.0.norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.layers.0.norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.pooling_queries.1024 - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.pooling_queries.2048 - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.pooling_queries.256 - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.pooling_queries.512 - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.upsample_queries.128x128 - torch.Size([16384, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.upsample_queries.32x32 - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.backbone.mvvit.upsample_queries.64x64 - torch.Size([4096, 256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.neck.lateral_convs.0.conv.weight - torch.Size([256, 256, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

teacher.neck.lateral_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.neck.lateral_convs.1.conv.weight - torch.Size([256, 512, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

teacher.neck.lateral_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.neck.lateral_convs.2.conv.weight - torch.Size([256, 1024, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

teacher.neck.lateral_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.neck.lateral_convs.3.conv.weight - torch.Size([256, 2048, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

teacher.neck.lateral_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

teacher.neck.fpn_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

teacher.neck.fpn_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

teacher.neck.fpn_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.neck.fpn_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

teacher.neck.fpn_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewSoftTeacher  

teacher.rpn_head.rpn_conv.weight - torch.Size([256, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=0 

teacher.rpn_head.rpn_conv.bias - torch.Size([256]): 
NormalInit: mean=0, std=0.01, bias=0 

teacher.rpn_head.rpn_cls.weight - torch.Size([15, 256, 1, 1]): 
NormalInit: mean=0, std=0.01, bias=0 

teacher.rpn_head.rpn_cls.bias - torch.Size([15]): 
NormalInit: mean=0, std=0.01, bias=0 

teacher.rpn_head.rpn_reg.weight - torch.Size([60, 256, 1, 1]): 
NormalInit: mean=0, std=0.01, bias=0 

teacher.rpn_head.rpn_reg.bias - torch.Size([60]): 
NormalInit: mean=0, std=0.01, bias=0 

teacher.roi_head.bbox_head.fc_cls.weight - torch.Size([6, 1024]): 
NormalInit: mean=0, std=0.01, bias=0 

teacher.roi_head.bbox_head.fc_cls.bias - torch.Size([6]): 
NormalInit: mean=0, std=0.01, bias=0 

teacher.roi_head.bbox_head.fc_reg.weight - torch.Size([20, 1024]): 
NormalInit: mean=0, std=0.001, bias=0 

teacher.roi_head.bbox_head.fc_reg.bias - torch.Size([20]): 
NormalInit: mean=0, std=0.001, bias=0 

teacher.roi_head.bbox_head.shared_fcs.0.weight - torch.Size([1024, 12544]): 
XavierInit: gain=1, distribution=uniform, bias=0 

teacher.roi_head.bbox_head.shared_fcs.0.bias - torch.Size([1024]): 
XavierInit: gain=1, distribution=uniform, bias=0 

teacher.roi_head.bbox_head.shared_fcs.1.weight - torch.Size([1024, 1024]): 
XavierInit: gain=1, distribution=uniform, bias=0 

teacher.roi_head.bbox_head.shared_fcs.1.bias - torch.Size([1024]): 
XavierInit: gain=1, distribution=uniform, bias=0 
2025/12/08 15:14:35 - mmengine - WARNING - "FileClient" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io
2025/12/08 15:14:35 - mmengine - WARNING - "HardDiskBackend" is the alias of "LocalBackend" and the former will be deprecated in future.
2025/12/08 15:14:35 - mmengine - INFO - Checkpoints will be saved to /home/coder/data/trong/KLTN/Soft_Teacher/work_dirs/soft_teacher_custom_multi_view.
2025/12/08 15:14:36 - mmengine - INFO - [Supervised Step 0] Groups: 2 , G0:6box | G1:14box
2025/12/08 15:14:41 - mmengine - INFO - [Supervised Step 1] Groups: 2 , G0:14box | G1:8box
2025/12/08 15:14:43 - mmengine - INFO - [Supervised Step 2] Groups: 2 , G0:3box | G1:11box
2025/12/08 15:14:46 - mmengine - INFO - [Supervised Step 3] Groups: 2 , G0:8box | G1:3box
2025/12/08 15:14:49 - mmengine - INFO - [Supervised Step 4] Groups: 2 , G0:3box | G1:6box
2025/12/08 15:14:52 - mmengine - INFO - [Supervised Step 5] Groups: 2 , G0:8box | G1:8box
2025/12/08 15:14:55 - mmengine - INFO - [Supervised Step 6] Groups: 2 , G0:19box | G1:18box
2025/12/08 15:14:58 - mmengine - INFO - [Supervised Step 7] Groups: 2 , G0:11box | G1:10box
2025/12/08 15:15:01 - mmengine - INFO - [Supervised Step 8] Groups: 2 , G0:4box | G1:13box
2025/12/08 15:15:04 - mmengine - INFO - [Supervised Step 9] Groups: 2 , G0:18box | G1:9box
2025/12/08 15:15:47 - mmengine - INFO - [Teacher Predictions] Total boxes: 160, Score range: [0.937, 1.000], Mean: 0.977, Median: 0.981
2025/12/08 15:15:47 - mmengine - INFO - [After Filtering] Threshold: 0.4, Kept: 80/160 (50.0%), Avg: 10.0 boxes/img
2025/12/08 15:15:48 - mmengine - INFO - [Cross-View Uncertainty] Groups: 1, G0:80box(unc=0.124)
2025/12/08 15:17:14 - mmengine - INFO - Epoch(train) [1][ 50/125]  lr: 7.4257e-06  eta: 3 days, 15:59:19  time: 3.1692  data_time: 0.0298  memory: 25317  loss: 2.6779  sup_loss_rpn_cls: 0.1952  sup_loss_rpn_bbox: 0.8214  sup_loss_cls: 1.3250  sup_acc: 70.1754  sup_num_pred_fg: 20.0000  sup_num_pred_bg: 37.0000  sup_num_gt_fg: 19.0000  sup_num_gt_bg: 38.0000  sup_acc_fg: 36.8421  sup_acc_bg: 86.8421  sup_loss_bbox: 0.2056  unsup_loss_rpn_cls: 0.2364  unsup_loss_rpn_bbox: 0.8896  unsup_loss_cls: 5.4077  unsup_acc: 66.6667  unsup_num_pred_fg: 0.0000  unsup_num_pred_bg: 270.0000  unsup_num_gt_fg: 90.0000  unsup_num_gt_bg: 180.0000  unsup_acc_fg: 0.0000  unsup_acc_bg: 100.0000  unsup_loss_bbox: 0.0000
2025/12/08 15:17:14 - mmengine - INFO - [Supervised Step 50] Groups: 2 , G0:10box | G1:8box
2025/12/08 15:19:52 - mmengine - INFO - Epoch(train) [1][100/125]  lr: 9.9010e-06  eta: 3 days, 15:50:42  time: 3.1620  data_time: 0.0422  memory: 25317  loss: 1.9847  sup_loss_rpn_cls: 0.1622  sup_loss_rpn_bbox: 0.5686  sup_loss_cls: 0.3747  sup_acc: 70.4626  sup_num_pred_fg: 47.0000  sup_num_pred_bg: 234.0000  sup_num_gt_fg: 93.0000  sup_num_gt_bg: 188.0000  sup_acc_fg: 21.5054  sup_acc_bg: 94.6808  sup_loss_bbox: 0.8450  unsup_loss_rpn_cls: 0.1927  unsup_loss_rpn_bbox: 0.7869  unsup_loss_cls: 2.9069  unsup_acc: 78.7524  unsup_num_pred_fg: 101.0000  unsup_num_pred_bg: 412.0000  unsup_num_gt_fg: 171.0000  unsup_num_gt_bg: 342.0000  unsup_acc_fg: 47.3684  unsup_acc_bg: 94.4444  unsup_loss_bbox: 0.2355
2025/12/08 15:19:52 - mmengine - INFO - [Supervised Step 100] Groups: 2 , G0:7box | G1:17box
2025/12/08 15:21:12 - mmengine - INFO - Exp name: soft_teacher_custom_multi_view_20251208_151419
2025/12/08 15:22:29 - mmengine - INFO - Epoch(train) [1][150/125]  lr: 1.2376e-05  eta: 3 days, 15:37:33  time: 3.1467  data_time: 0.0507  memory: 25316  loss: 1.9551  sup_loss_rpn_cls: 0.1438  sup_loss_rpn_bbox: 0.4822  sup_loss_cls: 0.3296  sup_acc: 60.7655  sup_num_pred_fg: 24.0000  sup_num_pred_bg: 185.0000  sup_num_gt_fg: 69.0000  sup_num_gt_bg: 140.0000  sup_acc_fg: 1.4493  sup_acc_bg: 90.0000  sup_loss_bbox: 0.9651  unsup_loss_rpn_cls: 0.1789  unsup_loss_rpn_bbox: 0.7333  unsup_loss_cls: 2.0614  unsup_acc: 65.2921  unsup_num_pred_fg: 120.0000  unsup_num_pred_bg: 462.0000  unsup_num_gt_fg: 194.0000  unsup_num_gt_bg: 388.0000  unsup_acc_fg: 28.3505  unsup_acc_bg: 83.7629  unsup_loss_bbox: 0.3465
2025/12/08 15:22:29 - mmengine - INFO - [Supervised Step 150] Groups: 2 , G0:13box | G1:12box
2025/12/08 15:25:05 - mmengine - INFO - Epoch(train) [1][200/125]  lr: 1.4851e-05  eta: 3 days, 15:15:11  time: 3.1118  data_time: 0.0436  memory: 25316  loss: 2.0322  sup_loss_rpn_cls: 0.1414  sup_loss_rpn_bbox: 0.4592  sup_loss_cls: 0.3048  sup_acc: 68.4039  sup_num_pred_fg: 5.0000  sup_num_pred_bg: 302.0000  sup_num_gt_fg: 99.0000  sup_num_gt_bg: 208.0000  sup_acc_fg: 3.0303  sup_acc_bg: 99.5192  sup_loss_bbox: 1.1268  unsup_loss_rpn_cls: 0.1789  unsup_loss_rpn_bbox: 0.7333  unsup_loss_cls: 2.0614  unsup_acc: 65.2921  unsup_num_pred_fg: 120.0000  unsup_num_pred_bg: 462.0000  unsup_num_gt_fg: 194.0000  unsup_num_gt_bg: 388.0000  unsup_acc_fg: 28.3505  unsup_acc_bg: 83.7629  unsup_loss_bbox: 0.3465
2025/12/08 15:25:05 - mmengine - INFO - [Supervised Step 200] Groups: 2 , G0:8box | G1:10box
2025/12/08 15:27:41 - mmengine - INFO - Epoch(train) [1][250/125]  lr: 1.7327e-05  eta: 3 days, 15:05:55  time: 3.1275  data_time: 0.0410  memory: 25316  loss: 2.0677  sup_loss_rpn_cls: 0.1359  sup_loss_rpn_bbox: 0.4562  sup_loss_cls: 0.3190  sup_acc: 65.1042  sup_num_pred_fg: 19.0000  sup_num_pred_bg: 173.0000  sup_num_gt_fg: 64.0000  sup_num_gt_bg: 128.0000  sup_acc_fg: 9.3750  sup_acc_bg: 92.9688  sup_loss_bbox: 1.1566  unsup_loss_rpn_cls: 0.1789  unsup_loss_rpn_bbox: 0.7333  unsup_loss_cls: 2.0614  unsup_acc: 65.2921  unsup_num_pred_fg: 120.0000  unsup_num_pred_bg: 462.0000  unsup_num_gt_fg: 194.0000  unsup_num_gt_bg: 388.0000  unsup_acc_fg: 28.3505  unsup_acc_bg: 83.7629  unsup_loss_bbox: 0.3465
2025/12/08 15:27:41 - mmengine - INFO - [Supervised Step 250] Groups: 2 , G0:8box | G1:8box
2025/12/08 15:30:22 - mmengine - INFO - Epoch(train) [1][300/125]  lr: 1.9802e-05  eta: 3 days, 15:23:45  time: 3.2172  data_time: 0.0388  memory: 25317  loss: 2.1800  sup_loss_rpn_cls: 0.1301  sup_loss_rpn_bbox: 0.4475  sup_loss_cls: 0.3380  sup_acc: 51.7134  sup_num_pred_fg: 150.0000  sup_num_pred_bg: 171.0000  sup_num_gt_fg: 105.0000  sup_num_gt_bg: 216.0000  sup_acc_fg: 36.1905  sup_acc_bg: 59.2593  sup_loss_bbox: 1.1832  unsup_loss_rpn_cls: 0.1802  unsup_loss_rpn_bbox: 0.6487  unsup_loss_cls: 1.5670  unsup_acc: 62.8205  unsup_num_pred_fg: 27.0000  unsup_num_pred_bg: 441.0000  unsup_num_gt_fg: 156.0000  unsup_num_gt_bg: 312.0000  unsup_acc_fg: 1.2821  unsup_acc_bg: 93.5897  unsup_loss_bbox: 0.4085
2025/12/08 15:30:22 - mmengine - INFO - [Supervised Step 300] Groups: 2 , G0:16box | G1:13box
2025/12/08 15:32:56 - mmengine - INFO - Epoch(train) [1][350/125]  lr: 2.2277e-05  eta: 3 days, 15:03:59  time: 3.0835  data_time: 0.0441  memory: 25317  loss: 2.1597  sup_loss_rpn_cls: 0.1323  sup_loss_rpn_bbox: 0.4400  sup_loss_cls: 0.3683  sup_acc: 66.8508  sup_num_pred_fg: 9.0000  sup_num_pred_bg: 172.0000  sup_num_gt_fg: 53.0000  sup_num_gt_bg: 128.0000  sup_acc_fg: 0.0000  sup_acc_bg: 94.5312  sup_loss_bbox: 1.2190  unsup_loss_rpn_cls: 0.1802  unsup_loss_rpn_bbox: 0.6487  unsup_loss_cls: 1.5670  unsup_acc: 62.8205  unsup_num_pred_fg: 27.0000  unsup_num_pred_bg: 441.0000  unsup_num_gt_fg: 156.0000  unsup_num_gt_bg: 312.0000  unsup_acc_fg: 1.2821  unsup_acc_bg: 93.5897  unsup_loss_bbox: 0.4085
2025/12/08 15:32:56 - mmengine - INFO - [Supervised Step 350] Groups: 2 , G0:7box | G1:11box
2025/12/08 15:35:31 - mmengine - INFO - Epoch(train) [1][400/125]  lr: 2.4752e-05  eta: 3 days, 14:48:13  time: 3.0821  data_time: 0.0496  memory: 25317  loss: 2.1933  sup_loss_rpn_cls: 0.1273  sup_loss_rpn_bbox: 0.4378  sup_loss_cls: 0.3325  sup_acc: 69.1358  sup_num_pred_fg: 62.0000  sup_num_pred_bg: 505.0000  sup_num_gt_fg: 189.0000  sup_num_gt_bg: 378.0000  sup_acc_fg: 15.3439  sup_acc_bg: 96.0317  sup_loss_bbox: 1.2604  unsup_loss_rpn_cls: 0.1816  unsup_loss_rpn_bbox: 0.6179  unsup_loss_cls: 1.4207  unsup_acc: 66.8817  unsup_num_pred_fg: 18.0000  unsup_num_pred_bg: 447.0000  unsup_num_gt_fg: 155.0000  unsup_num_gt_bg: 310.0000  unsup_acc_fg: 5.1613  unsup_acc_bg: 97.7419  unsup_loss_bbox: 0.4109
2025/12/08 15:35:31 - mmengine - INFO - [Supervised Step 400] Groups: 2 , G0:9box | G1:18box
2025/12/08 15:38:05 - mmengine - INFO - Epoch(train) [1][450/125]  lr: 2.7228e-05  eta: 3 days, 14:37:32  time: 3.0937  data_time: 0.0461  memory: 25317  loss: 2.1584  sup_loss_rpn_cls: 0.1276  sup_loss_rpn_bbox: 0.4216  sup_loss_cls: 0.3662  sup_acc: 64.3606  sup_num_pred_fg: 17.0000  sup_num_pred_bg: 460.0000  sup_num_gt_fg: 157.0000  sup_num_gt_bg: 320.0000  sup_acc_fg: 0.6369  sup_acc_bg: 95.6250  sup_loss_bbox: 1.2061  unsup_loss_rpn_cls: 0.1788  unsup_loss_rpn_bbox: 0.5944  unsup_loss_cls: 1.3264  unsup_acc: 64.3590  unsup_num_pred_fg: 23.0000  unsup_num_pred_bg: 367.0000  unsup_num_gt_fg: 130.0000  unsup_num_gt_bg: 260.0000  unsup_acc_fg: 5.3846  unsup_acc_bg: 93.8462  unsup_loss_bbox: 0.4192
2025/12/08 15:38:05 - mmengine - INFO - [Supervised Step 450] Groups: 2 , G0:8box | G1:4box
2025/12/08 15:40:41 - mmengine - INFO - Epoch(train) [1][500/125]  lr: 2.9703e-05  eta: 3 days, 14:33:25  time: 3.1235  data_time: 0.0370  memory: 25317  loss: 2.1491  sup_loss_rpn_cls: 0.1274  sup_loss_rpn_bbox: 0.4196  sup_loss_cls: 0.3410  sup_acc: 68.2609  sup_num_pred_fg: 12.0000  sup_num_pred_bg: 448.0000  sup_num_gt_fg: 150.0000  sup_num_gt_bg: 310.0000  sup_acc_fg: 5.3333  sup_acc_bg: 98.7097  sup_loss_bbox: 1.2315  unsup_loss_rpn_cls: 0.1758  unsup_loss_rpn_bbox: 0.5672  unsup_loss_cls: 1.2131  unsup_acc: 63.9042  unsup_num_pred_fg: 49.0000  unsup_num_pred_bg: 494.0000  unsup_num_gt_fg: 181.0000  unsup_num_gt_bg: 362.0000  unsup_acc_fg: 9.3923  unsup_acc_bg: 91.1602  unsup_loss_bbox: 0.4327
2025/12/08 15:40:41 - mmengine - INFO - [Supervised Step 500] Groups: 2 , G0:17box | G1:4box
2025/12/08 15:43:19 - mmengine - INFO - Epoch(train) [1][550/125]  lr: 3.2178e-05  eta: 3 days, 14:35:03  time: 3.1598  data_time: 0.0263  memory: 25317  loss: 2.1413  sup_loss_rpn_cls: 0.1242  sup_loss_rpn_bbox: 0.4224  sup_loss_cls: 0.3168  sup_acc: 63.0229  sup_num_pred_fg: 46.0000  sup_num_pred_bg: 695.0000  sup_num_gt_fg: 245.0000  sup_num_gt_bg: 496.0000  sup_acc_fg: 2.4490  sup_acc_bg: 92.9436  sup_loss_bbox: 1.2457  unsup_loss_rpn_cls: 0.1700  unsup_loss_rpn_bbox: 0.5512  unsup_loss_cls: 1.1379  unsup_acc: 65.7778  unsup_num_pred_fg: 16.0000  unsup_num_pred_bg: 659.0000  unsup_num_gt_fg: 225.0000  unsup_num_gt_bg: 450.0000  unsup_acc_fg: 2.2222  unsup_acc_bg: 97.5556  unsup_loss_bbox: 0.4432
2025/12/08 15:43:19 - mmengine - INFO - [Supervised Step 550] Groups: 2 , G0:7box | G1:7box
